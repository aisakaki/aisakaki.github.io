<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Boosting(XGBoost、LightGBM)和Random Forest</title>
    <url>/2019/11/19/Forest/</url>
    <content><![CDATA[<p><em>施工中</em></p>
<p><em>XGBoost和LightGBM都属于GBDT的变种，见上篇；Random Forest属于Bagging方法；</em></p>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>原论文： <a href="https://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">https://arxiv.org/abs/1603.02754</a> </p>
<p>基于CART回归树的生成方法：依然是先确定最优树的结构，然后通过最小化损失函数确定叶子结点的输出值，XGBoost在GBDT上做了一定改进</p>
<p>注意理解上一篇GBDT中的【注意理解】部分，读本篇需要先熟练掌握CART回归树和GBDT算法。</p>
<h3 id="引入正则化项来控制树的复杂度"><a href="#引入正则化项来控制树的复杂度" class="headerlink" title="引入正则化项来控制树的复杂度"></a>引入正则化项来控制树的复杂度</h3><p><strong>表记</strong>$T(x;\Theta_{m})=w_{q(x)}$，$x$为一个样本，$q(x)$表示<strong>该样本所在的叶子结点</strong>，$w_q$为叶子节点$q$的回归输出；于是$w_{q(x)}$即表示每个样本的回归输出，即决策树的输出。定义集合$I_j=\{i|q(x^{(i)})=j\}$表示划分到叶结点$j$的所有训练样本的集合即上一篇的表示法：$\{x|x∈R_{j}\}$。$T$为叶子节点个数（即划分个数）$m$为步/轮数，$N$为样本个数。</p>
<p>定义每一步生成新决策树的损失函数的正则项：</p>
<script type="math/tex; mode=display">
\Omega(T(x;\Theta_m))=\gamma T+\frac{1}{2}\lambda||w||^2</script><p>可以看出惩罚考虑了两项：$T$为<strong>叶子节点个数</strong>，$w$为<strong>每个样本的输出值</strong></p>
<p>（也可以用$c$表示：$\Omega(T(x;\Theta_m))=\gamma T+\frac{1}{2}\lambda||c||^2$，$c$即每个划分的输出值，也就对应了每个样本的输出）</p>
<p>XGBoost向损失函数加入此正则项($L’$为不含正则项的原损失函数)变为（样本总体损失函数）：</p>
<script type="math/tex; mode=display">
L(y_{i+1},f_{m+1}(x_i))=\sum_iL'(y_{i+1},f_{m+1}(x_i))+\Omega(T(x;\Theta_{m+1}))</script><a id="more"></a>    
<h3 id="修改拟合目标"><a href="#修改拟合目标" class="headerlink" title="修改拟合目标"></a>修改拟合目标</h3><p>在AdaBoost中我们是拟合残差</p>
<p>在GBDT中我们是去拟合负梯度（一阶导数，泰勒公式一阶展开求导得出）来替代拟合残差</p>
<p>而XGBoost中， 直接<strong>用泰勒展开式将损失函数二阶展开，求其一阶梯度和二阶梯度，正因为使用损失函数的二阶泰勒展开，因此与损失函数更接近，比只使用了一阶展开的GBDT收敛速度更快</strong>（前提是函数一阶、二阶都连续可导，而且在这里计算一阶导和二阶导时可以并行计算） </p>
<script type="math/tex; mode=display">
g_i=\frac{\partial L'(y_i,f(x_i))}{\partial f(x_i)},h_i=\frac{\partial^2 L '(y_i,f(x_i))}{\partial f(x_i)}</script><p>于是用和GBDT一样的方法，对无正则的原损失函数（单个样本）$L’(y_{i+1},f_{m+1}(x_i))$在$\hat f_m(x_i)$处进行泰勒二阶展开，这里简要推导：</p>
<p>$L’(y_{i+1},f_{m+1}(x_i))=L’(y_{i+1},f_m(x_i)+T(x,\Theta_{m+1}))$ </p>
<p>对右式在$\hat f_m(x_i)$二阶泰勒展开，并令$x’=\hat f_m(x_i)+T(x,\Theta_{m+1})$，$y$为无关常量</p>
<p>$\therefore 原式≈L’(y_{i+1},f_m(x_i))+g_i(x’-\hat f_m(x_i))+\frac{1}{2}h_i(x’-\hat f_m(x_i))^2$</p>
<p>$=L’(y_{i+1},f_m(x_i))+g_iT(x,\Theta_{m+1})+\frac{1}{2}h_iT^2(x,\Theta_{m+1})$</p>
<p>由于对本轮优化来说，$L’(y_{i+1},f_m(x_i))$是个常数，不会影响损失函数，于是：</p>
<p>$L’(y_{i+1},f_{m+1}(x_i))≈g_iT(x,\Theta_{m+1})+\frac{1}{2}h_iT^2(x,\Theta_{m+1})$</p>
<p>于是计算整个样本集的损失函数（前面的损失函数都是单个样本的）加上前面的正则构成目标损失函数：</p>
<p>$Obj_{m+1}=\sum_{i=1}^N[g_iT(x,\Theta_{m+1})+\frac{1}{2}h_iT^2(x,\Theta_{m+1})]+\gamma T+\frac{1}{2}\lambda||w||^2$，转换表示，见开头的表记方法</p>
<p>$=\sum_{i=1}^N[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}]+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2$ ，将对样本求和改为对划分求和，同一个叶子节点/划分$j$的输出值是相同的，即$w_{q(x_i)}=w_j\quad if\quad i∈I_j$，于是改写为：</p>
<p>$Obj_{m+1}=\sum_{j=1}^T[(\sum_{i∈I_j} g_i)w_j+\frac{1}{2}(\sum_{i∈I_j}h_i+\lambda)w_j^2]+\gamma T$</p>
<script type="math/tex; mode=display">
令G_j=\sum_{i∈I_j} g_i \quad ,H_j=\sum_{i∈I_j}h_i\quad,则：</script><script type="math/tex; mode=display">
Obj_{m+1}=\sum_{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2]+\gamma T</script><p><strong>①</strong>于是<strong>【根据此损失函数先确定$(j,s)$划分，确定决策树的结构】</strong></p>
<p>但是注意到实际上在确定决策树结构（选择最优特征和分割点）的时候，决策树结构数量是无穷的，所以<strong>实际上并不能穷举所有可能的决策树结构。</strong>什么样的决策树结构是最优的呢？<strong>通常使用贪心策略来生成决策树的每个结点。</strong>——因为对某个结点采取的是二分策略，分别对应<strong>左子结点和右子结点</strong>，除了当前待处理的结点，其他结点对应的$Obj_{m+1}$值都不变，所以对于收益的计算只需要考虑当前结点的$Obj_{m+1}$值即可。</p>
<p>（其它DT和基于DT的boosting通常也用贪心策略）</p>
<ol>
<li>从深度为0的树开始<strong>对每个叶子结点穷举所有的可用特征</strong>；</li>
<li>针对每一个特征，把属于该结点的训练样本的该<strong>特征升序排列</strong>，通过线性扫描的方式来决定该特征的<strong>最佳分裂点</strong>，并采用最佳分裂点时的<strong>收益</strong>；</li>
<li>选择<strong>收益最大的特征作为分裂特征</strong>，用该特征的最佳分裂点作为分裂位置，把该结点生成出左右两个新的叶子结点，并为每个新结点关联新的样本集；</li>
<li>退回到第一步，继续<strong>递归</strong>操作直到满足特定条件。</li>
</ol>
<p>不过原作者没有直接这样暴力枚举，因为即使贪心策略只考虑当前结点，该结点内可能的分裂点也很多。于是原作者<strong>通过加权分位数的算法选出了一些可能的分裂点（见后文）</strong>。</p>
<p>下面我们就要通过一个<strong>增益函数/打分函数</strong>来判断谁是最佳特征和分裂点了：</p>
<p>分裂前针对该结点的最优目标函数(<strong>这个最优目标函数是假设结构已经固定的情况下在第②步中先求出来的</strong>）为： </p>
<script type="math/tex; mode=display">
Obj_{m+1}^{(before)}=-\frac{1}{2}\frac{G^2}{H+\lambda}+\gamma=-\frac{1}{2}\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}+\gamma</script><p>分裂后的最优目标函数为：（$G分为左G_L和右G_R,H同理$）</p>
<script type="math/tex; mode=display">
Obj_{m+1}^{(later)}=-\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}]+2\gamma</script><p>于是对于目标函数$Obj_m$，分裂后的收益为$Gain=Obj_{m+1}^{(before)}-Obj_{m+1}^{(later)}$：</p>
<script type="math/tex; mode=display">
Gain=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}]-\gamma</script><p><strong>为了限制树的生长</strong>，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的<strong>$\gamma$即阈值</strong>，它是正则项里叶子节点数$T$的系数，所以XGBoost在优化目标函数的同时<strong>相当于做了预剪枝</strong>。 同时可以看出该式子中<strong>$\lambda$起了对叶结点score做平滑的作用</strong>。（这俩参数定义见最前面的正则化公式）这两个参数在这里求分裂的时候也<strong>都起了正则化作用</strong></p>
<p><strong>故可用此公式决定最优分裂特征和分裂点，贪心地选择收益最大的作为分裂点</strong></p>
<p><strong>②【根据损失函数确定每个结点的最优回归输出】</strong></p>
<p>由于决策树的结构已经固定了，那么就知道了$q(),I_j$。（<strong>这里的①与GBDT的第③步一样的，先求出划分，结构就固定了</strong>，然后在GDBT中下一步③求出残差的替代让新决策树去拟合之（即缩小与残差的损失函数），求得每个结点的最优输出值；不过<strong>在XGBoost中这里推导出了一个由一阶导，二阶导构成的损失函数，于是XGBoost就是要去缩小这个损失函数，选取每个结点的最优输出值</strong>）；</p>
<p>又由于$g_i,h_i$是对当前已知模型(上一步/轮求出来的)$f_m(x_i)$的导数，那么$G_j,H_j$也是已知常数，于是让目标损失函数$Obj_{m+1}$对$w_j$<strong>求0导</strong>（我们原来是要对$T(x;\Theta_{m})$求导的，而$T(x;\Theta_{m})=w_{q(x)}$）可得</p>
<script type="math/tex; mode=display">
w^o=-\frac{G_j}{H_j+\lambda}</script><script type="math/tex; mode=display">
最优损失函数:Obj_{m+1}=-\frac{1}{2}\sum_{j=1}^T\frac{G^2_j}{H_j+\lambda}+\gamma T</script><p>$w^o=-\frac{G_j}{H_j+\lambda}$即为叶子节点的最优输出，相当于GDBT的第④步</p>
<h3 id="引入学习率-缩减（Shrinkage）"><a href="#引入学习率-缩减（Shrinkage）" class="headerlink" title="引入学习率/缩减（Shrinkage）"></a>引入学习率/缩减（Shrinkage）</h3><p>在更新前向分步模型的时候，在新树的输出引入学习率，以防止过拟合</p>
<script type="math/tex; mode=display">
f_{m+1}(x)=f_{m}(x)+\epsilon T^o(x;\Theta_{m+1})</script><h3 id="引入分布式加权分位数略图算法"><a href="#引入分布式加权分位数略图算法" class="headerlink" title="引入分布式加权分位数略图算法"></a>引入分布式加权分位数略图算法</h3><p>在决策树分裂的时候引入的减小计算量的方法。 weighted quantile sketch </p>
<p>决策树的分裂基本都是基于贪心的，也就是只考虑当前结点的最优特征和划分而不考虑全局最优，但这样依然需要穷举该结点内每个可能的分裂点。当数据没法全部加载到内存中时，这种方法会比较慢，XGBoost提出了一种近似的方法去高效的生成候选分割点——分布式加权分位数略图算法 </p>
<p>先看<strong>加权分位数略图算法</strong></p>
<p>对原目标式：$Obj_{m+1}=\sum_{i=1}^N[g_iT(x,\Theta_{m+1})+\frac{1}{2}h_iT^2(x,\Theta_{m+1})]+\Omega(T(x;\Theta_m))$变形，在里面添加一个常数：</p>
<p>$=\sum_{i=1}^N[g_iT(x,\Theta_{m+1})+\frac{1}{2}h_iT^2(x,\Theta_{m+1})+\frac{1}{2}\frac{g_i^2}{h_i}]+\Omega(T(x;\Theta_m))+constant$</p>
<p>$=\sum_{i=1}^n\frac{1}{2}h_i[T(x,\Theta_{m+1})-(-\frac{g_i}{h_i})^2]+\Omega(T(x;\Theta_m))+constant$</p>
<p>可以看出，最后的目标代价函数就是一个$h_i$的加权平均损失函数</p>
<p>于是进一步我们想要如何以分位数的方式划分区域来取候选点。</p>
<p><strong>思想：</strong>我们将特征样本排序，划分成不同区域，对样本取分位数点，每个分位数点落在区域内，但由于<strong>样本权重越高的区域，预测结果越是不确定的样本区域，切分的粒度就应该越密集。</strong></p>
<p>那么具体怎么实现呢：</p>
<p>对于数据集$D_k=\{(x_{1k},h_1),(x_{2k},h_2),\cdots,(x_{nk},h_n)\}$，$k$表示某特征，$h$即为损失函数在该样本在$k$特征对应的二阶梯度</p>
<p>s在加权分位缩略图算法中，取值是按照Rank来取的：</p>
<script type="math/tex; mode=display">
r_k(z)=\frac{\sum_{(x,h)∈D_k,x<z}h}{\sum_{(x,h)∈D_k}h}</script><p>输入为某个特征的值$z$,计算的是该特征所有可取值中<strong>小于$z$的特征值的总权重占总的所有可取值的总权重和的比</strong>例，输出为一个<strong>比例值</strong>。 这个Rank计算出来就相当于<strong>将原特征值用$h$加了权映射到$(0,1)$上</strong></p>
<p>于是按照此公式选取候选分割点$\{s_{k1},s_{k2},\cdots,s_{kl}\}$：</p>
<script type="math/tex; mode=display">
|r_k(s_{k,j})-r_k(s_{k,j+1})|<\epsilon\quad,s_{k1}=\min_ix_{ik},s_{kl}=\max_ix_{ik}</script><p>$\epsilon$ 是一个近似比例，或叫扫描步幅，就是说<strong>在加权后的特征取值区间上，按照步幅$\epsilon$取值</strong>，得到$\frac{1}{\epsilon}$个候选点</p>
<p><strong>取值分两种</strong>：</p>
<p>①<strong>global proposal</strong> ：也就是<strong>在最开始树还没分裂的时候就选好一组$\{s_{k1},s_{k2},\cdots,s_{kl}\}$，在总体样本里选择，每次树切分的时候都不变 。</strong></p>
<p>②<strong>local proposal</strong>：它是在每次树<strong>分割成子树之后再选择出一组$\{s_{k1},s_{k2},\cdots,s_{kl}\}$</strong>，来自子树的样本集！</p>
<p><strong>两种都要，共同组成分割候选点集合</strong>。对于子树中存在的global proposal选取的分割点，需要加入子树local proposal选取的分割点集合中，组成子树候选点</p>
<p><strong>[总结]</strong>如何分割成子树：</p>
<ol>
<li><p>求global proposal和local proposal，得到一系列候选分裂点集合$\{s_{k1},s_{k2},\cdots,s_{kl}\}$（加权分位数略图算法）</p>
</li>
<li><p>候选点将样本集合划分为了很多区间，每个区间叫分桶。在每个分桶上对样本统计值$g$、$h$进行累加统计求$Gain$，最后在这些累计的统计量上寻找最佳分裂点。（这步其实就是使用增益公式遍历选最佳分裂点的步骤）</p>
</li>
</ol>
<p>结合贪心，替代上面“修改拟合目标”中的1234暴力枚举法</p>
<p>对于<strong>分布式加权分位数略图算法</strong>，多了两个操作合并merge和修剪prune，可以看这篇文章<a href="http://datavalley.github.io/2017/09/11/xgboost%E6%BA%90%E7%A0%81%E4%B9%8B%E5%88%86%E4%BD%8D%E7%82%B9" target="_blank" rel="noopener">xgboost之分位点算法</a></p>
<h3 id="允许缺失值存在"><a href="#允许缺失值存在" class="headerlink" title="允许缺失值存在"></a>允许缺失值存在</h3><p>XGBoost模型允许缺失值的存在。对于特征值缺失的样本，XGBoost可以学习这些缺失值的分裂方向。</p>
<p>具体地，XGBoost把缺失值当做稀疏矩阵来对待，即本身的在节点<strong>[遍历寻找某特征的最优分裂点时不考虑缺失值的数值，只对非缺失值进行遍历]&lt;=允许缺失值存在的核心</strong>。所以缺失值对树生成不会有影响，缺失值数据会继续被其他特征进行最优划分，最终学习到分类（如果用下面的①可以最优但增加计算负担，用②也可以，实际上影响不大）。</p>
<p>于是两种处理方法：</p>
<p>①将缺失值分到左子树和右子树分别计算损失，选择较优的那个。</p>
<p>②也可以直接设定缺失值默认分类方向。实际处理时，比如可以将缺失值设置成missing=-999等，大大提高运算速度。</p>
<p>如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。 </p>
<p>（在原始的GBDT策略中是要手动对缺失值进行处理的）</p>
<h3 id="计算机系统优化"><a href="#计算机系统优化" class="headerlink" title="计算机系统优化"></a>计算机系统优化</h3><p>这部分略，可以阅读原论文</p>
<h3 id="XGBoost算法总结"><a href="#XGBoost算法总结" class="headerlink" title="XGBoost算法总结"></a>XGBoost算法总结</h3><ol>
<li><p>初始化</p>
</li>
<li><p>迭代计算每一步：</p>
<p>①计算损失函数在当前模型的一阶梯度和二阶梯度：</p>
<script type="math/tex; mode=display">
g_i=\frac{\partial L'(y_i,f(x_i))}{\partial f(x_i)},h_i=\frac{\partial^2 L '(y_i,f(x_i))}{\partial f(x_i)}</script><script type="math/tex; mode=display">
G_j=\sum_{i∈I_j} g_i \quad ,H_j=\sum_{i∈I_j}h_i\quad</script><p>② 通过加权分位数算法选出了一些可能的分裂点。 </p>
<p>根据增益公式计算可能的特征和分裂点，选择最大化该公式的最优特征和最优分裂点，确定新树结构：</p>
<script type="math/tex; mode=display">
Gain=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}]-\gamma</script><p>$\lambda$有正则效果，$\gamma$作为分裂阈值，也起了正则化效果</p>
<p>③根据损失函数极小化确定每个子结点的回归输出</p>
</li>
</ol>
<script type="math/tex; mode=display">
T^o(x;\Theta_{m+1})=-\frac{G_j}{H_j+\lambda}</script><p>​        ④新树乘以学习率，前向分步模型更新</p>
<script type="math/tex; mode=display">
f_{m+1}(x)=f_{m}(x)+\epsilon T^o(x;\Theta_{m+1})</script><p>​    3. 迭代直到满足停止条件</p>
<h3 id="相比GBDT的优点"><a href="#相比GBDT的优点" class="headerlink" title="相比GBDT的优点"></a>相比GBDT的优点</h3><ol>
<li><p>由于XGBoost使用了损失函数的泰勒<strong>二阶展开</strong>，因此<strong>与损失函数更接近</strong>，对其进行拟合，收敛速度更快。</p>
<p>而GBDT只进行了一阶展开。</p>
</li>
<li><p>加入了<strong>正则项</strong>，控制模型复杂度，避免过拟合。正则项$\gamma$一方面在分割的时候相当于做了预剪枝，正则项$\lambda$相当于对leaf score做了平滑作用，这俩都等效于正则化；另一方面在损失函数中起到了正则化</p>
</li>
<li><p>采用<strong>[近似的]最优特征与最优分割点选择方法（加权中位数方法+贪心算法 ）</strong>。</p>
<p><strong>a.</strong>贪心即只考虑当前结点最优特征和最优划分。（贪心算法一般决策树分裂都用，否则找全局最优，决策树枚举完计算量是非常大的）</p>
<p><strong>b.</strong>同时决策树结点在分裂时需要穷举每个可能的分裂点，当数据没法全部加载到内存中时，这种方法会比较慢，XGBoost提出了一种近似的方法去高效的生成候选分割点——加权分位数的方法寻找可能的分裂点。</p>
<p><strong>①可以加速和减小内存消耗</strong></p>
<p>②<strong>支持并行</strong>，在对每棵决策树进行结点分裂时，需要每个特征的增益，选择最大的那个特征作为分裂特征，各个特征的增益计算可以多线程进行</p>
<p>③支持<strong>对缺失值的处理</strong>，对于特征值缺失的样本，XGBoost可以学习这些缺失值的分裂方向。见上</p>
<p>（在原始的GBDT策略中是手动对缺失值进行处理的）</p>
</li>
<li><p>在每一轮更新前向分步模型的时候引入了<strong>学习率</strong>，防止过拟合。学习率可以降低每颗学到的新树的影响。在实际操作的时候可以将学习率设置低一点，迭代次数设置更大。（GBDT也有学习率）</p>
</li>
<li><p>引进了<strong>特征子采样/列抽样 </strong>，像Random Forest那样，既能降低过拟合，还能减少计算。 </p>
</li>
</ol>
<h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><p>原论文： <a href="https://www.microsoft.com/en-us/research/publication/lightgbm-a-highly-efficient-gradient-boosting-decision-tree/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/publication/lightgbm-a-highly-efficient-gradient-boosting-decision-tree/</a> </p>
<p>LightGBM是微软去年才发表并开源的新算法，也是一种<strong>基于GBDT</strong>的算法。其能达到速度比XGBoost快，但精度却一样。</p>
<p><strong>其核心是在GBDT的每个CART分裂子树，选取最优分裂点的方法上进行了改进。</strong>原始GBDT中，即使使用了贪心，要对每个分裂点进行枚举，计算量也非常大。在XGBoost中，使用了加权分位数略图算法来解决这个问题。</p>
<p>但对于GBDT，是没有权重的（没有像XGBoost采用二阶梯度），于是在LightGBM中为了基于GBDT改进，提出了<strong>Gradient-based One-Side Sampling (GOSS)</strong>和<strong>Exclusive Feature Bundling (EFB)</strong>算法来解决最优分割点选取问题</p>
<h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><p>随机森林不属于Boosting方法，但属于集成学习</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol>
<li><p><strong>实际上绝大多数树模型在划分结点的时候都是采用贪心算法，只计算当前结点的最优特征和最优划分，否则树模型的可能实在太多，枚举完的代价是巨大的</strong></p>
</li>
<li><p><strong>Bagging与Boosting</strong></p>
<p>用XGBoost/GBDT在在调参的时候把树的最大深度调成6就有很高的精度了。但是用DecisionTree或RandomForest的时候需要把树的深度调到15或更高，why？</p>
<p>①随机森林属于<strong>Bagging方法，主要降低方差</strong>， 因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。  （dropout就可以视为神经网络的一种bagging）</p>
<p>②XGB/GBDT属于<strong>Boosting方法，主要降低偏差</strong>，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成； </p>
<p>Bagging和Boosting都属于集成学习</p>
</li>
<li><p>分割=分裂=划分，这三个词都是描述决策树变成子树的过程，我是混着用的</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>11月游戏简评</title>
    <url>/2019/11/18/11%E6%9C%88%E6%B8%B8%E6%88%8F%E7%AE%80%E6%8A%A5/</url>
    <content><![CDATA[<p>历来的年末都是各大厂商血拼的时候，今年也不例外。</p>
<p>在死亡搁浅搁浅之后，还以为今年的GOTY会陷入无人可领的地步，没想到横空杀出了个大黑马，星球大战：绝地真的是把我惊艳到了，或者说震惊到了。</p>
<h2 id="星球大战：香"><a href="#星球大战：香" class="headerlink" title="星球大战：香"></a>星球大战：香</h2><h3 id="星球大战：绝地武士陨落"><a href="#星球大战：绝地武士陨落" class="headerlink" title="星球大战：绝地武士陨落"></a>星球大战：绝地武士陨落</h3><p>集各家游戏之所长，继承泰坦陨落的灵魂，以星战之衣钵，重生工作室又一次给玩家们交了一张几近满分的答卷（不是满分是因为发行商名字没取对，改名某某某就满分了）。我在这个游戏里看到了塞尔达，黑魂只狼，神海等等各家游戏的优点，又结合了自家出色的手感（可重生以前打击感做得好都是FPS的枪感好，并没有什么ACT的打击感开发经验，这是怎么做到一出手就如此优秀的？incredible. 仁王好好学学别人！！）</p>
<p>当然也有不少缺点。可以看出来这个游戏还是有很多地方欠打磨，比如各种神奇的穿模和bug…而且第一章画面辣眼睛（但第二章这EA的画面不是又回来了嘛！P.S.然后掉帧也回来了），多敌人战斗的时候有时候视角很奇怪等等</p>
<p>其实要认真来说，星球大战并没有一个非常突出的地方，但是为何如此之香？就是因为它的各方面不突出，但都很良好，也就是说基本没有短板，所以给玩家的体验非常好。</p>
<p>地图设计非常大而充实，可探索要素充足，而且地图也四通八达。</p>
<a id="more"></a>
<p>配乐和音效是原汁原味的星战味，但音乐有点偏少，尤其缺少战斗音乐，光听见切肉和光剑愣愣愣的声音了</p>
<p>战斗部分深度并不高，但很爽。由于原力机制和判定并不严苛的防反存在，再厉害的怪也不会太难，作为一个ARPG已经完全足够了。</p>
<p>战斗的打击感很到位，光剑战斗真的是视觉和听觉的盛宴</p>
<p>画面优良。引擎使用的是虚幻4而不是寒霜，实在是非常遗憾。第一章画面不行，但到了第二章，画面又好了起来。据开发组说，最开始使用寒霜引擎遇到了很多致命的麻烦，因为寒霜引擎是专门为FPS等游戏设计的，本作中有很多攀爬等元素，得自己改引擎，最后走到死路，改用虚幻开发。</p>
<p>解谜和关卡设计优秀，可以看出借鉴了很多优秀的游戏设计，完美融合进了星战里。我甚至不敢相信这个关卡设计是出自EA之手，非常用心的解谜设计，是一个大大的加分项。</p>
<p>剧情总体平稳，介于我还没来得及通关，就先不扯淡了。</p>
<p>总结一下：好看又好玩，这样的游戏一定不会差。</p>
<h3 id="重生，让EA重生"><a href="#重生，让EA重生" class="headerlink" title="重生，让EA重生"></a>重生，让EA重生</h3><p>EA这几年来走的并不顺利，很多工作室的表现都令人大跌眼镜，也许是内部管理的原因，这里不过多谈论。但是重生工作室的表现一直很稳定，前有TTF，APEX等佳作，EA也顺势将星球大战这等重量级的游戏交给重生开发。它也不负众望，交给EA也交给玩家们一张完美的答卷。</p>
<p>希望EA借着这一仗口碑的胜利，凤凰涅槃，继续下去，加油！</p>
<p>又回过头来说说死亡搁浅</p>
<h2 id="拿3A游戏来做革新，也只有小岛秀夫敢这么干"><a href="#拿3A游戏来做革新，也只有小岛秀夫敢这么干" class="headerlink" title="拿3A游戏来做革新，也只有小岛秀夫敢这么干"></a>拿3A游戏来做革新，也只有小岛秀夫敢这么干</h2><p>众所周知，3A游戏的制作成本是非常昂贵的，也因为成本太高，很少有3A游戏厂商敢拿3A游戏来开刀革新的。所以我们看到很多非常有创意的游戏都是独立游戏或者以前的老游戏（在当时很有创意）。</p>
<p>但在死亡搁浅中，小岛秀夫把3A游戏的核心玩法拿来完全革新，而且这种创新不像战神4之于战神3对自我的革新，因为那是有路子可寻的，而死亡搁浅却是创造了一种全新的核心玩法。</p>
<p>这在游戏界，恐怕只有小岛秀夫敢这么干了——一个个人声望大过集体，游戏制作界的明星人物</p>
<p>下面谈谈死亡搁浅</p>
<h3 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h3><p>死亡搁浅的核心玩法简要的说就是共建。整个服务器的玩家一起共同在一片宽旷且到处都是阻碍的地面上建设设施，一起重建世界，玩家之间可以共享资源以及自己建设的设施，玩家之间可以相互为对方建的设施点赞。</p>
<p>这很好玩吗？不一定。但是如果你云通关，一定感受不到其中的乐趣。</p>
<p>这个点赞，其实就是Connect设计中的反馈，而且是一种正向的反馈。在FPS中，你杀人之后，你也会获得一种反馈，但这种反馈就是负面反馈，你每杀一个人，就会得到一次反馈，激励你杀更多的人。</p>
<p>死亡搁浅中的正向反馈就是你在游戏里干的事情，会被其他玩家感受到，其他玩家感受到你的帮助之后，会给你点赞。当你受到点赞的时候，就会有“我做的事情是有意义的，带给了其他人帮助”的感受。这种感受是一种很美妙的感觉，确实会促进玩家再去更多地为世界做建设——于是千千万万的connect就构建起了死亡搁浅的世界。</p>
<p><del>这是什么精神？这是社会主义精神！Make American Socialism Again！</del></p>
<h3 id="共建的核心玩法能否支撑起剧情与设定的厚度？"><a href="#共建的核心玩法能否支撑起剧情与设定的厚度？" class="headerlink" title="共建的核心玩法能否支撑起剧情与设定的厚度？"></a>共建的核心玩法能否支撑起剧情与设定的厚度？</h3><p>这个问题有很多答案。有的玩家说有，有的玩家说没有（云玩家除外）。其实这个问题要回答很简单，那就是你是否喜欢这个玩法？如果喜欢，那它一定能支撑起这整个游戏，你会感觉到予人赞，其乐无穷，你会给这个游戏打9分甚至10分。但如果你不喜欢这个玩法，会感觉这个游戏的游戏性非常空洞，无聊，6.8分不能更多。</p>
<p><strong>所以答案是，让时间证明吧。</strong></p>
<p>一个新的游戏类型的提出，一般都不会立刻被所有玩家接受，就像小岛秀夫最开始创造潜行类游戏的时候一样。死亡搁浅也才发售不到半个月，未来，这种玩法会不会被广大玩家接受，甚至掀起潮流，谁知道呢？</p>
<h3 id="电影性"><a href="#电影性" class="headerlink" title="电影性"></a>电影性</h3><p>小岛秀夫作为电影化游戏的开拓者，可以看出他一直致力于在模糊电影和游戏的界限。游戏里用了很多著名的演员，音乐也是到了游戏里的每一首歌都能单曲循环的程度。</p>
<p>有的玩家很喜欢这样，但有的玩家不喜欢，这就是游戏界很常见的争论——<strong>游戏性与艺术性之争</strong></p>
<p>这个以后专门开一篇来写好了。</p>
<h2 id="GF：我们重质量不重数量"><a href="#GF：我们重质量不重数量" class="headerlink" title="GF：我们重质量不重数量"></a>GF：我们重质量不重数量</h2><p>神奇GF重新定义“质量”。</p>
<p>这里我就打个滑稽，以下一万字略</p>
<p><img src="//aisakaki.com/2019/11/18/11月游戏简报/1.png" alt="1"></p>
<h2 id="XGN？"><a href="#XGN？" class="headerlink" title="XGN？"></a>XGN？</h2><p>你说的这个某GN，它厉害吗？厉害。可信吗？不好说。</p>
<p>经过多年观察，我发现某GN有几个明显的特点：</p>
<p>①喜欢同一系列作纵向，如果前作一般，但这次大大超越了前作，会给很高的分</p>
<p>②对独立游戏异常宽容</p>
<p>③不同类型的游戏，给分的标准完全不同</p>
<p>④偏向给游戏性而非艺术性的游戏高分</p>
<p>⑤不同编辑口味差别很大，甚至前后矛盾</p>
<p>⑥打游戏打到心情不好会情绪化（比如Prey，真的就是彻底的不负责任）</p>
<p>⑦一般是以一种涉猎甚广的老玩家眼光来评价游戏</p>
<p>所以其实，IGN的评分重要吗？在这么多主观且漂浮不定的标准下，IGN能做到完全公平公正客观吗？</p>
<p>再者，什么是客观？连艺术性和游戏性这两大流派之间都还没争出个谁更重要，IGN又凭什么说是什么就是什么呢？</p>
<p>所以我一直认为，这种“客观”机构的打分机制就应该被革除，这种因为一个主观的评分，就能影响整个风向的机构，就应该消失。IGN的编辑最多也就是一个“身经百战，见得多了的老玩家”而已，他只能代表他这个人的看法，说出他的感受，<strong>但他并没有权力给游戏定分，定性</strong></p>
<hr>
<p><em>此文同步发在社团公众号</em></p>
]]></content>
      <categories>
        <category>Game</category>
      </categories>
  </entry>
  <entry>
    <title>Boosting(AdaBoost、DBT、GBDT)</title>
    <url>/2019/11/17/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p><em>Boost算法在我的黑白判项目，动态项目都有用到，而且在实际中也非常常用，是一种非常有效的算法</em></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p><strong>思想：</strong>对于一个训练集，弱分类器可以比较容易地学习到比较粗糙的分类问题，而一个复杂的分类问题很难学习。于是我们想到<strong>将复杂的分类问题分而治之</strong>，由<strong>每个</strong>弱分类器负责学习一个相对比较简单的分类问题，于是<strong>每个弱分类器各有所长</strong>，<strong>将多个弱分类器组合</strong>成一个强分类器，就可以解决复杂分类问题。</p>
<p><strong>具体地，</strong>我们<strong>在训练集上迭代学习多个弱分类器</strong>，<strong>每一次迭代学习出一个弱分类器</strong>，这个弱分类器对<strong>部分样本</strong>有较好的分类作用，而对有些样本分类效果很差。于是我们加大分类效果差的样本的权值，降低分类效果好的样本的权值，以学习一个新的弱分类器来分开它们（<strong>在整个训练集上学习</strong>），它能够较好的分出这些效果较差的样本中的<strong>一部分</strong>样本。于是为了分开那一部分类效果差的样本，继续迭代$\cdots\cdots$最后以每一个分类器的分类效果为依据，设计弱分类器之间的组合，最终得到强分类器</p>
<p><strong>根据这个思想，Boost方法要考虑两个问题：</strong></p>
<p>①每一轮<strong>训练数据的权值</strong>（广义的权值，即每个样本重要性）如何改变</p>
<p>②<strong>如何将弱分类器组合</strong>成一个强分类器</p>
<a id="more"></a>    
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><p>Adaptive Boost 适应性提升算法</p>
<p>①每一轮训练数据的权值如何改变：根据分类器效果，样本权值直接改变</p>
<p>②如何将弱分类器组合成一个强分类器：分类器加权相加</p>
<h3 id="AdaBoost-Algorithm"><a href="#AdaBoost-Algorithm" class="headerlink" title="AdaBoost Algorithm"></a>AdaBoost Algorithm</h3><p>$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，$y_i∈\{-1,1\}$</p>
<p>开始进行<strong>第一轮迭代</strong>，$m$为迭代轮数</p>
<p><strong>①</strong>初始训练集<strong>每个样本</strong>的权值平均分布，学得第一个弱分类器$G_1$</p>
<p>$D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N})\quad,w_{1i}=\frac{1}{N}\quad,i=1,2,\cdots,N$</p>
<p>$G_1(x):\mathbb X\to\{-1,+1\}$</p>
<p><strong>②</strong>计算<strong>分类误差率</strong>$e_1$</p>
<script type="math/tex; mode=display">
e_1=\sum_{i=1}^N P(G_1(x_i)≠y_i)=\sum_{i=1}^N w_{1i}I(G_1(x_i)≠y_i)</script><p><strong>③</strong>通过$e_1$计算<strong>弱分类器$G_1$的权值$\alpha_1$</strong></p>
<p>这个系数既用来作为分类器权重，也用来计算训练集权值</p>
<script type="math/tex; mode=display">
\alpha_1=\frac{1}{2}\log\frac{1-e_1}{e_1}</script><p>这个公式恰好使得在$e=\frac{1}{2}$的时候，$\alpha=0$；$e$越大，则误差越大，就让权值$\alpha$越小。</p>
<p>由于二分类问题的最差情况就是正确率$e≥50\%$，所以这个式子实际上就是使得$\alpha$的值域为$[0,1)$</p>
<p><strong>④</strong>通过$\alpha_1$计算<strong>新的训练集每个样本的权值</strong>，每个样本用$w_{2i}$表示，在权值为$w_2$的训练集（<strong>依然是在整个训练集上学习</strong>）上学得第二个弱分类器$G_2$</p>
<script type="math/tex; mode=display">
w_{2i}=\frac{1}{Z_1}w_{1i}e^{-\alpha_1y_iG_1(x_i)}</script><p>权值为概率，之和必为1，要归一化</p>
<p>其中$y_iG_1(x_i)=c(u)=\begin{cases} 1\quad,y_i=G_1(x_i)\\ -1\quad,y_i≠G_1(x_i) \end{cases}$。也就是说，[A]如果预测准确，指数部分为负，新的权值减小；如果预测[B]如果预测不准确，指数部分为正，新的权值增加。增加和减少都是$e^{\alpha_1}$倍</p>
<p><strong>⑤</strong>进入<strong>第二轮迭代</strong>，计算$e_2,\alpha_2,G_{3}$</p>
<p><strong>⑥</strong>$\cdots$，进入第$m$轮迭代，迭代计算$e_m,\alpha_m,G_{m+1}$</p>
<p><strong>⑦</strong>迭代结束后，得到强分类器</p>
<script type="math/tex; mode=display">
G(x)=sign(\sum_m\alpha_mG_m(x))</script><hr>
<p>AdaBoost算法的训练误差分析：<strong>略</strong></p>
<h2 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h2><p><strong>具体算法与公式略</strong></p>
<p>思想：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。</p>
<p>AdaBoost算法的前向分布算法解释：AdaBoost算法是前向分步算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数就是指数函数（<strong>证略</strong>）</p>
<p>Boost方法实质上是使用了加法模型与前向分步算法。</p>
<p>前向分步算法每一步相当于AdaBoost中的每一轮（但具体细节会有差别）</p>
<p>前向分步算法的求解本质是<strong>贪心方法</strong></p>
<p>更多地可以通过提升树来理解，提升树就是直接用的前向分布算法。</p>
<h2 id="BDT-提升树"><a href="#BDT-提升树" class="headerlink" title="BDT 提升树"></a>BDT 提升树</h2><p>学习DBT需要对CART有深刻掌握</p>
<p>提升树是以决策树为基函数的提升方法。表示为决策树的加法模型：</p>
<script type="math/tex; mode=display">
f_{M(x)}=\sum_{m=1}^MT(x;\Theta_m)</script><p>$T(x;\Theta_m)$表决策树，$\Theta_m$为决策树的参数，$M$为树的个数</p>
<p>树的线性组合可以很好地拟合训练数据，即使输入与输出之间的关系很复杂也如此，于是提升树是一个很高功能的学习算法</p>
<p>于是根据前向分步算法，<strong>拟合初始训练数据集</strong>学得初始分类器$f_0(x)=T(x;\Theta_0)$，</p>
<p>第$m+1$步的模型为：$f_{m+1}(x)=f_{m}(x)+T(x;\Theta_{m+1})$$\quad ,f_{m}(x)$即为当前第$m$步的已有模型，<strong>由上一步第$m-1$步学到</strong></p>
<p>于是通过<strong>ERM</strong>确定下一个决策树的最优参数：$\hat \Theta_{m+1}=\arg\min_{\Theta_{m+1}}\sum_{i=1}^NL(y_i,f_{m}(x_i)+T(x_i,\Theta_{m+1}))$</p>
<p>根据不同的问题，使用不同的损失函数；回归问题用平方误差，分类问题用指数损失函数；一般决策问题用一般损失函数</p>
<p><strong>①二分类问题：线性分类器为二类分类树（CART分类树）的AdaBoost</strong></p>
<p><strong>②回归问题</strong>：<strong>基函数是CART回归树</strong></p>
<p>对第$m$步</p>
<p>采用平方误差损失，则$L(y_i,f_{m}(x_i)+T(x,\Theta_{m+1}))=(y_i-f_{m}(x_i)-T(x_i;\Theta_{m+1}))^2$</p>
<p>则第$m$步（轮）第$i$样本的残差$r_{mi}=y_i-f_{m}(x_i)$，这个残差算的当前已有模型（由上一步计算出的当前步）的残差，于是我们要求出<strong>下一个新的决策树</strong>$T(x;\Theta_{m+1})$，就是去<strong>拟合这个残差，以使得损失函数最小化</strong>，求得最优参数的决策树$T(x;\hat\Theta_{m+1})$，也可以求出其输出值（回归值，具体怎么求出参考决策树章节）</p>
<p>然后更新分类器$f_{m+1}(x)=f_{m}(x)+T(x;\Theta_{m+1})$  ，完成一次迭代</p>
<p>然后计算下一步$m+1$的残差，拟合下下步$m+2$的模型，更新线性模型$\cdots\cdots$</p>
<p>最后得到的回归问题提升树就为$f_{M(x)}=\sum_{m=1}^MT(x;\Theta_m)$</p>
<p>具体算法略</p>
<p><strong>注意：</strong></p>
<p>①分类器<strong>$f_m(x)$函数是用于迭代作用的，最后组合强分类器的时候和它无关，是将每一步迭代得到的决策树$T(x;\Theta_m)$拿来组合</strong>。</p>
<p>②为什么要每轮都要<strong>累加分类器</strong>，计算$f_m(x)$而不像AdaBoost一样每步直接用新学得的弱分类器进行分类？因为<strong>除了第一步求得的决策树是拟合训练数据集本身以外，以后每一步求得的决策树拟合的都是残差！而不是训练数据本身！举例：</strong>原始训练集 $(2,3,4,5)$，假设拟合得初始决策树：$\begin{cases} 2.5\quad,y_i＜3.5\\ 4.5\quad,y_i≥3.5 \end{cases}$，于是求得第一步的残差$r$为$(-0.5,0.5,-0.5,0.5)$，然后拟合这个残差求得下一步的决策树。<strong>所以用$f_m(x)$累加后才是分类器，因为其包含了初始训练集的完整数据，而不是仅是根据残差求得的那个决策树。</strong>（这个决策树只对当前步的残差起决策作用）</p>
<p>所谓A拟合B就是让A更接近B</p>
<p>总结（回归问题）：</p>
<p>①每一轮训练数据的权值如何改变：不直接设置权值，而是计算残差</p>
<p>②如何将弱分类器组合成一个强分类器：拟合残差得到的决策树相加</p>
<p>BDT的求解过程一般化就是前向分步算法的求解过程，需要理解，以便后面变形方便使用</p>
<h2 id="GBDT-梯度提升决策树（回归）"><a href="#GBDT-梯度提升决策树（回归）" class="headerlink" title="GBDT 梯度提升决策树（回归）"></a>GBDT 梯度提升决策树（回归）</h2><h3 id="GDBT-Algorithm"><a href="#GDBT-Algorithm" class="headerlink" title="GDBT Algorithm"></a>GDBT Algorithm</h3><p>依然基于CART分类器，是<strong>回归问题提升树的改进</strong></p>
<p>像前面的平方，指数损失函数都很容易最优化，但对于一般损失函数，如huber损失、quantile损失，每一步求最优化都不容易。于是使用<strong>梯度提升（不是梯度上升！梯度依然是下降以求最小损失函数）算法来求解损失函数最优化问题</strong>，其是<strong>最速下降法</strong>（搜索最佳步长的梯度下降法）的近似方法。</p>
<p><strong>在提升树基础上修改：</strong>利用损失函数的<strong>[负梯度在当前模型的值]</strong>替代回归问题提升树算法中的<strong>残差</strong></p>
<p>GBRT 几乎可用于所有的回归问题（线性/非线性） 问题O</p>
<hr>
<p><strong>过程：</strong></p>
<p>$m$代表步（轮）数</p>
<p><strong>①初始化：</strong></p>
<script type="math/tex; mode=display">
f_0(x)=\arg\min_c\sum_{i=1}^NL(y_i,c)</script><p><strong>②对该步（轮）每个样本计算[负梯度在当前模型的值=残差的估计]为：</strong>（②其实为③的一个步骤）</p>
<script type="math/tex; mode=display">
r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m}(x)}</script><p>推导见下面【②中残差的推导】</p>
<p><strong>③拟合上面计算出的每个样本的[残差]，得到下一步用的新决策树的[结构]：</strong></p>
<p>用这个新决策树<strong>对残差</strong>学习得到<strong>叶结点区域</strong>（划分）$R_{mj}\quad,j=1,2,\cdots,J$</p>
<p><strong>这里其实就是</strong>极小化损失函数$L(y_{i+1},f_{m+1}(x_i))=L(r_{mi},T(x=特征点与切割点;\Theta_{m+1}))$（中间推导就是残差的推导那儿），以选取新决策树的最优特征和最优划分</p>
<p><strong>[此时只是确定了树的结构，但是还未确定叶子节点中的最优输出值，下一步④中通过最小化损失函数:</strong></p>
<p><strong>$L(y_i,f_{m}(x_i)+c)$，求得每个叶子节点中的输出值，和CART中的算法一样，不同的就在于下面求最优输出值的时候是让输出而不是残差去拟合$y_i$ ，也就是损失函数不一样]</strong> 。更多细节见下面的【注意理解】</p>
<p><strong>④</strong>对<strong>每个划分</strong>用线性搜索叶结点区域的最优值（最速下降法），使得<strong>损失函数最小化</strong>：</p>
<script type="math/tex; mode=display">
c_{mj}=\arg\min_c\sum_{x_i∈R_{mj}}L(y_i,f_{m}(x_i)+c)</script><script type="math/tex; mode=display">
更新：f_{m+1}(x)=f_{m}(x)+\sum_{j=1}^Jc_{mj}I(x∈R_{mj})</script><p>注意这里的表示方法。$I(x∈R_{mj})$就表示了新学得的决策树输出的结构，乘上该区域结点输出值$c_{mj}$，此即在当前步下最终学到的下一步用的新决策树$\sum_{j=1}^Jc_{mj}I(x∈R_{mj})$</p>
<p><strong>⑤最终的强分类器为：</strong></p>
<script type="math/tex; mode=display">
\hat f(x)=f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x∈R_{mj})</script><p>于是新的决策树就去拟合这个残差，其它不变。</p>
<hr>
<p><strong>【注意理解】：这里需要好好回想生成CART回归树的步骤</strong>。将CART回归树表达式中是损失函数扩展为任意，则：</p>
<p>$\min_{j,s}[\min_{c_1}\sum_{x_i∈R_1(j,s)}L(y_i,c_1)+\min_{c_2}\sum_{x_i∈R_2(j,s)}L(y_i,c_2)]$</p>
<p>③中就相当于通过拟合残差（也就是该式子损失函数中的拟合$y_i$替换为拟合$r_i$）先求了CART回归树的外围参数最小值$j,s$，即<strong>特征选择</strong>和<strong>划分</strong>（回看CART回归生成，遍历选择$j,s$<strong>最小化损失函数选最优特征和划分</strong>）</p>
<p>④中就是求在划分下，使得损失函数最小的$c$，而这个$c$就是对应划分的残差的回归输出（通过求该划分下的平均值求得，为CART中损失函数中的模型输出值），也就是上面所提到的叶结点区域的值。（回看CART回归生成，在一个划分（结点）内<strong>最小化损失函数选择一个最优的输出</strong>，使得该划分每个样本的损失最小）</p>
<p>由于前面求得决策树拟合的是残差（除了第一步决策树是拟合的训练集）以得到最优划分结构；下面要去拟合<strong>训练集的真实输出</strong>还要让新决策树的输出加上从第一颗决策树输出与中间决策树拟合残差输出迭代而来的前$m$步模型输出$f_{m}(x_i)$（第$m$轮），所以某划分的模型实际回归输出为$f_{m}(x_i)+c$。（前向分步算法的基本求法。）<strong>所以我们在每轮（第一轮除外）拟合决策树求划分的时候，是拟合残差（③）；但是在求划分得到的叶结点区域的输出值的时候，我们是要去拟合[真实输出值$y_i$]而不是残差，所以是用$f_{m}(x_i)+c$去拟合！得损失函数极值及极值的时候的$c$（④）可以看出③和④损失函数目标不一样。</strong>（但是同一个损失函数）</p>
<hr>
<p><strong>【②中残差的推导】：</strong>损失函数为（这里是单个样本的损失函数，多个样本加个求和符号即可，求出的每个样本的残差结论不变）</p>
<p>$L(y_{i+1},f_{m+1}(x_i))=L(y_{i+1},f_{m}(x_i)+T(x,\Theta_{m+1}))$，$y$为常数（样本真实输出）</p>
<p>令$x’=\hat f_{m}(x_i)+T(x,\Theta_{m+1})$，对于在$x_0$的泰勒展开：</p>
<p>$f(x)=f(x_0)+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$</p>
<p>(此$x’$即为泰勒展开式形式的自变量，为了区分损失函数中的$x$)</p>
<p>则该损失函数<strong>在当前模型$\hat f_{m}(x_i)$处</strong>进行<strong>一阶泰勒展开</strong>：</p>
<p>$L(y_{i+1},f_{m+1}(x_i))=L(y_{i+1},f_{m}(x_i)+T(x,\Theta_{m+1}))≈L(y_{i+1},\hat f_{m}(x_i))+\frac{\partial L(y_{i+1},\hat f_{m}(x_i))}{\partial f_{m}(x_i)}(x’-\hat f_{m}(x_i))$</p>
<p>$=L(y_{i+1},\hat f_{m}(x_i))+\frac{\partial L(y_{i+1},\hat f_{m}(x_i))}{\partial f_{m}(x_i)}T(x,\Theta_{m+1})$</p>
<p>所以要使损失函数最低，那么按照梯度下降思想，我们求出梯度，然后只需要令损失函数向梯度反方向增加即可。所以我们让损失函数对要优化的变量$T(x,\Theta_{m+1})$求导（$L(y_{i+1},\hat f_{m}(x_i))$显然为无关项）：</p>
<p>$\nabla L(y_{i+1},f_{m+1}(x_i))=\frac{\partial L(y_{i+1},f_{m+1}(x_i))}{\partial T(x,\Theta_{m+1})}=\frac{\partial L(y_{i+1},\hat f_{m}(x_i))}{\partial f_{m}(x_i)}$，于是我们就要让函数在：</p>
<p>$r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m}(x)}$上梯度下降，该负梯度的在当前模型的值即为残差的估计。</p>
<hr>
<p>残差其实是梯度提升中负梯度的一种特例，就是导数的近似值，将损失函数换为平方差损失可以推导出</p>
<p>具体算法略</p>
<h3 id="GDBT分类算法O"><a href="#GDBT分类算法O" class="headerlink" title="GDBT分类算法O"></a>GDBT分类算法O</h3><p>如果样本输出类别为$k$，则$y_k=1$；$p_k(x)$表示模型$f(x)$判定$x$属于第$k$类的概率：$p_k(x)=\frac{e^{f_k(x)}}{\sum_{l=1}^Ke^{f_l(x)}}$</p>
<p>注意，对于多分类问题，回归树训练时，会为每一个类别训练一个决策树。</p>
<h3 id="GDBT的正则化"><a href="#GDBT的正则化" class="headerlink" title="GDBT的正则化"></a>GDBT的正则化</h3><ol>
<li>与AdaBoost一样，每个决策树乘上一个弱化系数</li>
<li>对CART树剪枝，降低CART树复杂度</li>
<li>采用子采样，每次随机抽取部分样本（SGBT）</li>
</ol>
<h3 id="GDBT-损失函数选择"><a href="#GDBT-损失函数选择" class="headerlink" title="GDBT 损失函数选择"></a>GDBT 损失函数选择</h3><p>OReason</p>
<ol>
<li><p>分类问题 </p>
<p>指数损失函数、对数损失函数</p>
</li>
<li><p>回归问题</p>
<p>均方差损失函数、绝对损失函数</p>
</li>
<li><p>huber损失函数和分位数（quantile）损失函数，也用于回归问题，可以增加回归问题的健壮性，减少异常点对损失函数的影响</p>
</li>
</ol>
<h3 id="GDBT的优缺点"><a href="#GDBT的优缺点" class="headerlink" title="GDBT的优缺点"></a>GDBT的优缺点</h3><p>可以处理各种类型的数据，预测准确率高，使用huber和分位数损失函数可以增加回归问题的健壮性（Huber函数 在值为0时也是可微分的 ）</p>
<p>但是由于基学习器存在依赖关系，难以并行化处理，不过通过子采样的SGBT来实现部分并行 </p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol>
<li>$I(True)=1,I(False)=0$</li>
<li>更多资料可以看此文 <a href="https://blog.csdn.net/zhang15953709913/article/details/84586592" target="_blank" rel="noopener">https://blog.csdn.net/zhang15953709913/article/details/84586592</a> </li>
<li>梯度下降法/最速下降法中用的就是加上负梯度（相当于减去梯度）以缩小损失函数，加梯度就越来越大了</li>
<li>这里有一个GDBT回归问题求解的一个例子： <a href="https://blog.csdn.net/zpalyq110/article/details/79527653" target="_blank" rel="noopener">https://blog.csdn.net/zpalyq110/article/details/79527653</a> ，也可以作为自己练习O</li>
<li>表记轮数/步数下标的时候我都是观测的$m,m+1$，可以各自减一改为观测$m-1,m$</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2019/11/13/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2><p>机器学习最基本模型之一，DT</p>
<p>决策树可以看成路径的集合，路径的集合<strong>可以看成$if-then$规则的集合</strong>，内部结点表示特征或属性，叶结点表示一个类别（结论）。决策树的$if-then$规则集合是<strong>互斥且完备</strong>的（每一个实例都被包含在DT里）</p>
<p>决策树也可以对应于一个<strong>条件概率分布</strong></p>
<a id="more"></a>    
<hr>
<p>数据集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$</p>
<p>其中$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$为输入实例（特征向量），$n$为特征个数，$y_i∈\{1,2,\cdots,K\}$为类别标记，</p>
<p>$i=1,2,\cdots,N$，$N$为样本容量。</p>
<p><strong>目标</strong>就是学习到这个$if-then$规则集合，同时与训练数据矛盾较小（不欠拟合），又可以很好的预测位置数据（不过拟合）。DT学习算法是对训练数据分割的过程，也是特征空间划分的过程，也是决策树的构建过程。</p>
<p><strong>DT算法一般三大步骤①特征选择②决策树生成③剪枝</strong></p>
<p><strong>剪枝</strong>的目的是防止过拟合（剪掉了树上过于细分的结点，使其退回到父节点或更高的结点）</p>
<p>DT是一种启发式算法，求得的是近似最优解</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>特征用来划分特征空间，对数据进行实际分类。我们通过<strong>信息增益</strong>或<strong>信息增益比</strong>来选择特征。</p>
<p>$P(X=x_i)=p_i\quad,P(X=x_i,Y=y_j)=p_{ij}\quad,i=1,2,\cdots,n;j=1,2,\cdots,m$</p>
<p><strong>随机变量$X$的熵</strong>：</p>
<script type="math/tex; mode=display">
H(X)=-\sum_{i=1}^np_i\log p_i</script><p>熵表达了随机变量$X$的不确定性</p>
<p>由于<strong>只依赖$X$的分布</strong>而与$X$的取值没关系，则也可表示为$H(p)=-\sum_{i=1}^np_i\log p_i$</p>
<p>(对于$X=0,1$的熵表达式可以写为$H(p)=-p\log_2p-(1-p)\log_2(1-p)$)</p>
<p>给定$X$下随机变量<strong>$Y$的条件熵</strong>即为条件概率分布的熵$H(Y|X=x_i)$对$X$分布的<strong>数学期望</strong>（给每个$X_i$下的熵加了一个权值）：</p>
<script type="math/tex; mode=display">
H(Y|X)=\sum_{i=1}^nP(X=x_i)H(Y|X=x_i)</script><p>条件熵表达了在已知条件$X$下随机变量$Y$的不确定性</p>
<h3 id="信息增益-Information-Gain"><a href="#信息增益-Information-Gain" class="headerlink" title="信息增益(Information Gain)"></a>信息增益(Information Gain)</h3><p><strong>信息增益(Information Gain)</strong>：表示<strong>如果得知</strong>特征$X$的信息而使得<strong>类$Y$的信息</strong>的<strong>不确定性减少的程度</strong></p>
<p>对于决策树，<strong>特征$A$对训练数据集$D$的信息增益$g(D,A)$为：</strong></p>
<script type="math/tex; mode=display">
g(D,A)=H(D)-H(D|A)</script><p>$H(D)$为数据集的经验熵，$H(D|A)$为条件经验熵（$H(Y)-H(Y|X)$叫互信息，与决策树中的信息增益等价）</p>
<p>很明显可以理解，<strong>某特征的信息增益越大，该特征分类能力越强</strong>（因为IG越大，就代表我能够因此获得的确定性越多，也就是分类越好）</p>
<h3 id="信息增益比-Information-Gain-Ratio"><a href="#信息增益比-Information-Gain-Ratio" class="headerlink" title="信息增益比(Information Gain Ratio)"></a>信息增益比(Information Gain Ratio)</h3><p><strong>信息增益比(Information Gain Ratio)：</strong></p>
<script type="math/tex; mode=display">
g_R(D,A)=\frac{g(D,A)}{H_A(D)}\quad，H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}</script><p>其中$n$为特征$A$的取值个数，$H_A(D)$为训练数据集$D$关于特征$A$的值的熵（$\frac{1}{H_A(D)}$是一个<strong>惩罚系数</strong>，<strong>将特征$A$的取值作为随机变量，求的是特征$A$的熵</strong>，具体见下面-理解）</p>
<p><strong>信息增益存在偏向选择取值较多的特征的问题，于是信息增益比可以对这一问题进行校正。</strong></p>
<hr>
<p><strong>对于实际训练集</strong>：如果由数据估得则为<strong>经验熵</strong>，<strong>经验条件熵</strong>。那么设定$D$为数据集，$C_k$是类别为第$k$的样本集（<strong>真实标签</strong>），总共有$K$个类别；根据特征$A$可以将数据集划分为$n$个子集。$D_1,D_2,\cdots,D_n$（<strong>根据特征判断的标签</strong>），第$i$个子集即为$D_i$，$D_{ik}$表示$D_{i}$中同属于类$C_k$样本集的样本即$D_{ik}=D_i∩C_k$（也就是该类别中标签打对了的样本），则：</p>
<script type="math/tex; mode=display">
H(D)=-\sum_{i=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}</script><script type="math/tex; mode=display">
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{i=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}</script><p>代入信息增益即可求得特征$A$对数据集$D$的信息增益</p>
<h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><p>这里的$H(D)$表达了<strong>数据集</strong>整体的不确定性，随机变量为$p(第i类的样本集)$；</p>
<p>条件熵中求的依然是<strong>数据集</strong>的熵，只是先让特征$A$进行了划分，而不是特征的熵。我们在求被$A$划分成的第$i$个$H(D_i)$的时候，$H(D_i)=-\sum_{i=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$。注意$H(D_i)$的<strong>随机变量</strong>是<strong>被$A$划分之后的条件下</strong>（这里就是条件熵多的东西）<strong>第$i$个数据集中属于$k$类的概率。</strong></p>
<p>条件熵$H(D|A)$即表达了在用特征$A$对$D$进行划分为$D_1,D_2,\cdots,D_n$，所有$D_i$的不确定性之和，即每个被$A$划分为$D_i$后的熵的和。然后再<strong>在求和的时候乘了一个权值</strong>——$A$取该第$i$个划分的概率（也就是期望，此即条件熵）。</p>
<p>不确定性即为我们对数据分类的预测的不确定性</p>
<p>$P(A取第i个取值)=\frac{|D_i|}{|D|}$</p>
<p>特征的熵$H_A(D)$：特征$A$的取值作为随机变量。通过数据集中被$A$打为第$i$个标签的数量与总数量的比即可求得特征$A$打第$i$个标签的概率，因此可以求得特征$A$的熵；见上面的IGR的惩罚系数</p>
<p><strong>注意在计算每个子数据集$D_i$的经验熵时，随机变量为$D_i$中$D_{ik}$的概率，即$D_i$中属于$k$类（真实标签）的样本的概率。凡是求数据集的熵，随机变量必然都是该数据里某类（真实标签）的概率。</strong></p>
<h3 id="基尼指数-Gini"><a href="#基尼指数-Gini" class="headerlink" title="基尼指数(Gini)"></a>基尼指数(Gini)</h3><p>对于IGR，当特征取值较少时$H_A(D)$的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。 于是引入基尼指数。</p>
<p>分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：</p>
<script type="math/tex; mode=display">
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2</script><p>对于二分类问题，则为$Gini(p)=2p(1-p)$</p>
<p>于是对于样本集合：</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2</script><p>基尼指数$Gini(D)$表示集合$D$的不确定性</p>
<p>若$D$被特征划分$D_1,D_2$，则在特征$A$的集合下，集合$D$的基尼指数定义为：</p>
<script type="math/tex; mode=display">
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p>式子中如$\frac{|D_i|}{|D|}$与上面一样，也就是划分为$D_1$的概率（特征$A$取值为第一个的概率）。</p>
<p>$Gini(D,A)$表示经$A=a$划分后集合$D$的不确定性。基尼指数越大，样本集合的不确定性也越大，与熵一致。</p>
<h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><p>以下ID3和C4.5都是生成多叉树，且只适用分类问题</p>
<h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>在决策树各个结点上用<strong>信息增益</strong>来选择特征</p>
<p>设置一个<strong>阈值</strong>$\epsilon$，当$D_i$的IG小于该阈值的时候，表示该数据集可认为全是同一类别，无需再继续划分</p>
<p>只用于分类（离散数据）</p>
<hr>
<p><strong>举例：</strong></p>
<p>有$K$个特征</p>
<p>①对$D$，求$g(D,A_1),g(D,A_2),\cdots,g(D,A_K)$</p>
<p>②于是可以求得使得IG：$g(D,A_k)$最大的特征$A_{g_1}$</p>
<p>③使用$A_g$将$D$划分为$D_1,D_2,\cdots,D_n$；（假设$A_g$有$n$个取值）</p>
<p>已经确定特征$A_g$，则特征集$A\to A-A_g$，特征数量变为$K-1$</p>
<p>④对每一个$D_i$，求$g(D_i,A_1),g(D_i,A_2),\cdots,g(D_i,A_{K-1})$ </p>
<p>⑤求使得IG最大的$A_{g_2}$</p>
<p>⑤<strong>若</strong>此时$A_{g_2}＜\epsilon$ ，则表示$D_i$可认为全是同一类别，无需再继续划分，该结点为叶子结点；</p>
<p><strong>否则：</strong>再继续对$D_i$进行划分，上一个特征节点$A_g$为中间结点</p>
<p>再对每个划分的划分 ，求使得对应IG最大的$A_{g_k}$，判断继续划分or停止…</p>
<p>⑥依次类推$\cdots\cdots$</p>
<hr>
<p>其中第④步就是返回到了第①步，$D换成了D_i$。（不过注意这里默认$D$并不是同一类，默认要继续划分）</p>
<p>具体略</p>
<h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3><p>在决策树各个结点上用<strong>信息增益比</strong>来选择特征</p>
<p>设置一个阈值，当IGR小于该阈值的时候，算法停止</p>
<p>具体略</p>
<p>（连续属性值算法见后记-2）</p>
<p><strong>以上两种算法容易过拟合，所以要剪枝</strong></p>
<h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>决策树的剪枝是从<strong>全局角度减小决策树复杂度</strong>以防止过拟合</p>
<p>通过极小化DT整体的损失函数来实现</p>
<p>设：树$T$，叶结点个数$|T|$，$t$为某叶结点，该叶结点上有$N_t$个样本点，其中$k$类样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\alpha≥0$为参数。</p>
<p><strong>DT学习的损失函数：</strong></p>
<script type="math/tex; mode=display">
C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\quad\quad H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}</script><script type="math/tex; mode=display">
记C_\alpha(T)右端第一项为：C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log \frac{N_{tk}}{N_t}</script><script type="math/tex; mode=display">
则有简记：C_\alpha(T)=C(T)+\alpha|T|</script><p>$C(T)$就是计算出<strong>每个叶子结点的已分类数据集的熵之和</strong>，熵越小，则DT整体的不确定性程度越低，就代表分类越确定。此项越小，就代表模型与训练集的<strong>拟合程度</strong>越高。</p>
<p>$|T|$是叶子结点的个数，它就能表征DT模型的复杂度。此项越小，模型的复杂度就越低，就越不容易过拟合。</p>
<p>于是$\alpha≥0$就控制了两者之间的关系；如当$\alpha=0$，就代表模型只考虑拟合度不考虑复杂度，etc。该损失函数就相当于一个正则化的极大似然估计</p>
<p>含有这两者的损失函数就代表了拟合度和复杂度的一个平衡关系。可以看到决策树的生成是学习局部模型的，但决策树的剪枝是学习整体的模型。该</p>
<p><strong>剪枝算法的目的，就是在给定$\alpha$下，求出使得损失函数$C_\alpha(T)$最小的子树$T’$</strong></p>
<p><strong>剪枝算法：（在用前面的算法生成完决策树之后）</strong></p>
<p>①计算每个结点的经验熵</p>
<p>②递归地从树的叶结点$t_{|T|}$向上回缩直到根节点$T$。具体地，</p>
<p>对于该叶结点，如果将其回缩到其父节点之后：</p>
<p>——如果树整体的损失函数变小，则回缩该结点；</p>
<p>——否则，不回缩该结点。</p>
<p>③以此迭代</p>
<p>由于回缩计算差值的时候，树上只有局部有改变，所以在计算损失函数差值的时候只需要在局部进行</p>
<h2 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h2><p>CART: classification and regression tree 分类与回归树。CART既可以用于分类也可以用于回归。</p>
<h3 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h3><p>CART算法生成的是<strong>二叉决策树</strong>。（二叉决策树不是哈夫曼树形式，别搞错了）</p>
<p><strong>思想：</strong>使用启发式算法，<strong>不断地</strong>寻找<strong>最优特征</strong>和<strong>最优切割点</strong>$(j,s)$，对数据集进行<strong>二元切分</strong>。</p>
<p><strong>多值/多段回归</strong>特征很明显可以<strong>重复使用</strong>，一次无法将多值的某特征从数据集分离开（二值特征使用一次就不能再使用）（多段回归特征类似于分类中的多值特征，只不过预测的连续变量可以被划分为多个段）</p>
<ol>
<li><p><strong>分类：</strong></p>
<p>（使用基尼指数来衡量最优特征和最优切分点，基尼指数越小，不确定越低，每个样本分类越相同）</p>
<p>通过对数据集$D_m$求$D_m$对每个特征$A_i$的基尼系数：<strong>即遍历$i,k$，求使得$Gini(D,A_i=k)$最小的最优特征和最优特征的切分点$(j,s)$组合</strong>（对应于<strong>遍历组合</strong>的标号$i,k$）。求出一个$D_m$的$(j,s)$就能将其<strong>划分</strong>为<strong>两个子数据集</strong>。再对两个子数据集<strong>重复迭代</strong>上述过程，若结点中的<strong>样本个数小于预订阈值/样本集的基尼指数低于预定阈值（样本基本属于同一类）/没有更多特征</strong>，该结点就是叶子结点。</p>
<p>具体算法略</p>
</li>
<li><p><strong>回归：【重要，是Boosting中许多模型的基础】</strong></p>
<p><strong>CART回归树生成是先确定结构（划分），再确定回归值(CART分类树就只有前面那个步骤)，以此到Boosting（GDBT XGBoost)也是这个求法</strong></p>
<p>在回归问题中，模型输出一个连续变量。</p>
<p>$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，数据集被划分为了$M$个单元$R_1,R_2,\cdots,R_M$，在每个单元上有一个固定的输出值为$c_m$</p>
<p>在回归中用<strong>平方误差来替代分类中的基尼指数</strong>：使用该结点数据集的每个样本中的【<strong>每个真实输出$y_i$与模型输出$f(x_i)$的均值$c_m$</strong>】的<strong>平方误差</strong>$\sum_{x_i}(y_i-c_m)^2$来找最优特征和最优切分点$(j,s)$。<strong>平方误差越小，样本分类越准确</strong>。（这个是回归与分类不同的关键）【平方误差即是这里CART使用的损失函数，也可以用其它损失函数，比如Boosting里的损失函数就都不一样】</p>
<p>与分类中一样的思路：</p>
<p><strong>①根据损失函数选定最优特征和切割</strong>第$j$个特征和其取值$s$为切分点，将父节点数据集$R$切分成了两个子节点（子数据集）：</p>
<p>$R_1(j,s)=\{x|x^{(j)}≤s\}$，$R_2(j,s)=\{x|x^{(j)}＞s\}$</p>
<p>以$R_1$为例，$\{x|x^{(j)}≤s\}$是指数据集中第$j$个特征的取值$≤s$的样本组成的新数据集，也就是一个划分，$R_2$同理。</p>
<p><strong>这里就确定了决策树的结构</strong>，于是：</p>
<p>②<strong>根据损失函数选取最优回归输出值</strong></p>
<p><strong>这里损失函数为平方误差，对其就0导容易推得最优回归输出$c$就是划分上的均值</strong></p>
<p>设$c_m=ave(y_i|x∈R_m)=\frac{1}{N_m}\sum_{x_i∈R_m(j,s)}y_i$，表示被特征与切分组合$(j,s)$切分后数据集上模型输出$y$的均值，则这个$c_m$就是该节点的最优回归输出</p>
<p>于是我们想要求得能够使得平方误差最小的<strong>特征与划分选取组合</strong>$(j,s)$以及其对应的最优输出（均值$\hat c_m$）：</p>
<script type="math/tex; mode=display">
\min_{j,s}[\min_{c_1}\sum_{x_i∈R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i∈R_2(j,s)}(y_i-c_2)^2]</script><p>于是再对两个被划分的子节点重复上面的步骤，直到满足停止条件</p>
<p>这样生成的树叫<strong>最小二乘回归树</strong></p>
<p><strong>理解：</strong></p>
<p>①【<strong>选特征与切割</strong>】：<strong>求得$(j,s)$就告诉我们决策树怎么生成，选什么特征怎么切分（同分类），遍历选择$j,s$选最优划分。即【确定了决策树的结构】【这时候计算损失就是根据左\右划分（结点）的[所有点各自的真实输出]与切割点（一个回归输出值）之间的损失之和，来选取使共损失最小的特征与切割。】</strong></p>
<p>②【<strong>决定回归输出</strong>】：每确定一个<strong>最优</strong>结点划分（即确定一组$(j,s)$），就能<strong>根据损失函数求出该结点内的最优输出：在各自每一个划分（结点）内选择一个最优的输出（即计算结点内的各点真实输出到选择的输出回归值的损失之和），使得该划分每个样本的损失最小。</strong></p>
<p>由于这里用的损失函数是平方误差，可以求得其最优值就是样本均值，对应<strong>特征$j$</strong>在划分数据集上的均值$\hat c_{1}$（特征值$≤s$的子结点）和$\hat c_2$（特征值$＞s$的子节点），所以这个均值就是特征值$j$的回归输出。</p>
<p><strong>【注意①和②在求最优切割和最优均值中求的损失都是用的[同一个损失函数计算]！看这个公式说得很明白，理解透这个公式】</strong></p>
<p>对于使用一般损失函数的形式：</p>
<script type="math/tex; mode=display">
\min_{j,s}[\min_{c_1}\sum_{x_i∈R_1(j,s)}L(y_i,c_1)+\min_{c_2}\sum_{x_i∈R_2(j,s)}L(y_i,c_2)]</script></li>
</ol>
<p>（连续属性值算法见后记-2）</p>
<p>（注意，如果每个训练样本是一维的，就不需要选切分特征（切分变量）了，回归和分类都是。如$x∈[0.5,10.5]$，在实数轴上，特征维度为1）</p>
<h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p><strong>思想：</strong>尝试所有的$\alpha$对树进行剪枝，找出最佳子树</p>
<p><strong>过程：</strong></p>
<ol>
<li><p><strong>迭代$\alpha$，剪枝，求每个$\alpha$下的最小子树</strong></p>
<p>①对一棵树$T$，<strong>令$\alpha$从0从小到大开始进行一次迭代</strong>。</p>
<p>对<strong>一个$\alpha$</strong>，对<strong>从下到上</strong>对<strong>每一个</strong>结点$t$，以$t$为单结点（<strong>即$t$以下的树结点全部剪掉</strong>）的损失函数为：</p>
<p>$C_\alpha(t)=C(t)+\alpha|t|=C(t)+\alpha$</p>
<p>以$t$为根节点（即<strong>不剪枝</strong>）的树$T_t$的损失函数为：$C_\alpha(T_t)=C(T_t)+\alpha|T_t|$</p>
<p>当$C_\alpha(T_t)=C_\alpha(t)$的时候，联立以上两式得：$\alpha^o=\frac{C(t)-C(T_t)}{|T_t|-1}$，这个式子即表示<strong>剪枝与不剪枝损失函数相等</strong>的时候$\alpha$的<strong>值$\alpha^o$，我们把它用$g(t)$表示</strong>。</p>
<p>根据上述式子推可得，$\alpha$越大，那么不剪枝的整体损失函数就会大于剪枝的整体损失函数（因为$C_\alpha(T_t)$的$\alpha$项后面跟着$|T_t|≥1$)，那么就该剪（因为目的是最小化损失函数），反之就不该剪。于是$g(t)$就可以作为一个用来判断对于一个结点$t$，给定$\alpha$下该不该剪枝的<strong>阈值</strong>。</p>
<p>于是对于本次迭代的$\alpha$下，计算树$T$中<strong>每个结点$t$</strong>的剪枝判断阈值$g(t)$值：</p>
<script type="math/tex; mode=display">
g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}</script><p>于是<strong>从下往上</strong>对树中的<strong>每个结点$t$</strong>作<strong>阈值判断</strong>：如果$\alpha≥g(t)，剪；\alpha＜g(t)，不剪$</p>
<p>②继续增大$\alpha$，选择下一个$\alpha$重复上述过程</p>
<p>③迭代结束，得到一个子树序列：$\{T_0,T_1,\cdots,T_n\}$，每个子树对应于不同的$\alpha$值，每个子树都是对应$\alpha$值的最小子树</p>
</li>
<li><p><strong>对获取的最小子树序列进行选择最优$\alpha$和最优子树</strong></p>
<p>利用独立验证数据集测试各子树的平方误差或基尼指数，求得最优解</p>
</li>
</ol>
<h2 id="比较ID3-C4-5和CART来理解"><a href="#比较ID3-C4-5和CART来理解" class="headerlink" title="比较ID3/C4.5和CART来理解"></a>比较ID3/C4.5和CART来理解</h2><ol>
<li><p><strong>划分</strong></p>
<p>在ID3和C4.5中，通过IG或IGR求出<strong>最优特征</strong>之后，直接将特征的所有可能取值作为切分点，那么就成了<strong>多分类</strong>。那么ID3/C4.5生成的是一个多叉树</p>
<p>而CART要求出<strong>最优特征</strong>和<strong>最优切分点</strong>的组合，以将该特征下的取值切分成<strong>二类</strong>。CART生成一个二叉树。</p>
<p><strong>举例：</strong>特征为年龄，取值为青年、中年、老年，那么ID3/C4.5就可能分成$O\{青年\}\{中年\}\{老年\}$，CART就可能分为$O\{青年\}\{中年、老年\}$ （$O$代表父节点，$\{\}代表子节点集合$）</p>
</li>
<li><p><strong>特征变量的使用</strong></p>
<p><strong>特征变量</strong>的使用中，多分的分类变量ID3和C4.5层级之间只<strong>单次使用</strong>，CART可<strong>多次重复使用</strong></p>
<p><strong>举例：</strong>继续上面的例子，在ID3/C4.5中，该特征（年龄）被一次划分为三个子数据集$\{青年\}\{中年\}\{老年\}$，于是该特征被删除特征集，不再使用。继续迭代就在其它特征中来选择最优特征</p>
<p>而在CART中，两个子节点为$O\{青年\}\{中年、老年\}$，对于这两个子节点进行迭代的时候，<strong>由于特征（年龄）并没有将所有年龄特征分开，则还可以再使用用过的特征（年龄）来继续二分类！</strong>（假设在右节点使用年龄的基尼系数相比其他特征和划分最低的话，那么下次就再使用年龄特征，只有二分类所以直接划分为$\{中年\}\{老年\}$）不过再次使用该特征并不一定是接着使用，这需要<strong>根据基尼指数最小/平方误差最小计算</strong>。</p>
<p>也因此CART一般比ID3/C4.5深</p>
</li>
<li><p>输入与输出</p>
<p>ID3和C4.5只能做分类，CART可以做回归和分类</p>
<p>ID3只能作用离散值属性，C4.5和CART可以作用连续值属性</p>
<p>注意区别见后记-3</p>
</li>
<li><p>计算代价</p>
<p>CART计算更快，消耗资源更少，因为CART不需要进行熵中的log运算（基尼指数和熵效果差不多，但没用log，这就是用基尼指数的原因）</p>
</li>
<li><p>缺失值</p>
<p><strong>ID3对缺失值敏感</strong>，而C4.5和CART对缺失值可以进行多种方式的处理 O</p>
</li>
<li><p>剪枝方法不同</p>
<p>C4.5是通过枝剪来修正树的准确性，而CART是直接利用<strong>全部数据发现所有</strong>树的结构进行对比 </p>
</li>
<li><p>C4.5是ID3的优化 IGR替代了IG</p>
<p>因为信息增益的缺点是倾向于选择取值较多的屬性，在有些情况下这类属性可能不会提供太多有价值的信息。 </p>
</li>
<li><p>样本量</p>
<p>只从样本量考虑，小样本建议考虑c4.5、大样本建议考虑cart。而cart本身是一种大样本的统计方法，小样本处理下泛化误差较大</p>
</li>
</ol>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol>
<li><p>缺失值处理</p>
<p>①抛弃缺失值</p>
<p>②补充缺失值</p>
<p>③概率化缺失值（C4.5使用此方式， 对缺失值的样本赋予该属性所有属性值的概率分布）</p>
<p>④缺失值单独分支</p>
</li>
<li><p>属性为连续值的划分问题</p>
<p>离散化技术</p>
<p>C4.5用于连续值属性的算法：（于是就成了二叉树了）</p>
<p>采用离散化技术（如二分法）进行处理。<strong>将属性值从小到大排序，然后选择中间值作为分割点</strong>，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，对所有可能的分割法，计算分割的信息增益率，选择信息增益率IGR最大的属性值进行分割。 </p>
<p>CART的连续值属性的算法：只是把IGR换成了基尼指数或平方误差，其它一样</p>
</li>
<li><p><strong>【注意】回归预测</strong>与<strong>连续值属性</strong>的区别</p>
<p><strong>回归预测</strong>指<strong>输出</strong>（预测）的是一个连续变量的值，比如要求输出概率（又比如说要预测房价）</p>
<p><strong>属性为连续值</strong>指的是<strong>输入</strong>的属性是连续的，即取值无限，如输入的$x是实数$，那就得用离散化方法，C4.5采用了二分，即把$x是实数\to \{x&lt;A_i\},\{x≥A_i\}$两个结点。（或者如输入的值是房价）</p>
<p><strong>属性就是特征，属性的值即特征的取值</strong></p>
</li>
<li><p>CART回归树的例可以见Boosting的GDBT后记</p>
</li>
<li><p>寻找最优切分和特征一般都是贪心算法，只考虑当前结点下的最优特征和最优分割</p>
</li>
<li><p>决策树以及基于Boosting的算法的优点</p>
<p>①天然对缺失值和噪音有很好的鲁棒性</p>
<p>②天然可以很好的处理各种类型的特征</p>
<p>③天然对离群值有很好的鲁棒性</p>
<p>④（对Boosting DT）数据规模影响不大，因为我们对弱分类器的要求不高 </p>
</li>
<li><p>但也有缺点，就在于假设无噪音的情况下，性能不如SVM，LR等，但如果数据集有噪音的话，这个弱势就不能么明显了。而且，可以用Boosting嘛，一个DT不给力，多个DT一起干，效果就更强</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>非线性支持向量机与序列最小最优化算法</title>
    <url>/2019/11/12/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    <content><![CDATA[<p>（只突出与线性SVM不同的地方，具体的推导大部分过程相似见前面的讲线性SVM的文章）</p>
<p>线性（可分）SVM的分类平面是线性的，这就无法对如椭圆形特征空间进行分类，于是我们想到<strong>将原来特征空间$\mathbb X$（欧式空间$\mathbb R^n$或离散集合）上的非线性问题（超曲面模型），映射到新的高维特征空间$\mathbb H$（希尔伯特空间）成为线性可分（超平面模型）问题</strong>，这样我们就可以对新的线性可分的特征空间使用线性支持向量机的学习算法来解决非线性支持向量机的学习问题。<strong>（核技巧思想一）</strong></p>
<a id="more"></a>    
<h2 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h2><p>kernel trick</p>
<p>运用<strong>核技巧思想一</strong>，会发现如果我们想要直接计算映射，会非常困难。于是我们选择<strong>计算内积</strong>（<strong>将输入空间中的内积转换为输出空间的内积</strong>），恰好在使用SVM学习算法的时候，<strong>无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积</strong>，于是有以下方法——</p>
<p>在学习和预测中只定义核函数$K(x,z)$，而不显式定义映射函数$\phi$。因为通常直接计算核函数$K(x,z)$比较容易，而直接计算映射$\phi(x)和\phi(z)$并不容易。</p>
<p>观察到线性SVM的对偶问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积，于是在对偶问题的目标函数中的内积$x_i·x_j$可以用核函数$K(x_i,x_j)=\phi(x_i)·\phi(x_j)$来代替。此时：</p>
<script type="math/tex; mode=display">
对偶问题目标函数：\min_\alpha\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i</script><script type="math/tex; mode=display">
最后的分类决策函数也修改：f(x)=sign(\sum_{i=1}^N\alpha_i^oy_iK(x_i,x_j)+b^o)</script><p><strong>然后就只需要按照原来的线性可分SVM的学习算法</strong>，求对偶问题的外部问题，即求极值下的最优参数$\alpha^o$（可用SMO），然后再用KKT或等价求得$w^o,b^o$，得到超平面表达式</p>
<p>这等价于经过映射函数$\phi$将原来的输入空间变换到一个新的特征空间，<strong>将输入空间中的内积$x_i·x_j$变换为特征空间中的内积$\phi(x_i)·\phi(x_j)$</strong></p>
<p><strong>总结一下，核方法做了两件事：①样本空间映射到特征空间②计算了两样本在特征空间的内积</strong>（核技巧）</p>
<p>核技巧并不是SVM专用，可以用在其它地方</p>
<h2 id="寻找核函数-正定核"><a href="#寻找核函数-正定核" class="headerlink" title="寻找核函数-正定核"></a>寻找核函数-正定核</h2><p>函数$F(x,z)$满足什么条件才能成为核函数？也就是说对于映射函数$\phi$，直接计算$F(x,z)$即可而不用计算$\phi(x),\phi(z)$的条件是什么？</p>
<p>这种核函数一般都是<strong>正定核</strong>，直接给出结论：</p>
<p><strong>正定核的充要条件</strong>：设$K:\mathbb X×\mathbb X\to \mathbb R$是<strong>对称函数</strong>，则$K(x,z)$为正定核函数的<strong>充要条件</strong>是对任意$x_i∈\mathbb X,i=1,2,\cdots,m,K(x,z)$对应的<strong>Gram矩阵</strong>：</p>
<script type="math/tex; mode=display">
K=[K(x_i,x_j)]_{m×m}</script><p>是<strong>半正定矩阵</strong>（见后记-1）</p>
<p><strong>正定核的等价定义</strong>：设$\mathbb X \subset \mathbb R^n,K(x,z)$是定义在$\mathbb X×\mathbb X$上的<strong>对称函数</strong>，如果对任意</p>
<p>$x_i∈\mathbb X,i=1,2,\cdots,m,K(x,z)$对应的<strong>Gram矩阵</strong>：</p>
<script type="math/tex; mode=display">
K=[K(x_i,x_j)]_{m×m}</script><p>是<strong>半正定矩阵</strong>，则称$K(x,z)$是正定核</p>
<p>推导和证明略（先定义映射，构造向量空间；再构造内积空间；再完备化为希尔伯特空间；再在此之上证明）</p>
<p>这一定义在构造核函数的时候很有用，但要验证某函数是否为正定核函数是不容易的，因为要对所有输入集验证$K$对应的Gram矩阵是否为半正定矩阵。所以在实际问题中一般使用已有的核函数，见下</p>
<h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><ol>
<li><p>多项式核函数</p>
<script type="math/tex; mode=display">
K(x,z)=(x·z+1)^p</script></li>
<li><p>高斯核函数</p>
<script type="math/tex; mode=display">
K(x,z)=e^{-\frac{||x-z||^2}{2\sigma^2}}</script></li>
<li><p>字符串核函数 O</p>
<p>$k_n(s,t)$给出字符串$s$和$t$中长度为$n$的所有子串组成的特征向量的余弦相似度。两个字符串相似的字串越多，它们就越相似，字符串核函数的值就越大。</p>
</li>
</ol>
<h2 id="非线性支持向量机算法"><a href="#非线性支持向量机算法" class="headerlink" title="非线性支持向量机算法"></a>非线性支持向量机算法</h2><p>第一部分即讲了与线性支持向量机算法的区别，用核函数替换内积即可，其它一样，略</p>
<h2 id="序列最小最优化算法"><a href="#序列最小最优化算法" class="headerlink" title="序列最小最优化算法"></a>序列最小最优化算法</h2><p>sequential minimal optimization，SMO</p>
<p><strong>思想：不断</strong>地将原多变量的二次规划问题<strong>分解</strong>为只有两个变量的二次规划子问题，并对子问题进行<strong>解析求解</strong>，<strong>直到所有变量满足KKT条件为止</strong>（因为KKT条件是下面要求的最优化问题的充分必要条件）</p>
<p><strong>目的：</strong>SMO算法主要是为了求解SVM中对偶问题的外部问题：</p>
<script type="math/tex; mode=display">
目标函数：\min_\alpha\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i</script><script type="math/tex; mode=display">
s.t.\quad \sum_{i=1}^N\alpha_iy_i=0</script><script type="math/tex; mode=display">
0≤\alpha_i≤C\quad,i=1,2,\cdots,N</script><p>其中$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T$，那么如何才能求得一个$\alpha_i$序列（也即向量$\alpha$）使得该问题为最优解呢？也即如何使得<strong>序列最小最优化</strong>。</p>
<p>SMO是一个<strong>启发式算法</strong>，通过<strong>循环迭代</strong>的方式求得序列最优最小值。SMO输出的是一个<strong>近似解</strong>。</p>
<p><strong>算法：</strong></p>
<p>知乎上看到一个讲得很好的，直接贴上来</p>
<ol>
<li><p>两个二次规划变量的求解： <a href="https://zhuanlan.zhihu.com/p/78599113" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/78599113</a> </p>
<p><strong>将其中一个作为变量来求解</strong></p>
<p>求解过程中设定的记号$v,g(x_i),E,\eta$都是为了方便表示求解过程，实际上求解思路就是</p>
<p>①将目标函数改写为记号表示形式（这里很复杂），$x_i,x_j分别为\alpha_1,\alpha_2$</p>
<p>②根据约束条件表示出$\alpha_1$，代入目标函数中，于是目标函数就只含$\alpha_1$变量</p>
<p>③对目标函数用对$\alpha_2$的0导数法，求得最优的$\alpha_2$表示</p>
<p>④上面求出的是未经剪辑的最优解，下一步求出经过剪辑的最优解即可</p>
<p>注意：$k=\alpha_1^{old}-\alpha_2^{old}，k=\alpha_1^{old}+\alpha_2^{old}$这个表示方法理解</p>
</li>
<li><p>变量的选择： <a href="https://zhuanlan.zhihu.com/p/78788836" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/78788836</a> </p>
<p>①选取第一个变量$\alpha_1$的过程称为外层循环，选取<strong>违反KKT条件最严重的点</strong></p>
<p>②选取第二个变量$\alpha_2$的过程称为内层循环，选取能使$\alpha_2$<strong>变化足够大的点</strong>（加快计算速度，使得目标函数尽快变小。根据公式$\alpha_2$依赖于$y_2(E_1-E_2)$，由于$\alpha_1$已定，那么$E_1$就定了，那么就可以选择合适的$\alpha_2$）</p>
<p>如果第二个$\alpha_2$不能够使得目标函数有足够的下降，那就遍历数据集选取$\alpha_2$，如果遍历完了还没找到，那就通过外层循环重新寻找另外的$\alpha_1$</p>
</li>
<li><p>先选择变量，然后计算最优解，然后就要<strong>更新阈值$b$和差值$E_i$</strong></p>
</li>
</ol>
<p><strong>总结：</strong></p>
<p>先选择两个变量，再求解析解，直到达到<strong>停机条件</strong>（检验是否所有样本点都符合KKT条件，在精度$\epsilon$下进行）</p>
<p>具体略</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol>
<li><p>正定矩阵和半正定矩阵</p>
<p>半正定矩阵：$A∈\mathbb R^{n×n},x∈\mathbb R^n$，对任意$x$，$x^TAx≥0$恒成立</p>
<p>正定矩阵：$A∈\mathbb R^{n×n},x∈\mathbb R^n$，对任意$x$，$x^TAx＞0$恒成立</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
      <tags>
        <tag>矩阵论</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Unity2D基础</title>
    <url>/2019/11/10/Unity2D%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p><em>施工中</em></p>
<p>如行走，跌倒等动画是绘图之后创建unity2d动画素材，导入后进行裁剪和选取动画导入的</p>
<p>给实体分组归类是好习惯，在Hierarchy建立一个新的empty game object：<code>create object</code>，然后将组件拖入即可</p>
<a id="more"></a>
<h2 id="重力与碰撞"><a href="#重力与碰撞" class="headerlink" title="重力与碰撞"></a>重力与碰撞</h2><p>给物体添加重力组件：<code>Add Component-Rigidbody 2D</code></p>
<p>但是拥有重力的物体会一直往下坠，所以我们要给地面和物体<strong>都添加</strong>一个碰撞（各种组件都需要碰撞体）</p>
<p>给物体添加Box碰撞组件：<code>Add Component-Box Collider 2D</code>，然后点击<code>Edit Collider</code>，拖动绿色长方形来选取碰撞体积。碰撞体有很多种类型，这里只是Box类型</p>
<h2 id="速度"><a href="#速度" class="headerlink" title="速度"></a>速度</h2><p>想给主角添加一个运动速度，<strong>通过脚本实现</strong>。</p>
<p>创建脚本：<code>Add Component-new script</code></p>
<p>右键<code>edit script</code>，打开IDE编写脚本（写C#我用的VS）</p>
<p>打开后会发现已经是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">using System.Collections;</span><br><span class="line">using System.Collections.Generic;</span><br><span class="line">using UnityEngine;</span><br><span class="line"></span><br><span class="line">public class aisaka : MonoBehaviour</span><br><span class="line">&#123;</span><br><span class="line">    // Start is called before the first frame update</span><br><span class="line">    void Start()</span><br><span class="line">    &#123;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Update is called once per frame</span><br><span class="line">    void Update()</span><br><span class="line">    &#123;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>void Update()</code>方法是不停地执行的方法，<code>Start()</code>方法仅在实例化完成后调用一次    </p>
<p>我们为其加入速度，写如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>GetCompoment &lt;T&gt;()</code>从当前游戏对象获取组件T（<strong>返回该组件</strong>），只在当前游戏对象中获取，没得到的就返回null，不会去子物体中去寻找。</p>
<p><code>GetCompomentInChildren&lt;T&gt;()</code>先从本对象中找，有就返回，没就子物体中找，知道找完为止。</p>
<p><code>GetComponents&lt;T&gt;()</code>获取本游戏对象的所有T组件，不会去子物体中找。</p>
<p><code>GetComponentsInChildren&lt;T&gt;()=GetComponentsInChildren&lt;T&gt;(true)</code>取本游戏对象及子物体的所有组件</p>
<p><code>GetComponentsInChildren&lt;T&gt;(false)</code>取本游戏对象及子物体的所有组件 除开非活跃的游戏对象，不是该组件是否活跃。</p>
<p><strong>注意当前游戏对象指的是这个脚本的对象</strong>，比如这里就是获取该脚本对象（主角）的<code>Rigidbody2D</code>组件</p>
<h2 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h2><p><strong>如果IDE无法联想Unity组件和函数</strong>，有两种可能（可能都存在）</p>
<p>①需要在Unity中，Edit-Preferences-External Tools中在External Script Editor里选择对应的IDE</p>
<p>②VS报错OmniSharp failed 版本兼容问题</p>
<h2 id="神奇的Unity使用心得"><a href="#神奇的Unity使用心得" class="headerlink" title="神奇的Unity使用心得"></a>神奇的Unity使用心得</h2><p>unity下载就个残废，那个hub这么烂还好意思摆出来，明明有本体直接下载放到一个巨巨巨巨巨小的地方非要宣传一个破烂hub；中国官网一万年备案中，国外官网打开就是一片空白，速度慢得堪比IE；程序退出经常未响应；老版本Unity就直接和VS新版本不兼容了；装了新版本之后竟然老版本清除不干净，快捷方式打不开新版；然后装了新版本老版本的project打不开了，提示我得同时存在老版本才能打开，然后又没法从程序里调用VS了。。。什么破软件我吃你萌，贵司程序员全体去吃屎！！！！</p>
]]></content>
      <categories>
        <category>开发</category>
        <category>Unity</category>
      </categories>
  </entry>
  <entry>
    <title>夢に僕らで帆を張って　来るべき日のために夜を越え</title>
    <url>/2019/11/10/%E5%A4%A9%E6%B0%94%E4%B9%8B%E5%AD%902/</url>
    <content><![CDATA[<div align="MIDDLE">
<iframe src="//player.bilibili.com/player.html?aid=73527359&cid=125772545&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="564"> 
</iframe>
</div>

<div style="text-align: center">当大地渐渐失去了重力，在千年一遇的今天</div>
<div style="text-align: center">乘着花火的声音，我们离开这颗星球吧</div>
<div style="text-align: center">在他睁开眼的那一瞬间，向着那无法返回的地方</div>
<div style="text-align: center">齐声喊着「1、2...」跳出大地，向着其他行星前进</div>
<div style="text-align: center">........</div>
<div style="text-align: center"><strong>出发吧！</strong></div>
<div style="text-align: center"><strong>出发吧！</strong></div>
<div style="text-align: center"><strong>出发吧！</strong></div>
<div style="text-align: center"><strong>为了那个梦我们扬帆起航、为了理应到来的那天跨越无尽黑夜</strong></div>]]></content>
      <categories>
        <category>Anime</category>
      </categories>
  </entry>
  <entry>
    <title>线性（可分）支持向量机</title>
    <url>/2019/11/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h2 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h2><p>支持向量机，SVM</p>
<p>SVM应用于二分类问题。</p>
<p>输入空间和特征空间为两个不同的空间。输入空间为欧式空间或离散集合，特征空间为欧式空间或希尔伯特空间。</p>
<p>线性可分SVM和线性SVM在两个空间的元素一一对应，非线性SVM是非线性映射。</p>
<a id="more"></a>
<p><em>本篇中的SVM都指线性可分SVM，其它SVM可能会有一些变化</em></p>
<h2 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h2><p><strong>假定训练集线性可分。</strong></p>
<p><strong>数据集：</strong>假定特征空间上的训练集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中</p>
<p>$x_i∈\mathbb R^n,y_i∈\{+1,-1\},i=1,2,\cdots,N$，$x_i$为第$i$个特征向量，$y_i$为类标记。当$y_i=+1$，称$x_i$为正例；当当$y_i=-1$，称$x_i$为负例。$(x_i,y_i)$称为<strong>样本点</strong>。</p>
<p>具体举例：如正例点$x_i=(3,5),y_1=+1$，$(3,5)$存在于<strong>欧式空间</strong>而$(x_1,+1)$存在于<strong>特征空间</strong>。</p>
<p><strong>学习目标：</strong>在特征空间上找到一个<strong>分离超平面</strong>，能将实例分到不同的类。分离超平面对应于方程$w·x+b=0$，由法向量$w$和截距$b$决定，可以用$(w,b)$表示。其将特征空间划分为两部分：法向量指向的一侧为正类，另一侧为负类。</p>
<p><strong>理解这个超平面：</strong>$w·x+b=0$中$w,x,b$都是向量，向量中每一个位置都是一个特征维度，可以理解为这个超平面有很多个维度，在每一个维度都有一个分界线，从而整体构成一个高维度的超级分界面。</p>
<p><strong>这个超平面就是分类器</strong></p>
<p>显然这种超平面是有无穷个的。对于感知机，则是使用误分类最小策略来求得超平面；对于线性可分SVM，则使用<strong>间隔最大化</strong>来求得<strong>最优</strong>超平面，解是唯一的。</p>
<p><strong>线性可分支持向量机：</strong>给定线性可分训练数据集，通过间隔最大化等价地求解相应的凸二次规划问题<strong>学习得到的</strong>分离超平面为（学习得到的最优参数就是$w^o,b^o$）</p>
<script type="math/tex; mode=display">
w^o·x+b^o=0</script><p>对应的<strong>分类决策函数</strong>为：</p>
<script type="math/tex; mode=display">
f(x)=sign(w^o·x+b^o)</script><p>也就是：</p>
<script type="math/tex; mode=display">
f(x)=\begin{cases} +1,w^o·x+b^o≥0\\ -1,w^o·x+b^o＜0 \end{cases}</script><p>称为线性可分支持向量机。<strong>注意</strong>其中的决策函数只有两种取值：$+1,-1$，对应了决策函数通过$f(x)=sign(w^o·x_i+b^o)$来判断点$x_i$是正例还是负例。$sign$是符号函数。</p>
<p><strong>确信度：</strong>一个点如果距离超平面越远，那么表示其<strong>预测的确信度</strong>越高，反之反之。假设一个样本点<strong>越正向远离分界面</strong>，那么我们越认为其为<strong>正例的概率越大</strong>，方向远离分界面那么负例的概率越大。</p>
<p><strong>点$x$距离超平面的距离</strong> $=|w·x+b|$</p>
<p>$w·x_i+b$的符号若与训练样本$x_i$的标签$y_i$一致，则<strong>分类正确</strong>；不一致则分类错误。</p>
<h2 id="从逻辑斯谛回归LR来理解SVM"><a href="#从逻辑斯谛回归LR来理解SVM" class="headerlink" title="从逻辑斯谛回归LR来理解SVM"></a>从逻辑斯谛回归LR来理解SVM</h2><p>LR的输出是一个概率输出，LR实际上就是将通过线性分类器$w·x+b,x∈\mathbb R^n$将输入$x$映射到概率空间$(0,1)$上，其越靠近上边界$1$，就认为其标签为$1$的概率越高；其越靠近下边界$0$，就认为其标签为$0$的概率越高，这时候我们可以给一个<strong>决策标准</strong>$th=0.5$，对于一个样本$x_i$和逻辑斯谛回归分类器$f(w·x+b)$（也可以表示为$P(Y=1|x)$)，此时对$x_i$的<strong>决策结果</strong>就是$f(x_i)=\begin{cases} 1,f(w·x_i+b)≥0.5\\ 0,f(w·x_i+b)＜0.5 \end{cases}$</p>
<p><strong>如果</strong>我们将LR中的决策标准的阈值改为$0$，分类器改为$f(w·x+b)=w·x+b$，标签从$0,1$改为$-1,+1$，此时对$x_i$的分类结果就是：$f(x)=\begin{cases} +1,w·x_i+b≥0\\ -1,w·x_i+b＜0 \end{cases}$。<strong>震惊，这不就是个标准的SVM了吗！</strong></p>
<p>①<strong>LR中的$0$相当于SVM的负例，$1$相当于SVM的正例；</strong></p>
<p>②<strong>LR中的阈值$0.5$，而SVM的阈值为$0$</strong>，即在SVM中，分类器的输出$≥0$，就判断为正例，$＜0$就判断为负例；分类器输出越大则样本为正例的概率越大（<strong>确信度越高</strong>），输出越小则样本为负例的概率越大（<strong>确信度越高</strong>）。（SVM分类器输出的绝对值乘以标签($+1 or-1$)：$y_i|w·x_i+b|$即为<strong>函数间隔</strong>）</p>
<p>③<strong>LR的分类器输出在$(0,1)$上</strong>，且决策阈值为$0.5$；<strong>而SVM的分类器输出可以在$(-∞,+∞)$</strong>，决策阈值为$0$。<strong>为什么分类器输出差别这么大？</strong>因为LR的输出值是线性分类器$w·x+b$的<strong>逻辑斯谛分布作为映射</strong>，就是它将输出映射到了$(0,1)$上，其恰好就是概率。而线性SVM的分类器就直接是一个<strong>线性函数映射</strong>，它的输出映射到了整个实域</p>
<p><strong>结论：LR和SVM本质是一样的</strong>。借LR可以更好地理解SVM（这段写得有点啰嗦）。</p>
<h2 id="间隔表示"><a href="#间隔表示" class="headerlink" title="间隔表示"></a>间隔表示</h2><p>前面引出了间隔，这里具体定义。对于样本点$(x_i,y_i)∈T$到超平面$(w,b)$。</p>
<p><strong>某点到超平面面的函数间隔：</strong></p>
<script type="math/tex; mode=display">
\hat\gamma_i=y_i(w·x_i+b)</script><p><strong>样本集$T$所有点到面的函数间隔：</strong>这就是前面所说的我们最想要的最优超平面的标准（见上）：</p>
<script type="math/tex; mode=display">
\hat\gamma=\min_{i=1,\cdots,N}\hat\gamma_i</script><p>如果$w,b$成比例改变，那么实际上超平面$\lambda w·x_i+\lambda b=0=w·x_i+b=0$并没有变，但是函数间隔却番倍了，所以我们要统一让超平面的参数标准化，使得该间隔确定。所以<strong>标准化</strong>后，得：</p>
<p><strong>某点到超平面面的几何间隔：</strong>（其实就是欧式空间中的点到平面的距离公式再加个正负来判断分类正确性）</p>
<script type="math/tex; mode=display">
\gamma_i=y_i(\frac{w}{||w||}·x_i+\frac{b}{||w||})</script><p><strong>样本集$T$所有点到面的几何间隔：</strong></p>
<script type="math/tex; mode=display">
\gamma=\min_{i=1,\cdots,N}\gamma_i</script><p><strong>函数间隔和几何间隔的特点：同时表示了分类的「正确性」和「确信度」</strong>（确信度见上文理解，正确性是因为<strong>如果分类错</strong>误，那么算出来的间隔会是<strong>负</strong>；分类正确才会得出正值，根据式子容易看出来）</p>
<p>函数间隔无论如何改变，几何间隔都是<strong>不变</strong>的。<strong>表征特征空间上的关系的是几何间隔。</strong>（也就是SVM最常见的那个图）</p>
<p>显然有<strong>函数间隔$\hat \gamma$与几何间隔$\gamma$的关系</strong>：$\gamma_i=\frac{\hat \gamma_i}{||w||},\gamma=\frac{\hat \gamma}{||w||}$</p>
<p>（$||w||$为向量$w$的第二范数，即$||w||=\sqrt{w_1^2+w_2^2+\cdots+w_n^2} $），后面取其平方以去掉根号方便计算）</p>
<h2 id="硬间隔最大化"><a href="#硬间隔最大化" class="headerlink" title="硬间隔最大化"></a>硬间隔最大化</h2><p><strong>学习SVM的过程</strong>就是学习最优超平面的过程，也就是确定超平面$w·x+b$的参数$w,b$</p>
<p><strong>什么是我们想要的最优的超平面（分类器）：「尽可能</strong>提高<strong>最</strong>难以分类的点的<strong>分类确信度</strong>」。</p>
<p><strong>从几何角度解释：</strong>使得离超平面最近的样本点到超平面的距离最大的超平面，「也就是说要尽量<u>扩大</u><strong>超平面</strong>与<strong>离超平面最近点</strong>的<u>间隔</u>」。这就是<strong>间隔最大化</strong></p>
<p><strong>间隔最大化可以表述为：</strong>（推导起点，从最大化几何间隔开始）</p>
<script type="math/tex; mode=display">
\max_{w,b}\gamma</script><script type="math/tex; mode=display">
s.t.\quad y_i(\frac{w}{||w||}·x_i+\frac{b}{||w||})≥\gamma,i=1,2,\cdots,N</script><p>即：<strong>尽量提高几何间隔的下限</strong></p>
<p>需求解使得目标函数最优的参数$w,b$</p>
<p>为了由$\gamma_i=\frac{\hat \gamma_i}{||w||},\gamma=\frac{\hat \gamma}{||w||}$，可把问题改写（但实际还是几何间隔）为<strong>用函数间隔表示</strong>为：</p>
<p>$\max_{w,b}\frac{\hat\gamma}{||w||}$</p>
<p>$s.t.\quad y_i(w·x_i+b)≥\hat\gamma,i=1,2,\cdots,N$</p>
<p>前面说过，函数间隔不会影响解的结果，也就是将$w,b$按比例改变，函数间隔虽然变了，但超平面是没变的。</p>
<p>于是，我们想到<strong>直接将函数间隔人为设定为1</strong>，就相当于让$w,b$以一个比例缩放，使得函数间隔$\hat\gamma=1$，这个乘上的比例因子是多少我们不需要求它，因为即使$w,b$以一个未知的比例等比缩放了，求得的超平面还是一样的，<strong>约束优化问题还是等价的</strong>，（此时几何间隔为$\gamma=\frac{1}{||w||}$），也就是将<strong>问题等价简化为</strong>：</p>
<p>$\max_{w,b}\frac{1}{||w||}$</p>
<p>$s.t.\quad y_i(w·x_i+b)≥1,i=1,2,\cdots,N$</p>
<p>显然$\max_{w,b}\frac{1}{||w||}$<strong>等价于</strong>$\min_{w,b}\frac{1}{2}||w||^2$，则原问题改写为一个易于求解的<strong>凸二次规划问题</strong>：</p>
<script type="math/tex; mode=display">
\min_{w,b}\frac{1}{2}||w||^2</script><script type="math/tex; mode=display">
s.t.\quad y_i(w·x_i+b)-1≥0,i=1,2,\cdots,N</script><p>求得该凸二次规划问题即可得到最优解$w^o,b^o$。注意这里是<strong>不等式约束问题</strong>，而前面比如在MEM中的是等式约束问题。</p>
<p>由此得到分离超平面$w^o·x+b^o=0$，分类决策函数$f(x)=sign(w^o·x+b^o)$</p>
<p>这个求解过程就是线性可分SVM的学习算法：<strong>最大间隔法(maximum margin method)</strong></p>
<p>线性可分SVM的最大间隔分离超平面<strong>存在</strong>且<strong>唯一</strong>（证略）</p>
<hr>
<p><strong>支持向量(support vector)：</strong>训练数据集中的样本点与分离超平面距离最近的样本点的实例</p>
<p>支持向量点$(x_i,y_i)$是使得约束条件取等号：$y_i(w·x_i+b)-1=0$的点</p>
<p>对于正例：$y_i=+1,支持向量在超平面\quad w·x+b=1$上；</p>
<p>对于负例：$y_i=-1,支持向量在超平面\quad w·x+b=-1$上</p>
<p><strong>支持向量到超平面的距离就是几何间隔</strong>$\gamma=\frac{\hat \gamma}{||w||}$，由于设定了$\hat \gamma=1$，则$\gamma=\frac{1}{||w||}$</p>
<p>两个超平面的距离称为<strong>间隔(margin)</strong>。$\therefore 间隔 =\frac{2}{||w||}$，两个超平面称为间隔边界。</p>
<p>支持向量在确定分离超平面上取决定性作用，SVM实际上是由很少的重要的训练样本——支持向量所确定的，所以才得名支持向量机(Support Vector Machines)</p>
<h2 id="通过拉格朗日对偶性求解硬间隔最大化"><a href="#通过拉格朗日对偶性求解硬间隔最大化" class="headerlink" title="通过拉格朗日对偶性求解硬间隔最大化"></a>通过拉格朗日对偶性求解硬间隔最大化</h2><p>也叫线性可分SVM的<strong>对偶算法(dual alogorithm)</strong>。这样做①对偶问题往往更容易求解②自然引入核函数，可以进而推广到非线性SVM</p>
<p>见前面拉格朗日乘子与对偶性专题，这里就不详细解释了。</p>
<p>对于上一节的凸二次规划问题，首先<strong>引入拉格朗日函数</strong>(<strong>这里$w,b$是我们的目标变量，相当于标准形式中的$x$</strong>)</p>
<p>这里是<strong>减去</strong>拉格朗日乘子项（加乘子项也可以，但推理表示会麻烦一些，所以记得用减乘子项！）</p>
<script type="math/tex; mode=display">
L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i[y_i(w·x_i+b)-1]</script><p>注意这里设置<strong>减去算子项</strong>，是为了后面用KKT条件的时候，构造出$\alpha_i≥0$（<strong>不等式约束的乘子$\alpha$要规定$\alpha≥0$</strong>，是为了满足后文KKT的对偶可行条件）</p>
<p>根据拉格朗日对偶性，KKT条件是凸问题的充要条件，<strong>转换为等价对偶问题</strong>：</p>
<script type="math/tex; mode=display">
\min_{w,b}\frac{1}{2}||w||^2=L(w,b,\alpha)=\min_{w,\beta}\max_\alpha L(w,b,\alpha)=\max_\alpha \min_{w,\beta}L(w,b,\alpha)</script><p>于是求内部问题$\min_{w,\beta}L(w,b,\alpha)$：</p>
<p>对两个独立变量的子式使用零导数法：</p>
<p>$\nabla_w L(w,b,\alpha)=w-\sum_{i=1}^N\alpha_iy_ix_i=0$（<strong>注意</strong>第二范数求导：见后记2）</p>
<p>$\nabla_b L(w,b,\alpha)=-\sum_{i=1}^N\alpha_iy_i=0$</p>
<p>$\therefore w=\sum_{i=1}^N\alpha_iy_ix_i$</p>
<p>$\sum_{i=1}^N\alpha_iy_i=0$（<strong>不能丢，作为对偶问题的约束条件！</strong>除非是像第一个能够将原式所有的$w$全部替换的形式，这样该约束条件实质就已经被完全包含在代入后的式子中而失效了，才可以丢）</p>
<p>回代$L(w,b,\alpha)$得：</p>
<p>$L(w,b,\alpha)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i·x_j)-\sum_{i=1}^N\alpha_iy_i((\sum_{j=1}^N\alpha_jy_jx_j)·x_i+b)+\sum_{i=1}^N\alpha_i$</p>
<p>$=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i·x_j)+\sum_{i=1}^N\alpha_i$</p>
<p><strong>这里注意</strong>①2-范数运算，见后记3 ②无关求和代入的时候，求和符号的迭代变量要换一个以免重合</p>
<p><strong>内部问题求解出来了，于是问题等价为外部问题</strong>，求：</p>
<p>$\max_\alpha-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i·x_j)+\sum_{i=1}^N\alpha_i$</p>
<p>$s.t.\quad \quad\sum_{i=1}^N\alpha_iy_i=0,\quad \alpha_i≥0,\quad i=1,2,\cdots,N$</p>
<p>转化为易求的$\min$形式，此为<strong>求解算法的第一步公式：</strong></p>
<script type="math/tex; mode=display">
\min_\alpha\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i·x_j)-\sum_{i=1}^N\alpha_i</script><script type="math/tex; mode=display">
s.t.\quad \quad\sum_{i=1}^N\alpha_iy_i=0,\quad \alpha_i≥0,\quad i=1,2,\cdots,N</script><p>于是这里就用通用的算法来求解此二次规划问题(<strong>用SMO求解</strong>，见后面的专题)</p>
<p><strong>假设已经求出</strong>对偶最优化的最优解为$\alpha^o$，于是可以求得原始最优化对$(w,b)$的解：$w^o,b^o$</p>
<p><strong>KKT条件是凸问题的充要条件，则原始问题与对偶问题完全等价</strong>，$w^o,b^o$就是最优解（前面已经说明过等价，这里再强调一下）</p>
<p><strong>于是可以得到：</strong>（<strong>可以由等价关系得：</strong>拉格朗日表达式对$w$求导得到见上求对偶问题最小化的时候，就已经求出来了内部极值$w^o,b^o$与$\alpha$的关系表达式；<strong>也可以由对偶问题</strong>的KKT的$w$梯度条件：</p>
<p>$\nabla_wL(w^o,\beta^o,\alpha^o)=0$得到）（顺便后记-6附上了这里的所有KKT条件表达式）：</p>
<script type="math/tex; mode=display">
w^o=\sum_{i=1}^N\alpha_i^oy_ix_i</script><p>对于$\alpha_i≥0$，至少存在一个$\alpha_j＞0$（反证法，若全$=0$，则回代上面公式得$w=0$，非解，矛盾；另外<strong>该乘子对应的样本点就是支持向量</strong>）</p>
<p>将此$\alpha_j$<strong>代入</strong>$y_j(w^o·x_j+b^o)-1=0$中，又由于$y_j^2=1$，$y_i$取值只有$1或-1$，则：</p>
<script type="math/tex; mode=display">
b^o=y_j-\sum_{i=1}^N\alpha_i^oy_i(x_i·x_j)</script><p>根据前面原始问题中超平面的定义，将两个参数代入超平面公式和决策函数公式即可，</p>
<script type="math/tex; mode=display">
最优超平面：w^ox+b^o=0\quad,最优决策函数：f(x)=sign(w^ox+b^o)</script><script type="math/tex; mode=display">
\therefore 于是分离超平面可以写成:\quad\sum_{i=1}^N\alpha_i^oy_i(x·x_i)+b^o=0（称作可分支持SVM的对偶形式）</script><script type="math/tex; mode=display">
分类决策函数:\quad f(x)=sign(\sum_{i=1}^N\alpha_i^oy_i(x·x_i)+b^o)</script><p><strong>支持向量的导出：</strong>将训练数据集中对应于$\alpha^o_i＞0$的样本点$(x_i,y_i)$的实例$x_i∈\mathbb R^n$称为<strong>支持向量</strong></p>
<p>（容易知道，因为仅对$\alpha_i^o＞0$的实例有$y_i(w^o·x_i+b^o)-1=0$，则该点$x_i$一定在间隔边界上）</p>
<p>可以看到：</p>
<p><strong>①分类决策函数只依赖于输入$x$和训练样本中的支持向量的内积</strong>。</p>
<p><strong>②训练的时候，$b^o,w^o$的取值只与$\alpha_i^o＞0$的样本$x_i$有关，也就是只和支持向量有关。</strong></p>
<p>这与前面定义的概念相符</p>
<h2 id="线性可分SVM学习算法总结"><a href="#线性可分SVM学习算法总结" class="headerlink" title="线性可分SVM学习算法总结"></a>线性可分SVM学习算法总结</h2><p>由上可得，线性可分支持向量机学习算法：略（整理整理就是了）</p>
<h2 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h2><p>我们引入<strong>松弛变量</strong>$\epsilon_i≥0$，使得$y_i(w·x_i+b)+\epsilon_i≥1$，也即$y_i(w·x_i+b)≥1-\epsilon_i$</p>
<p>同时目标函数变为$\frac{1}{2}||w||^2+C\sum_{i=1}^N\epsilon_i$（引入目的是使得对每个松弛变量$\epsilon_i$都支付一个代价$\epsilon_i$，代价也可以设定为$\epsilon^2$等）</p>
<p>$C&gt;0$是<strong>惩罚参数</strong>，值越大则对误分类的惩罚越大，反之反之</p>
<p>$C$是需要自己设定的</p>
<p>问题变成了<strong>软间隔最大化</strong>：</p>
<script type="math/tex; mode=display">
\min_{w,b,\epsilon}\frac{1}{2}||w||^2+C\sum_{i=1}^N\epsilon_i</script><script type="math/tex; mode=display">
s.t.\quad y_i(w·x_i+b)≥1-\epsilon_i\quad,i=1,2,\cdots,N</script><script type="math/tex; mode=display">
\epsilon_i≥0\quad,i=1,2,\cdots,N</script><p>需求解使得目标函数最优的参数$w,b,\epsilon$</p>
<p>此目标函数即包含了①前项表使得间隔尽量小②后项表使得误分类点尽量少，$C$即是二者的调和系数</p>
<p>注意用拉格朗日乘子法依然是<strong>减去</strong>乘子项，依然注意求得的对偶问题约束条件</p>
<p>对于求得的对偶约束条件被用来消去之后，$C-\alpha_i-\mu_i=0,\alpha_i≥0,\mu_i≥0$会被合写为$0≤\alpha_i≤C$，且可以证得不能取等号，即为：$0＜\alpha_i＜C$</p>
<p>最后推得的结果会发现，与线性可分SVM的区别在于<strong>约束条件多了一个限制</strong>：$0＜\alpha_i＜C,i=1,2,\cdots,N$，在计算$w^o$的时候，<strong>选取的$\alpha^o_j$需要考虑此条件</strong>（也就是说不符合此条件的$\alpha^o_j$对应的$x_j$会被忽略掉，或者说<strong>不拿来训练模型</strong>。<strong>这些点就是不可分点、误分类点、非支持向量点</strong>。我们可以<strong>类比线性可分SVM</strong>，在线性可分SVM中计算$w$时候，选$\alpha_j$的约束条件为$\alpha^o_j＞0$，也就是说只获取支持向量（事实上此约束不成立的点即$\alpha^o_j=0$，回代$w$即发现$x_j$与原式无关，自然也就等同于“选择$\alpha^o_j＞0$”，但在线性SVM的约束$0＜\alpha_i＜C,i=1,2,\cdots,N$中，就真的是要选择了））</p>
<p><strong>具体来说</strong>：对于实例$x_i$到间隔边界的距离为$\frac{\epsilon_i}{||w||}$。</p>
<p>若$\alpha^o_i＜C$，则$\epsilon_i=0$，支持向量$x_i$恰好落在间隔边界上；</p>
<p>若$\alpha^o_i=C,0＜\epsilon_i＜1$，则分类正确，$x_i$在间隔边界与分离超平面之间；</p>
<p>若$\alpha^o_i=C,\epsilon_i=1$，则$x_i$在分离超平面上；若$\alpha^o_i=C,\epsilon_i＞1$，则$x_i$位于分离超平面误分类一侧。</p>
<p>$b$的解可能不是唯一的（但一般实践只会出现一种情况）</p>
<p>整体推导方法和线性可分SVM差不多，这里<strong>略</strong></p>
<p>合页损失函数是线性SVM的另一种结束，这里<strong>略</strong></p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol>
<li><p><strong>凸二次规划问题</strong></p>
<script type="math/tex; mode=display">
 \min_w f(w)</script><script type="math/tex; mode=display">
 s.t. \quad g_i(w)≤0\quad,i=1,2,\cdots,k</script><script type="math/tex; mode=display">
 s.t. \quad h_i(w)=0\quad,i=1,2,\cdots,l</script><p> 其中目标函数$f(w)$和约束函数$g_i(w)$都是$\mathbb R^n$上的连续可微<strong>凸函数</strong>，约束函数$h_i(w)$是$\mathbb R^n$上的仿射函数。</p>
<p> 在此之上，当目标函数$f(w)$是<strong>二次函数</strong>且约束函数$g_i(w)$是<strong>仿射函数</strong>时，上述凸最优化问题转化为凸二次规划问题。</p>
<p> <strong>凸二次规划问题满足拉格朗日对偶性中的Slater条件和KKT条件</strong></p>
</li>
<li><p>2-范数求导</p>
<p>$\frac{\partial\frac{1}{2}||w||^2}{\partial w}=\frac{\frac{1}{2} \partial(\sqrt{w_1^2+w_2^2+\cdots+w_n^2})^2}{\partial (w_1,w_2,\cdots,w_n)}=\frac{\frac{1}{2} \partial (w_1^2+w_2^2+\cdots+w_n^2)}{\partial (w_1,w_2,\cdots,w_n)}$</p>
<p>$=(\frac{\frac{1}{2} \partial w_1^2}{\partial (w_1,w_2,\cdots,w_n)},\frac{\frac{1}{2} \partial w_2^2}{\partial (w_1,w_2,\cdots,w_n)},\cdots,\frac{\frac{1}{2} \partial w_n^2}{\partial (w_1,w_2,\cdots,w_n)})$</p>
<p>对于每个子分子中的求导变量$w_i$，分母中只有$w_i$为对应维度的变量，所以是对每个维度分别求导</p>
<p>$\therefore 原式 = (w_1,w_2,\cdots,w_n)=w$</p>
</li>
<li><p>范数计算</p>
<p>$x$是向量，$x$的2-范数：$||x||=||x·x^T||$</p>
<p>$\therefore ||w||^2=w^Tw$</p>
<p>$\therefore ||\sum_{i=1}^N\alpha_iy_ix_i||=(\sum_{i=1}^N\alpha_iy_ix_i)·(\sum_{j=1}^N\alpha_jy_jx_j)^T=\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i·x_j)$</p>
<p><strong>和式就两两交叉相点乘</strong></p>
<p>这个式子中只有$x$是向量，$\alpha,y$都视为$x$向量的系数</p>
</li>
<li><p>注意求和函数的作用域，如果作用域内还有迭代变量，不能直接局部求和</p>
</li>
<li><p>$w·x=w^Tx$</p>
</li>
<li><p>上面学习算法中对偶问题的KKT表达式</p>
<p> $\nabla_w(w^o,b^o,\alpha^o)=w^o-\sum_{i=1}^N\alpha_i^oy_ix_i=0$</p>
<p> $\nabla_b(w^o,b^o,\alpha^o)=-\sum_{i=1}^N\alpha_i^oy_i=0$</p>
<p> $\alpha_i^o(y_i(w^o·x_i+b^o)-1)=0\quad,i=1,2,\cdots,N$</p>
<p> $y_i(w^o·x_i+b^o)-1≥0\quad,i=1,2,\cdots,N$</p>
<p> $\alpha_i^o≥0\quad,i=1,2,\cdots,N$</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑斯谛回归模型</title>
    <url>/2019/11/06/%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="Binomial-Logistic-Regression-Model"><a href="#Binomial-Logistic-Regression-Model" class="headerlink" title="Binomial Logistic Regression Model"></a>Binomial Logistic Regression Model</h2><p>LR是最基本的模型之一。</p>
<p>二项逻辑斯谛回归模型</p>
<script type="math/tex; mode=display">
P(Y=1|x)=\frac{e^{wx+b}}{1+e^{wx+b}}</script><script type="math/tex; mode=display">
P(Y=0|x)=\frac{1}{1+e^{wx+b}}</script><p>$x∈\mathbb R^n$为输入，$Y∈\{0,1\}$为输出，$w∈\mathbb R^n$和$b∈R$是参数，$w$为权值，$b$为偏置，$wx$是$w·x$的简写，为内积</p>
<a id="more"></a>
<p>$P(Y=0|x)=1-P(Y=1|x)$</p>
<p><strong>简写：</strong>有时候可以把$b$包含进$w$里，$wx+b$写成$wx$，此时：</p>
<p>$w=(w^{(1)},w^{(2)},\cdots,w^{(n)},b)^T$，$x=(x^{(1)},x^{(2)},\cdots,x^{(n)},1)$</p>
<script type="math/tex; mode=display">
P(Y=1|x)=\frac{e^{wx}}{1+e^{wx}},P(Y=0|x)=\frac{1}{1+e^{wx}}</script><p><strong>对数几率：</strong>$\log(p=1)=\log\frac{p}{1-p}=\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=wx$（内积 $w·x$）</p>
<p><strong>也就是说：输出$Y=1$的对数几率是输入$x$的线性函数，</strong>或者说输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，也就是逻辑斯谛回归模型。<strong>所以说LR模型是一个线性对数模型（MEM也是个对数线性模型）</strong></p>
<p><strong>理解</strong>：$wx$为一个对$x$进行分类的线性函数（线性分类器），使用LR的定义将其转化为了一个概率：</p>
<p>$P(Y=1|x)=\frac{e^{wx}}{1+e^{wx}}$。线性函数的值越接近于正无穷，概率值越接近于1；其值越小，概率值越趋近于0。</p>
<p><strong>更本质地理解：</strong>其实逻辑斯谛模型就是$P(Y=1|x)=g(wx)$，相当于将线性分类器$wx,x∈\mathbb R^n$映射到了概率区间$(0,1)$上，即$g(wx)∈(0,1)$。见S形曲线。</p>
<p>LR分布与LRM曲线，略</p>
<p>逻辑斯谛分布的<strong>分布函数</strong>属于逻辑斯谛函数，<strong>逻辑斯谛回归模型</strong>$P(Y=1|x)$函数也属于逻辑斯谛函数。</p>
<p>逻辑斯谛函数都<strong>呈S形曲线</strong></p>
<p><strong>对于决策</strong>：分类器输出的是概率，那么给定阈值如$th=0.5$，则$x_i$的输出标签为：</p>
<p>$f(x_i)=\begin{cases} 1,f(wx)≥0.5\\ 0,f(wx)＜0.5 \end{cases}$</p>
<h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>对给定训练集：$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i∈\mathbb R^n,y_i∈\{0,1\}$</p>
<p>设$P(Y=1|x)=\pi(x)$，则$P(Y=0|x)=1-\pi(x)$</p>
<p>$y_i$表示第$i$项的输出</p>
<p><strong>注意这里如何构造MLE表示：</strong></p>
<p><strong>LR的构造MLE表示的想法和MEM一样，都是想构造指数函数来表示多分类的多项之积，但具体的构造方式「完全不同」</strong></p>
<script type="math/tex; mode=display">
MLE_{P(y|x)}=\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}</script><p><strong>这个式子将多分类的多项之积的MLE写成一个式子，当$y_i=1$时，只有$[\pi(x_i)]^{y_i}$会生效，$[1-\pi(x_i)]^{1-y_i}$由于指数部分为0使得该子项等于1；反之$y_i=0$时，只有后半部分会生效。</strong></p>
<p>于是其对数似然函数为$L(w)=\log MLE_{P(y|x)}$，即：</p>
<script type="math/tex; mode=display">
L(w)=\sum_{i=1}^N[y_i\log \pi(x_i)+(1-y_i)\log(1-\pi(x_i))]</script><p>化简之：$=\sum_{i=1}^N[y_i\log \pi(x_i)+\log(1-\pi(x_i))-y_i\log(1-\pi(x_i))]$</p>
<p>$=\sum_{i=1}^N[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))]$</p>
<p>所以可得</p>
<script type="math/tex; mode=display">
L(w)=\sum_{i=1}^N[y_i(w·x)-\log(1+e^{w·x})]</script><p>以此对数似然函数为极大化目标求出$w$即为模型参数，常用牛顿拟牛顿法，梯度下降法</p>
<p>以梯度下降法为例，求$\nabla_wL(w)$，然后在各个$w_i$的方向上梯度下降，这里写总式：$w\to w+\epsilon\nabla_wL(w)$</p>
<h2 id="多项逻辑斯谛回归"><a href="#多项逻辑斯谛回归" class="headerlink" title="多项逻辑斯谛回归"></a>多项逻辑斯谛回归</h2><p>$Y$的取值集合为$\{1,2,\cdots,K\}$</p>
<script type="math/tex; mode=display">
P(Y=k|x)=\frac{e^{w_kx}}{1+\sum_{k=1}^{K-1}e^{w_kx}},k=1,2,\cdots,K-1</script><script type="math/tex; mode=display">
P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}e^{w_kx}}</script><p>$x∈\mathbb R^{n+1},w_k∈\mathbb R^{n+1}$</p>
<p>这里的MLE构造就可以用和MEM一样的构造方法来构造了</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>拉格朗日乘子与对偶性</title>
    <url>/2019/11/05/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>假设$f(x),c_i(x),h_j(x)$是定义在$\mathbb R^n$上的连续可微函数。考虑约束最优化问题</p>
<script type="math/tex; mode=display">
\min_{x∈\mathbb R^n}f(x)</script><script type="math/tex; mode=display">
s.t.\quad c_i(x)≤0,i=1,2,\cdots,k</script><script type="math/tex; mode=display">
s.t.\quad h_j(x)=0,j=1,2,\cdots,l</script><p>称此约束最优化问题为原始最优化问题或原始问题</p>
<a id="more"></a>
<h2 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h2><p>对于原始问题，要求解就用拉格朗日乘子法：</p>
<p>$L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)$</p>
<p>这里$x=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T∈\mathbb R^n\quad\alpha_i≥0,\beta_j$是拉格朗日乘子。（<strong>不等式约束的乘子$\alpha$要规定$\alpha≥0$</strong>，是为了满足后文KKT的对偶可行条件）</p>
<p><strong>「拉格朗日乘子法将有约束问题转化为了无约束问题」</strong></p>
<p>那么我们就要<strong>对$L(x,\alpha,\beta)$求极值</strong>。这个式子$x,\alpha,\beta$都是变量，也就是说我们要求出多元变量式子的极值。</p>
<p>有的情况可以求出极值（给了足够多个方程可以解出算子，如HMM的EM中包含一个隐藏方程可以解出算子），但有的就无法直接求了。于是我们想要<strong>分离</strong>$x$和算子，于是我们将表达式改写为广义拉格朗日极小极大。</p>
<p>同时由于$\nabla_xL(x,\alpha,\beta)=\nabla_xf(x)$，所以这么转换是等价的，但$\nabla_\alpha L(x,\alpha,\beta)=0,则c(x)=0$，显然约束条件不等价，改写成了广义拉格朗日极小极大才使得式子和原式完全等价。</p>
<h2 id="广义拉格朗日极小极大形式"><a href="#广义拉格朗日极小极大形式" class="headerlink" title="广义拉格朗日极小极大形式"></a>广义拉格朗日极小极大形式</h2><p>尝试构造这个最大化</p>
<p>$\max_{\alpha,\beta:\alpha≥0} L(x,\alpha,\beta)=\max\{f(x)+\sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)\}$</p>
<p><strong>会发现一个特点</strong>：</p>
<p>如果原始问题的约束条件$c_i(x)≤0,h_j(x)=0$不成立，也就是说$c_i(x)≥0,h_j(x)≠0$，那为了让$L$最大，$\alpha_i$就会取$+∞$，$\beta_j$就会取$-sign(h_j(x))·∞$，这时候$\max_{\alpha,\beta:\alpha≥0}=+∞$</p>
<p>显然这是不可以的。</p>
<p>只有当约束条件$c_i(x)≤0,h_j(x)=0$成立的时候，才不会出现这种情况。因为这种情况下为了让$L$最大，$\alpha_i$就会取$0$，同时$h_j(x)=0$</p>
<p>所以当约束条件成立的时候，可以得到这个等式：$\max_{\alpha,\beta:\alpha≥0} L(x,\alpha,\beta)=f(x)$</p>
<p><strong>也就是说，Intro中的那串定义，包含约束条件的$f(x)$可以由一个式子表示：$\max_{\alpha,\beta:\alpha_i≥0} L(x,\alpha,\beta)$，成功地将算子和$x$分离开来，同时与原式完全等价。</strong>里面求使$L$最大化的算子，外面求使$L$最小化的$x$。</p>
<p>于是我们要求约束$c_i(x)≤0,h_j(x)=0$下的最优化问题$\min_x f(x)$，即是要求$\min_x\max_{\alpha,\beta:\alpha_i≥0} L(x,\alpha,\beta)$</p>
<p>这就是<strong>广义拉格朗日极小极大问题：</strong></p>
<script type="math/tex; mode=display">
原始问题等价于\min_x\max_{\alpha,\beta:\alpha≥0} L(x,\alpha,\beta)</script><p>设$d^o=\arg \min_x\max_{\alpha,\beta:\alpha≥0} L(x,\alpha,\beta)$</p>
<p><strong>「为什么要转化为极小极大形式：使得与原式完全等价，且对所有情况都可以求解。如果直接求拉格朗日乘子式的极值，在乘子多于已知方程的情况下无法求解」</strong></p>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>原始问题的广义拉格朗日极小极大问题$\min_x\max_{\alpha,\beta:\alpha≥0} L(x,\alpha,\beta)$的<strong>对偶问题为：</strong></p>
<script type="math/tex; mode=display">
\max_{\alpha,\beta:\alpha≥0} \min_x L(x,\alpha,\beta)</script><p>设$p^o=\max_{\alpha,\beta:\alpha≥0} \min_x L(x,\alpha,\beta)$</p>
<p><strong>「为什么要有对偶问题：对偶问题都是凸优化问题，而凸优化问题局部极值=全局极值而且更好求解。」</strong></p>
<p>（在ML中的凸优化是指函数呈<strong>U型</strong>！）</p>
<p><strong>举例</strong>：在MEM中的含等式约束问题，就是先求里面的极小化问题以$x$为变量求出最小的$P_w$，然后回代，只需要再求出能使$P_w$极大化的$w$即可，外部极大化$P_w$可证等于MLE，具体的方法，如IIS，牛顿法与伪牛顿法，梯度下降。</p>
<h3 id="弱对偶性"><a href="#弱对偶性" class="headerlink" title="弱对偶性"></a>弱对偶性</h3><p><strong>定理$1^o$：</strong>若原始问题和对偶问题都有最优值，则：</p>
<p><strong>对偶问题≤原始问题（或其广义拉格朗日极小极大问题）</strong></p>
<script type="math/tex; mode=display">
p^o=\max_{\alpha,\beta:\alpha≥0} \min_x L(x,\alpha,\beta)≤\arg \min_x\max_{\alpha,\beta:\alpha≥0} L(x,\alpha,\beta)=d^o</script><p>很容易理解，因为$L$中最小的一个的最大值肯定比最大的一个的最小值要小。此式即为弱对偶性。</p>
<p>此式就可以求出原始问题的下限。</p>
<hr>
<p><strong>何时相等（①强对偶性②是最优解）？</strong>即何时「解原始问题最优化=解对偶问题最优化」所以需要KKT定理，下面一步一步引出：</p>
<p><strong>推论$1^o$：</strong> 设$x^o$和$\alpha^o,\beta^o$分别是原始问题和对偶问题的可行解，并且$d^o=p^o$，则$x^o$是原始问题的最优解，$\alpha^o,\beta^o$是对偶问题的最优解。 </p>
<p>注意<strong>解</strong>是$\alpha,\beta,x$，<strong>值</strong>是式子的值$p^o,d^o$</p>
<p>翻译：如果原始问题的值和对偶问题的值相等，那么两个问题对应的参数是最优解</p>
<h3 id="强对偶性（还不等价）-Slater条件"><a href="#强对偶性（还不等价）-Slater条件" class="headerlink" title="强对偶性（还不等价）-Slater条件"></a>强对偶性（还不等价）-Slater条件</h3><p><strong>定理$2^o$：</strong>考虑原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是<strong>凸函数</strong>（凸优化问题），$h_j(x)$是<strong>仿射函数</strong>；并且假设<strong>不等式约束</strong>$c_i(x)$是严格可行的，即存在$x$，对所有$i$有$c_i(x)&lt;0$，则存在$x^o$和$\alpha^o,\beta^o$，使$x^o$是原始问题的解，$\alpha^o,\beta^o$是对偶问题的解，并且有$p^o=d^o=L(x^o,\alpha^o,\beta^o)$。此即<strong>Slater条件</strong>。</p>
<p>翻译：在某些条件下，<strong>存在解（不一定是最优解）</strong>且使得原始问题=对偶问题，<strong>即强对偶性成立：</strong></p>
<script type="math/tex; mode=display">
原始问题等价于\max_{\alpha,\beta:\alpha≥0} \min_x L(x,\alpha,\beta)</script><p>「也就是说只要满足<strong>目标函数和不等式约束为凸函数</strong>+<strong>等式约束为仿射函数</strong>，<strong>那么强对偶性就成立</strong>」</p>
<p><strong>注意：强对偶性存在的参数解可能有很多个（鞍点），并不能保证是极值点（最优解）！</strong></p>
<p><strong>于是</strong>下面KKT条件便是确保鞍点便是原函数最优解的充分条件 </p>
<h3 id="强对偶性-是最优解-完全等价-KKT条件"><a href="#强对偶性-是最优解-完全等价-KKT条件" class="headerlink" title="强对偶性+是最优解=完全等价-KKT条件"></a>强对偶性+是最优解=完全等价-KKT条件</h3><p>一个问题是不等式约束/等式问题，它的<strong>最优解</strong>就一定满足KKT条件。（必要条件，非充分；但如果是凸函数约束优化（强对偶成立），就变成了充分必要条件） </p>
<p><strong>定理$3^o$：</strong>在定理2的条件的基础上，$x^o,α^o,β^o$分别是原始问题和对偶问题的解（<strong>是最优解，且强对偶</strong>）的<strong>充分必要条件是</strong>$x^o,α^o,β^o$满足下面的<strong>KKT(Karush-Kuhn-Tucker)条件</strong>：</p>
<p><strong>定理$3^o$：在定理2的条件的基础上，$x^o,\alpha^o,\beta^o$分别是原始问题和对偶问题的解的充分必要条件</strong>是，$x^o,\alpha^o,\beta^o$ 满足下面的<strong>KKT(Karush-Kuhn-Tucker)条件</strong>：</p>
<script type="math/tex; mode=display">
\nabla_xL(x^o,\alpha^o,\beta^o)=0 \quad①</script><script type="math/tex; mode=display">
\alpha^o_ic_i(x^o)=0,i=1,2,\cdots,k\quad②</script><script type="math/tex; mode=display">
c_i(x^o)≤0,i=1,2,\cdots,k\quad③</script><script type="math/tex; mode=display">
\alpha_i^o≥0,i=1,2,\cdots,k\quad④</script><script type="math/tex; mode=display">
h_j(x^o)=0,j=1,2,\cdots,l\quad⑤</script><p>可以看出有几大类条件：①梯度为0条件；③⑤原约束条件（③是不等式约束，⑤是等式约束）；②对偶互补条件（ 互补松弛条件 ）；④ 对偶可行条件 </p>
<p>（注意哪些是不等式约束条件，哪些是等式约束条件，不一样的）</p>
<p>第二个式子称为<strong>KKT的对偶互补条件</strong>，由此条件可知，若对偶可行条件成立：$\alpha_i^o＞0,则c_i(x^o)=0$。</p>
<p>（这个性质导出了SVM中的支持向量）</p>
<p>任何满足强对偶性的优化问题，只要其目标函数与约束函数可微，<strong>任一对原始问题与对偶问题的解</strong>都是满足 KKT 条件的。 </p>
<p><strong>「KKT条件是凸问题的充分必要条件」（凸函数=&gt;KKT，KKT=&gt;凸函数）</strong></p>
<p>这是个<strong>充分必要</strong>条件， <strong>一般仅用KKT条件来「验证」找到的解是最优解</strong> </p>
<p><strong>对于凸优化</strong>，由于与KKT条件是充要条件，那么就可以用KKT条件求出最优解，也可以用回代原始问题求最优解（因为等价）</p>
<p>KKT的推导过程可以看 <a href="https://www.cnblogs.com/mo-wang/p/4775548.html" target="_blank" rel="noopener">深入理解拉格朗日乘子法和KKT条件</a></p>
<h2 id="关于零导数法求极值"><a href="#关于零导数法求极值" class="headerlink" title="关于零导数法求极值"></a>关于零导数法求极值</h2><p>对于极值问题，使用零导数法。</p>
<p>假定求$\min_{x,y,z}L(x,y,z),\quad L(x,y,z)=f(x)+g(y)+q(z)$</p>
<p>就可以对各个变量分别求偏导零导数：</p>
<p>$\nabla L(x,y,z)=0 \to \nabla_x f(x)=0,\nabla_y g(y)=0,\nabla_z q(z)=0 $</p>
<hr>
<p>但如果各个子式之间变量无法分离，如：</p>
<p>$\min_{x,y,z}L(x,y,z),\quad L(x,y,z)=f(x,y)+g(y,z)+q(x,y,z)$</p>
<p>那就得考虑如$\nabla_{x,y}f(x,y)=0$同时满足$x,y$，需要用<strong>多元函数求极值的方法</strong>。</p>
<p>具体就是</p>
<p>①解$\nabla_{x}f(x,y)=0,\nabla_{y}f(x,y)=0$，求出一切驻点。</p>
<p>②对于每一个驻点$(x_0，y_0)$，求出二阶偏导数的值A、B、C</p>
<p>③定出$AC-B^2$的符号，按充分条件进行判定$f(x_0，y_0)$是否是极大值、极小值。 </p>
<p>这只是二元不独立的情况，如果变量更多，是<strong>很难求甚至没法求的。所以这种情况下一般不用零导数法</strong>（比如IIS中就是因为这个原因要构造第二下界来求解）</p>
<p>（另外也要根据函数本身判断）</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>最大熵模型（二）</title>
    <url>/2019/11/04/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h2 id="最大熵模型求解"><a href="#最大熵模型求解" class="headerlink" title="最大熵模型求解"></a>最大熵模型求解</h2><h3 id="mathbb-S1-o-拉格朗日乘子法与拉格朗日对偶性"><a href="#mathbb-S1-o-拉格朗日乘子法与拉格朗日对偶性" class="headerlink" title="$\mathbb S1^o$拉格朗日乘子法与拉格朗日对偶性"></a>$\mathbb S1^o$拉格朗日乘子法与拉格朗日对偶性</h3><p>使用拉格朗日乘子法：</p>
<p>$L(P,w)=-H(P)+w_0(1-\sum_yP(y|x))+\sum_{i=1}^nw_i(\mathbb E_{\hat P}(f_i)-\mathbb E_{P}(f_i))$</p>
<p>$=\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)+w_0(1-\sum_yP(y|x))$</p>
<p>$+\sum_{i=1}^nw_i(\sum_{x,y}\hat P(x,y)f_i(x,y)-\sum_{x,y} \hat P(x)P(y|x)f_i(x,y))$</p>
<p>难以求极值，<strong>转化为广义拉格朗日极大极小问题</strong>，于是我们需要优化：</p>
<script type="math/tex; mode=display">
\min_{P∈\mathbb C}\max_wL(P,w)</script><p>由于拉格朗日函数$L(P,w)$是$P$的凸函数，<strong>满足KKT条件</strong>，<strong>于是该优化问题等价于其对偶问题</strong>：</p>
<script type="math/tex; mode=display">
\max_w \min_{P∈\mathbb C}L(P,w)</script><p>（<strong>更多阅读拉格朗日乘子和对偶性专题</strong>）</p>
<a id="more"></a>
<h3 id="mathbb-S2-o-内部求仅含-x-变量的0导数"><a href="#mathbb-S2-o-内部求仅含-x-变量的0导数" class="headerlink" title="$\mathbb S2^o$内部求仅含$x$变量的0导数"></a>$\mathbb S2^o$内部求仅含$x$变量的0导数</h3><p>先解决内部的极小化问题。</p>
<p>由于$L(P,w)$是凸函数，那对其直接求0导数即可</p>
<p>记$\Phi(w)=\min_{P∈\mathbb C}L(P,w)=L(P_w,w)$，$\Phi(w)$称为对偶函数</p>
<p>则对偶函数的解$P_w=\arg\min_{P∈\mathbb C}L(P,w)=P_w(y|x)$ ，即：求使得$L(P,w)$最小的$P(y|x)$</p>
<p><strong>于是使用0导数法</strong>： （易错。见HMM中求和函数求导）</p>
<p>$\frac{\partial L(P,w)}{\partial P(y|x)}=\hat P(x)\log P(y|x)+\hat P(x)-w_0-\hat P(x)\sum_{i=1}^nw_if_i(x,y)=0$</p>
<p><strong>为了利用更多分布信息，左右两边同时求$\sum_{x,y}$</strong>（就可以给$w_0$项再凑一个$\hat P(x)$来消去所有的$\hat P(x)$）</p>
<p>$\therefore \sum_{x,y}\hat P(x)\log P(y|x)+\sum_{x,y}\hat P(x)-\sum_{x,y}w_0-\sum_{x,y}\hat P(x)\sum_{i=1}^nw_if_i(x,y)=0$</p>
<p>由于$\sum_{x,y}\hat P(x)=1且w_0与x,y不相关$，$\therefore \sum_{x,y}w_0=w_0\sum_{x,y}=w_0\sum_{x,y}\hat P(x)$</p>
<p>$\therefore \sum_{x,y}\hat P(x)(\log P(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y))=0$</p>
<p>又经验分布$\hat P(x)&gt;0$，那么该乘式必然右边部分$=0$</p>
<p>$\therefore \log P(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y)=0$ </p>
<p>$\therefore P(y|x)=e^{\sum_{i=1}^nw_if_i(x,y)+w_0-1}=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}}$</p>
<p>$\because \sum_{y}P(y|x)=1$，所以为了归一化$P(y|x)$，引入归一化因子</p>
<p>$\therefore  P_w(y|x)=\frac{1}{Z_w(x)}e^{\sum_{i=1}^nw_if_i(x,y)}$，$Z_w(x)=\sum_y e^{\sum_{i=1}^nw_if_i(x,y)}$</p>
<p>（注意$Z_w(x)$式子中$y$被边缘化了，所以$Z_w(x)$只与$x$有关）</p>
<p>其中$Z_w$称为规范化因子，$f_i(x,y)$是特征函数，$w_i$是特征的权值。</p>
<p>$P_w=P_w(y|x)$就是最大熵模型，$w$是最大熵模型中的参数向量</p>
<h3 id="mathbb-S3-o-外部恰好等价于求极大似然估计"><a href="#mathbb-S3-o-外部恰好等价于求极大似然估计" class="headerlink" title="$\mathbb S3^o$外部恰好等价于求极大似然估计"></a>$\mathbb S3^o$外部恰好等价于求极大似然估计</h3><p>于是我们下一步要求的就是对偶问题外部极大化问题：</p>
<script type="math/tex; mode=display">
\max_w \Phi(w)</script><script type="math/tex; mode=display">
w^o=\arg\max_w\Phi(w)</script><p>（发现这个式子就可以用最优化算法来求了）</p>
<p><strong>对偶函数的极大化等价于最大熵模型的极大似然估计</strong>：$\Phi(w)=L_{\hat P}(P_w)$ </p>
<p>证明：将$\mathbb S2^o$中求出来的$P_w(y|x)$表达式代入$\mathbb S2^o$中的$L(P,w)$，即为对偶函数，恰好等于$P_w(y|x)$的似然函数$L_{\hat P}(P_w)$（见$\mathbb S4^o$中）。</p>
<p><strong>也就是说，我们对上面已经求得的内部最小化一般形式MEM：$P_w$，求出$w$使得$MLE_{P_w}$最大即可，也是MEM学习过程</strong></p>
<p><strong>可以将MEM表示为更一般的形式【重要】：</strong></p>
<script type="math/tex; mode=display">
P_w(y|x)=\frac{1}{Z_w(x)}e^{\sum_{i=1}^nw_if_i(x,y)}</script><script type="math/tex; mode=display">
Z_w(x)=\sum_y e^{\sum_{i=1}^nw_if_i(x,y)}</script><p>$x∈\mathbb R^n$为输入，$y∈\{1,2,\cdots,K\}$为输出，$w∈\mathbb R^n$为权值向量，$f_i(x,y),i=1,2,\cdots,n$为任意实值的特征函数</p>
<p><strong>【MEM的学习目标就转为学$w$】</strong></p>
<p><strong>形象化地举例：</strong></p>
<p>假如我们知道昨天天气是雨$x_1$，今天天气是雷暴$x_2$，那么想要预测明天天气$y$，就可以给出：</p>
<p>$P(y|x_1,x_2,subject)=\frac{e^{w_1(x_1,x_2,y)+w_2(subject,y)}}{Z(x_1,x_2,subject)}$</p>
<p>这里$f_1(x_1,x_2,y),f_2(subject,y)$即为特征函数，即是约束条件，比如可以设定$f_1(x_1,x_2,y)=$三天都是坏天气（自己定义什么是坏天气）不同时发生，等等。$w_1,w_2$即为权重</p>
<p><strong>确定参数$w$的过程就是训练模型的过程</strong></p>
<hr>
<p><strong>于是我们开始求MLE表达式：</strong></p>
<p><strong>先求出$P_w$的对数似然估计函数：</strong></p>
<p>$L_{\hat P}(P_w)=\log\prod_{i=1}^nP(y_i|x_i)=P(y_1|x_1)P(y_2|x_2)\cdots P(y_n|x_n)$</p>
<p><strong>将相同的样本乘在一起。【注意这里构造MLE表达式】</strong>对相同的一组$x,y$，那么有$t$组指数就会为$t$，也就相当于乘了$t$次，这样对所有相同的$x,y$，都可以表示为$P(y|x)^{||(x,y)||_0}$，则对所有不同和相同的$x,y$，最终表示成了：$L_{\hat P}(P_w)=\log\prod_{x,y}P(y|x)^{||(x,y)||_0}$</p>
<p>极大化这个$L_{\hat P}(P_w)$也等同于极大化（指数部分分母除以$N$）：$L_{\hat P}(P_w)=\log\prod_{x,y}P(y|x)^{\hat P(x,y)}$</p>
<p><strong>于是MLE可以表示为【重要】：</strong></p>
<script type="math/tex; mode=display">
L_{\hat P}(P_w)=\log\prod_{x,y}P(y|x)^{\hat P(x,y)}=\sum_{x,y}\hat P(x,y)\log P(y|x)</script><p>$\therefore L_{\hat P}(P_w)=\sum_{x,y}\hat P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\hat P(x,y)\log Z_w(x)$</p>
<p>$=\sum_{x,y}\hat P(x,y)\sum_{i=1}^nw_if_i(x)-\sum_{x}\log Z_w(x)\sum_y\hat P(x,y)$ （拆离只含$x$变量的$Z_w(x)$）</p>
<p>$=\sum_{x,y}\hat P(x,y)\sum_{i=1}^nw_if_i(x)-\sum_{x}\hat P(x)\log Z_w(x)$ （$y$被边缘化）</p>
<p>（<strong>注意</strong>这里MLE的化简！）</p>
<p><strong>最终形式：</strong></p>
<script type="math/tex; mode=display">
L(w)=\sum_{x,y}\hat P(x,y)\sum_{i=1}^nw_if_i(x)-\sum_{x}\hat P(x)\log Z_w(x)</script><h3 id="mathbb-S4-o-求解外部的极大似然估计"><a href="#mathbb-S4-o-求解外部的极大似然估计" class="headerlink" title="$\mathbb S4^o$求解外部的极大似然估计"></a>$\mathbb S4^o$求解外部的极大似然估计</h3><p>MEM的学习目标就转为学$w$后，我们就求解其$MLE_w$即可</p>
<p>IIS，梯度下降，牛顿法或拟牛顿法都可以求解，可以看前面的文章</p>
<p><strong>IIS：</strong></p>
<p>展开似然函数表达形式：$L(w)=\sum_{x,y}\hat P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\hat P(x)\log Z_w(x)$为目标函数</p>
<p>求$\arg\max_wL(w)$</p>
<p><strong>牛顿法：</strong></p>
<p>展开似然函数表达形式：$L_1(w)=\sum_x\hat P(x)\log \sum_y e^{\sum_{i=1}^nw_if_i(x,y)}-\sum_{x,y}\hat P(x,y)\sum_{i=1}^nw_if_i(x,y)$为目标函数</p>
<p>求$\arg\min_wL_1(w)$</p>
<p><strong>梯度下降法：</strong></p>
<p>略</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ol>
<li><p>回顾<strong>理解复合函数期望</strong>：$\mathbb E_{g(x)}f(x)=\int_x g(x)f(x)$</p>
<p>若$f(x)$理解为关于$x$的概率分布，那么$g(x)$可以理解为$x$的权重</p>
</li>
<li><p>为什么要用<strong>对数的</strong>似然？——对数函数将乘法变为和</p>
</li>
<li><p>拉格朗日对偶性见专题</p>
</li>
<li><p>拉个朗日对偶性求解的外部$\max_{w}$为什么不用零导数法？因为将内部求出的表达式代入回去之后发现无法分离出独立变量的子式，无法用零导法来求</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>天气之子</title>
    <url>/2019/11/03/%E5%A4%A9%E6%B0%94%E4%B9%8B%E5%AD%90/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/11/03/天气之子/0.jpg" alt="0"></p>
<div style="text-align: center"><strong>Weathering With You</strong></div>
<div><br></div>
<div style="text-align: center">何もない僕たちに　なぜ夢を見させたか</div>
<div style="text-align: center">为何要让一无所有的我们怀揣梦想？</div>
<div><br></div>
<div style="text-align: center">終わりある人生に　なぜ希望を持たせたか</div>
<div style="text-align: center">为何要让我们对有尽头的人生抱有希望？</div>
<div><br></div>
<div style="text-align: center">なぜこの手をすり抜ける　ものばかり与えたか</div>
<div style="text-align: center">为何赐予我们的一切只在手中一掠而过？</div>
<div><br></div>
<div style="text-align: center">それでもなおしがみつく　僕らは醜いかい</div>
<div style="text-align: center">即便如此还要紧紧抓住的我们 丑陋吗？</div>
<div><br></div>
<div style="text-align: center">それとも、きれいかい</div>
<div style="text-align: center">还是说 很美呢？</div>

<a id="more"></a>
<hr>
<p>《天气之子》真的是很不诚哥，结局真的很出乎意料，少了时间的错位所造成的悲剧感。</p>
<p>但这大概也是诚哥对自我的一个突破吧。</p>
<div><br></div>]]></content>
      <categories>
        <category>Anime</category>
      </categories>
      <tags>
        <tag>新海诚</tag>
      </tags>
  </entry>
  <entry>
    <title>Utopia</title>
    <url>/2019/11/02/Utopia/</url>
    <content><![CDATA[<p>那是我第一次爬到教学楼的楼顶。<br>平时挂在门上的锁不见了<br>顶层储物间的门外即是天台。<br>我坐在天台边<br>双手向后撑在地面上<br>仰着身子朝向碧青色的澄澈天空。</p>
<a id="more"></a>
<p><div><br></div><br>轻轻嗅到的是夏日的清香<br>视野所及是微风拂过的草原<br>伸一伸手可以够得着天上粉色的云彩。<br>我喜欢这里<br>于是我每每下午放学后就来到这里。<br>有时会有青鸟飞过<br>停留在我的身边<br>叽叽喳喳；<br>有时会有松鼠露出头<br>和我一样坐在天台边<br>感受夏风的温凉。</p>
<p><div><br></div><br>我种下了星辰花<br>她们渐渐地开满了整个天台<br>他们成为了我的伙伴。<br>有时会有阵雨掠过<br>但他每次很快就离开。<br>我张开内心的柔软，追忆过去<br>描绘着门里面发生的故事<br>倾诉理想和未来。<br>那是这个世界唯一的乌托邦。</p>
<p><div><br></div><br>可是有一天<br>门被锁上了。<br>——原来是因为食堂的厨师<br>想起很久以前落了汤勺在储物间<br>回去拿的时候发现门没锁<br>就顺带锁上了。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27909216&auto=1&height=66"></iframe>]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title>最大熵模型（一）</title>
    <url>/2019/11/01/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="Maximum-Entropy-Theory-Intro"><a href="#Maximum-Entropy-Theory-Intro" class="headerlink" title="Maximum Entropy Theory Intro"></a>Maximum Entropy Theory Intro</h2><p>最大熵原理：学习概率模型时，所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用<strong>约束条件</strong>来确定概率模型的集合，所以最大熵原理也可以表述为<strong>在给定已知信息下，满足约束条件的模型集合中，选取熵最大的模型</strong>。</p>
<p>也就是说，在目前已观测到的所有信息下，最合理的对未知信息的预测就是对所有未知信息最不确定的推断（每种未知情况等可能）。$H(P)$表示$P$的不确定性，<strong>最不确定的情况=熵最大的情况=每种未知情况都等可能发生</strong>，这就给出了最好模型选择的准则</p>
<script type="math/tex; mode=display">
H(P)=-\sum_xP(x)\log P(x),其中0≤H(P)≤\log|X|</script><p>表示随机变量$X$的概率分布$P(X)$的熵，$|X|$表示$X$的取值个数。当且仅当$X$的分布是均匀分布时右边等号成立，这时候<strong>条件熵最大</strong>。即：在已知信息下，$P(X)$的熵最大</p>
<p>对于模型$P(X)$，能够最大化熵$H(P)$的模型$P(X)$即是最好的模型</p>
<a id="more"></a>
<h2 id="Maximum-Entropy-Model"><a href="#Maximum-Entropy-Model" class="headerlink" title="Maximum Entropy Model"></a>Maximum Entropy Model</h2><p>给定训练数据集:$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$</p>
<p>我们要求最准确的模型：$P(y|x)$</p>
<p>$1^o$ <strong>特征函数与经验分布</strong></p>
<p>①<strong>特征函数(feature function)（$X$的约束）</strong>：</p>
<p>$f(x,y)$表示训练数据中<strong>输入$x$和输出$y$之间</strong>的一个<strong>事实（关系）</strong>，只要满足事实（一般存在<strong>很多对</strong>满足该事实的$(x_i,y_i)$，特征函数也有很多$f_i$，即有多个事实约束），则$(x_i,y_i)$就能用该特征函数$f(x,y)$表示<strong>一组数据</strong></p>
<script type="math/tex; mode=display">
f(x,y)=

\begin{cases}

1,x与y满足某一事实\\ 

0,否则 

\end{cases}</script><p>此函数为一个二值函数，要么0要么1（也可以是任意实值函数）。事实可以来源于<strong>先验知识</strong>等。</p>
<p>举几例：①$f(x,y)= \begin{cases}1,x为抛硬币出现正面,y为\frac{1}{2}\\ 0,否则 \end{cases}$</p>
<p>②已知从箱子里顺序两次抽出红球和白球的概率为$\frac{3}{10}$：</p>
<p>$f(x_1,x_2,y)= \begin{cases}1,x_1=红且x_2=白,或x_1=白且x_2=红,y=\frac{3}{10}\\ 0,否则 \end{cases}$</p>
<p>③也可以想CRF中的特征函数的意义，是一样的</p>
<p><strong>注：</strong>输入$x$可以是很多个随机变量，所以实际应该表示为$f(\vec x,y)$或写成$f(x_1,x_2,\cdots,x_n,y)$，为了方便写，全都记成$f(x,y)$，同时一个模型中的特征函数也是可以有很多的，每个特征函数都表述了一组数据。</p>
<p>理解特征函数的意义：<strong>一是泛化表示</strong>，一个特征函数可以表示多组数据，相当于“从输入和输出中抽取了特征”，其实就是<strong>用来形式化表示X的相关知识信息</strong>。（比如从箱子里顺序两次抽出红球和白球的概率为$\frac{3}{10}$，就可以用特征函数来形式化的表示它）</p>
<p><strong>二是</strong>放入公式中这样也相当于给数据添加了一个<strong>约束条件</strong>，即是在Maximum Entropy Theory Intro部分所述的约束条件，其也可以相当于是一个已知的<strong>前提信息，先验知识</strong>。（比如从箱子里顺序两次抽出红球和白球的概率为$\frac{3}{10}$就是一个先验知识，这就是个模型约束条件）</p>
<p>这里的特征feature，和机器学习中的特征feature<strong>不是一个概念</strong></p>
<p>②<strong>经验分布</strong>即已知<strong>训练集的分布（$X$的分布）</strong>：</p>
<script type="math/tex; mode=display">
\hat P(X=x,Y=y)=\frac{||(X=x,Y=y)||_0}{N}</script><script type="math/tex; mode=display">
\hat P(X=x)=\frac{||(X=x,Y)||_0}{N}</script><p>其中$||||_0$表该数据样本出现的次数。</p>
<p>为什么要有经验分布？经验分布即训练集现在已知的分布，是在Maximum Entropy Theory Intro部分所述的<strong>已知部分前提信息</strong>。其中包含了已知信息$x,y$出现的概率，根据最大熵原理，这是我们应该作为已知事实考虑的。</p>
<p>那么如何将经验分布考虑进最大熵？只需要求概率分布期望的时候是<strong>对经验分布求期望</strong>即可，$x,y$的经验分布就相当于$x,y$的权重，见$2^o$。</p>
<p>③<strong>特征函数与经验分布一起构成了已知信息</strong></p>
<p>所谓最大化条件熵，其中的“条件”就是指的这已知信息，包含$X$的分布和约束</p>
<p>$2^o$ <strong>分布的期望</strong></p>
<p>回顾<strong>理解复合函数期望</strong>（见后记）</p>
<p>特征函数$f(x,y)$(表征了已知$X$的约束）关于经验分布$\hat P(X,Y)$（表征了已知$X$的分布）的期望值<strong>构成已知信息</strong>，也即经验分布中<strong>样本满足某个特征函数</strong>$f(x,y)$的期望：</p>
<script type="math/tex; mode=display">
\mathbb E_{\hat P}(f)=\sum_{x,y}\hat P(x,y)f(x,y)</script><p><strong>关键：根据最大熵原理，我们观察到的已知信息已经成为事实，在真实分布也应该符合在经验分布中的已知信息。换句话说——这个已知信息的期望无论在经验分布中还是真实分布中都应该是同样的。</strong></p>
<p><strong>通过贝叶斯公式展开构造出$P(y|x)$，正好是我们要求的模型。这就形成了一个对我们要求的模型$P(y|x)$的约束条件。</strong>只有符合这个条件，才能是我们想要的模型。</p>
<p>即：</p>
<script type="math/tex; mode=display">
\mathbb E_{P}(f)=\mathbb E_{\hat P}(f)</script><script type="math/tex; mode=display">
或,\sum_{x,y}\hat P(x)P(y|x)f(x,y)=\sum_{x,y}\hat P(x,y)f(x,y)</script><p><strong>$3^o$最大熵模型</strong></p>
<p>在$2^o$中根据已知信息得到了模型的约束条件，于是我们就可以得到<strong>满足这样的约束条件的模型集合</strong>。</p>
<p><strong>$Def.$ 最大熵模型 Maximum Entropy Model</strong>：假设满足所有约束条件的模型集合为</p>
<script type="math/tex; mode=display">
\mathbb C=\{P∈\mathbb P|E_P(f_i)=E_{\hat P}(f_i),i=1,2,\cdots,n \}</script><p>定义条件概率分布$P(Y|X)$上的条件熵为</p>
<script type="math/tex; mode=display">
H(P)=-\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)</script><p>则模型集合$\mathbb C$中条件熵$H(P)$最大的模型称为最大熵模型。式中的对数为自然对数。</p>
<h2 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a>最大熵模型的学习</h2><p>满足这样的模型有很多，哪个才是最好的？在Maximum Entropy Theory Intro部分已述，<strong>熵最大</strong>的时候最好。</p>
<p>最大熵模型的学习过程就是求解最大熵模型的过程，也就是<strong>在满足约束条件的模型中求取最大熵模型</strong>的过程</p>
<p>所以，</p>
<p>对于给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$以及特征函数$f_i(x,y),i=1,2,\cdots,n$，最大熵模型的学习等价于约束最优化问题：</p>
<script type="math/tex; mode=display">
\max_{P∈\mathbb C}H(P)=-\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)</script><script type="math/tex; mode=display">
使得,\mathbb E_{P}(f_i)=\mathbb E_{\hat P}(f_i),i=1,2,\cdots,n</script><script type="math/tex; mode=display">
\sum_yP(y|x)=1</script><p>按照优化问题的习惯，将其改写为等价的求最小值问题：</p>
<script type="math/tex; mode=display">
\min_{P∈\mathbb C}-H(P)=\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)</script><script type="math/tex; mode=display">
使得,\mathbb E_{P}(f_i)-\mathbb E_{\hat P}(f_i)=0,i=1,2,\cdots,n</script><script type="math/tex; mode=display">
\sum_yP(y|x)=1</script><p>求得上述问题的解即是MEM学习的解</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>Game Month!</title>
    <url>/2019/10/31/month/</url>
    <content><![CDATA[<div style="text-align: center">纪念一下</div>

<hr>
<div style="text-align: center"> <strong>[10月25日]the outer world</strong></div>
<div style="text-align: center"> <strong>[10月31日]路易吉鬼屋3</strong></div>
<div style="text-align: center"> <strong>[11月8日]           死亡搁浅</strong></div>
<div style="text-align: center"> <strong>[11月8日]      极品飞车：热度</strong></div>
<div style="text-align: center"> <strong>[11月15日]       宝可梦：剑盾</strong></div>
<div style="text-align: center"> <strong>[11月15日]  星球大战：组织陨落</strong></div>

<hr>
<div style="text-align: center">这20天只能用暴力形容...真是活久见...</div>
<div style="text-align: center"> <strong>定一个小目标，全部通关</strong></div>

]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>3A</tag>
        <tag>震惊</tag>
        <tag>暴力</tag>
      </tags>
  </entry>
  <entry>
    <title>改进的迭代尺度法</title>
    <url>/2019/10/31/Scaling/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>IIS和EM一样，也是一种<strong>构造下界计算近似MLE</strong>的优化算法。事实上IIS算法就是对GIS（Generalized Iterative Scaling，通用迭代法，本质也是一种EM算法）算法进行改进而来的。</p>
<p>IIS利用$\log$函数的性质，以及指数函数的凸性，对目标函数进行了两次缩放，来求解下界函数。 对于MEM和CRF，使用IIS比GIS收敛快很多。</p>
<p>在EM算法中，每次迭代获取到新的边界$B(\theta,\theta^i)$后，需要求$\theta^{i+1}=\arg\max_\theta B(\theta,\theta^i)$，简化$B$成$Q$函数后，即是要求$\frac{\partial Q}{\partial \theta}=0$，然后$\theta^i\to\theta^{i+1}$。在这种方法下，如果$\theta$维度很高又不独立的话（即模型参数很多，含有多个变量）的话，就不能对各参数分别求0偏导，需要全局优化。于是IIS另辟蹊径，采用了一次只优化$\theta$中的一个变量$\delta_i$，而固定其他变量$\delta_j$的方法。</p>
<p>IIS可以用在CRF，MEM等模型中（HMM属于一种特殊的CRF，也可以用，但效果没EM好）</p>
<a id="more"></a>
<h2 id="Improved-Iterative-Scaling"><a href="#Improved-Iterative-Scaling" class="headerlink" title="Improved Iterative Scaling"></a>Improved Iterative Scaling</h2><p>IIS，改进迭代尺度法（本文求的是MEM模型的IIS）</p>
<p>首先已知MEM：$P_w(y|x)=\frac{1}{Z_w(x)}e^{\sum^n_{i=1}w_if_i(x,y)}$，$Z_w(x)=\sum_ye^{\sum_{i=1}^nw_if_i(x,y)}$</p>
<p>则对数似然函数为：$L(w)=\sum_{x,y}\hat P(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\hat P(x)\log Z_w(x)$</p>
<p>求：$\arg_w\max L(w)$</p>
<p><strong>思想：</strong>对MEM模型参数向量$w=(w_1,w_2,\cdots,w_n)^T$，寻找一个新的参数向量$w\to w+\delta$：</p>
<p>$w+\delta=(w_1+\delta_1,w_2+\delta_2,\cdots,w_n++\delta_n)^T$，使得$L(w+\delta)&gt;L(w)$，以此迭代</p>
<p><strong>推导：</strong>前面思想和EM基本一样，先算出$\Delta L$</p>
<p>$\Delta=L(w+\delta)-L(w)=\sum_{x,y}\hat P(x,y)\log P_{w+\delta}(y|x)-\sum_{x,y}\hat P(x,y)\log P_{w}(y|x)$</p>
<p>$=\sum_{x,y}\hat P(x,y)\sum_{i=1}^n\delta_if_i(x,y)-\sum_{x}\hat P(x)\log\frac{Z_{w+\delta}(x)}{Z_{w}(x)}$</p>
<p>$\because -\log\alpha≥1-\alpha,\alpha&gt;0$（<strong>这一步就把麻烦的$\log$给干掉了，使得原式可以计算，代价是要构造下界做近似MLE，同EM的思想</strong>）</p>
<p>$\therefore \Delta≥\sum_{x,y}\hat P(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_{x}\hat P(x)\frac{Z_{w+\delta}(x)}{Z_{w}(x)}$</p>
<p>$=\sum_{x,y}\hat P(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_{x}\hat P(x)\sum_yP_w(y|x)e^{\sum_{i=1}^n\delta_if_i(x,y)}$ （下界）</p>
<p>这个时候已经可以对$\Delta$求0导数来获取最大值以获取$\delta^{(n+1)}$了（这样就和EM一样，只是用了不同的下界不等式）。<strong>但是</strong>对目前的$\Delta$求$\frac{\partial \Delta}{\partial \delta_i}=0$会发现仍然存在$\sum_i\delta_i$形式（对负项中的$e^{\sum_{i=1}^n\delta_if_i(x,y)}$求导后根据链式法则显然无法消去$\sum_i\delta_i$部分），导致0导数式包含多个不同$i$的$\delta_i$变量，很难同时优化。（注意对求和函数求导，见EM算法专题中）</p>
<p>(<strong>想想为什么HMM中使用EM算法中对每个变量参数求0偏导就可行？</strong>因为在HMM中，$(A,B,\pi)$三个变量参数是独立的且求得的$Q$函数已经将三个变量放在三个子式中独立分开，满足</p>
<p>$\max_{A,B,\pi}Q=\{\max_A ④,\max_B ⑤,\max_ \pi ③\}$，每个子式子中只包含本要优化的变量）而本式中，各参数变量$\delta_i$之间根本不独立（求导式中就存在$\sum_i\delta_i$），不能分别求0偏导，传统做法只能让求一组$\delta_i$使得整体满足最大。</p>
<p><strong>于是</strong>我们想只将其中一个变量$\delta_i$作为变量，其它$\delta_j$作为固定常数$i≠j$，如何做到呢？那就要在求导中干掉$\sum_i\delta_i$形式，<strong>于是下面开始整骚操作：</strong></p>
<p><strong>构造Jensen不等式，以创造第二个下界。Jensen不等式可以将函数的作用范围缩小，这样就可以将$\sum$抛出$e$的指数部分，避免求导的时候链式法则再产生$\sum_i\delta_i$</strong>。</p>
<p>令$f^o(x,y)=\sum_if(x,y)$，MEM中$f_i(x,y)∈\{0,1\}$，是一个二值函数，于是$f^o(x,y)$表示所有特征在$(x,y)$出现的次数，<strong>这样就可以拼凑出一个求和等于1的分式并满足Jensen不等式形式</strong>。这里很需要技巧。</p>
<p>下面就是拼凑出的Jensen不等式形式（Jensen不等式见下面大标题）：</p>
<p>$\lambda_i=\frac{f_i(x,y)}{f^o(x,y)},x_i=\delta_if^o(x,y),f(\sum_i\lambda_ix_i)=e^{\sum_i\lambda_ix_i}$</p>
<p>其中$\lambda_i&gt;0,\sum_i\lambda_i=1,f(X)=e^X$为凸函数，则$e^{\sum_i\lambda_ix_i}≤\sum_i\lambda_ie^{x_i}$</p>
<p>$\therefore \Delta=\sum_{x,y}\hat P(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_{x}\hat P(x)\sum_yP_w(y|x)e^{\sum_{i=1}^n \frac{f_i(x,y)}{f^o(x,y)}\delta_if^o(x,y)}$</p>
<p>$≥\sum_{x,y}\hat P(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_{x}\hat P(x)\sum_yP_w(y|x)\sum_{i=1}^n\frac{f_i(x,y)}{f^o(x,y)}e^{ \delta_if^o(x,y)}$（新下界）</p>
<p>（注意前面有个负号！$-e^{\sum_i\lambda_ix_i}≥-\sum_i\lambda_ie^{x_i}$）</p>
<p>记右式为$B(\delta|w),\therefore L(w+\delta)-L(w)≥B(\delta|w)$</p>
<p>$\therefore \frac{\partial B(\delta|w)}{\partial \delta_i}=\sum_{x,y}\hat P(x,y)\sum_{i=1}^nf_i(x,y)-\sum_{x}\hat P(x)\sum_yP_w(y|x)\sum_{i=1}^nf_i(x,y)e^{ \delta_if^o(x,y)}$</p>
<p><strong>于是该式中只有指定$i$的$\delta_i$是变量，达到目的。</strong>令$\frac{\partial B(\delta|w)}{\partial \delta_i}=0$即可求得$\delta_i$</p>
<p>其中$\sum_{x,y}\hat P(x,y)\sum_{i=1}^nf_i(x,y)=\mathbb E_{\hat P}(f_i)$</p>
<p>于是计算每一大步迭代，$\delta$中的每一个参数$\delta_i$的公式为：</p>
<p>$\sum_{x}\hat P(x)\sum_yP_w(y|x)\sum_{i=1}^nf_i(x,y)e^{ \delta_if^o(x,y)}=\mathbb E_{\hat P}(f_i)$，其中$f^o(x,y)=\sum_if(x,y)$</p>
<p>一个公式计算一个$\delta_i$，依次计算$\delta_i$就可以求得$\delta$</p>
<h2 id="Improved-Iterative-Scaling-Algorithm"><a href="#Improved-Iterative-Scaling-Algorithm" class="headerlink" title="Improved Iterative Scaling Algorithm"></a>Improved Iterative Scaling Algorithm</h2><p>对于MEM</p>
<p>输入：特征函数$f_1,f_2,\cdots,f_n$；经验分布$\hat P(X,Y)$，模型$P_w(y|x)$</p>
<p>输出：最优参数$w_i^o$；最优模型$P_{w^o}$</p>
<p>（1）对所有$i∈\{1,2,\cdots,n\}$，取初值$w_i=0$</p>
<p>（2）对每一$i∈\{1,2,\cdots,n\}$</p>
<p>（a）令$\delta_i$是方差</p>
<script type="math/tex; mode=display">
\sum_{x}\hat P(x)\sum_yP_w(y|x)\sum_{i=1}^nf_i(x,y)e^{ \delta_if^o(x,y)}=\mathbb E_{\hat P}(f_i)</script><p>​    的解，其中</p>
<script type="math/tex; mode=display">
f^o(x,y)=\sum_if(x,y)</script><p>（b）更新$w_i$的值：$w_i \leftarrow  w_i+\delta_i$</p>
<p>（3）如果不是所有$w_i$都收敛，重复（2）</p>
<p>在（a）中计算$\delta_i$的时候，若$f^o(x,y)$是常数（即对所有的$(x,y)$，$f^o(x,y)$都相等。一般都不会是常数），那么</p>
<script type="math/tex; mode=display">
\delta_i=\frac{1}{M}\log\frac{E_{\hat P}(f_i)}{E_P(f_i)}</script><p>若$f^o(x,y)$不是常数，那就必须直接计算$\delta_i$，可以使用牛顿法计算。</p>
<p>以$g(\delta_i)=0$表示原方程，牛顿法通过迭代求得$\delta_i^o$，使得$g(\delta_i^o)=0$，迭代公式为</p>
<script type="math/tex; mode=display">
\delta_i^{(k+1)}=\delta_i^{(k)}-\frac{g(\delta_i^{(k)})}{g^{'}(\delta_i^{(k)})}</script><p>只要适当选取初始值$\delta_i^o$，由于原方程有单根，因此牛顿法恒收敛，且收敛速度很快</p>
<h2 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h2><p>由凸函数性质可得：$\lambda f(x_1)+(1-\lambda)f(x_2)≥f(\lambda x_1+ (1-\lambda)x_2)$</p>
<p>$\therefore$ 对于任意点集$\{x_i\}$，若$\lambda_i≥0$，且$\sum_i\lambda_i=1$</p>
<p>通过数学归纳法可以证明凸函数$f(x)$满足$f(\sum_{i=1}^M\lambda_ix_i)≤\sum_{i=1}^M\lambda_if(x_i)$</p>
<p>对于凹函数，$\lambda f(x_1)+(1-\lambda)f(x_2)≤f(\lambda x_1+ (1-\lambda)x_2)$则相反</p>
<p>即：</p>
<script type="math/tex; mode=display">
若\lambda_i≥0，\sum_i\lambda_i=1</script><script type="math/tex; mode=display">
若f(x)为凸函数，f(\sum_{i=1}^M\lambda_ix_i)≤\sum_{i=1}^M\lambda_if(x_i)</script><script type="math/tex; mode=display">
若f(x)为凹函数，f(\sum_{i=1}^M\lambda_ix_i)≥\sum_{i=1}^M\lambda_if(x_i)</script><p>若$\lambda_i$看成取值为$x_i$的离散变量$x$的概率分布，则可以写成：</p>
<p>若凸函数：$f(\mathbb E[x])≤\mathbb E[f(x)]$，若为凹函数：$f(\mathbb E[x])≥\mathbb E[f(x)]$</p>
<p>凸则大作用域≤小作用域，凹则大作用域≥小作用域</p>
<p><strong>构造Jensen不等式经常用拼凑法</strong>（IIS和EM）</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>条件随机场</title>
    <url>/2019/10/29/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/</url>
    <content><![CDATA[<h2 id="Conditional-Random-Field"><a href="#Conditional-Random-Field" class="headerlink" title="Conditional Random Field"></a>Conditional Random Field</h2><p>在我前面的文章中有一些概率图模型的基础知识：<a href="https://aisakaki.com/2019/10/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B/">深度学习中的结构化概率模型</a></p>
<p>如果数据量极大，配分函数有可能会遇到无法直接求和问题参看前文：<a href="https://aisakaki.com/2019/10/18/%E9%85%8D%E5%88%86%E5%87%BD%E6%95%B0/">配分函数</a>，其中用到了蒙特卡罗方法，参看前文：<a href="https://aisakaki.com/2019/10/12/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/">蒙特卡罗方法</a></p>
<p>将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的<strong>因子分解</strong>(factorization)</p>
<p><strong>标注问题：</strong>在条件概率模型$P(Y|X)$中，$Y$是输出变量，表示输出的标记序列（或状态序列），$X$是输入变量，表示需要标注的观测序列。<strong>注意被标注的是标记序列/状态序列</strong>，不是观测序列。HMM和CRF都可以用于标注问题（只是其中一个应用）。</p>
<p><strong>条件随机场(conditional random field,CRF)</strong>：给定随机变量$X$条件下，随机变量$Y$的马尔可夫随机场</p>
<p><strong>线性链条件随机场(linear chain conditional random field,Linear-Chain CRF)</strong>：定义如下</p>
<p>设$X=\{X_1,X_2,\cdots,X_n\},Y=\{Y_1,Y_2,\cdots,Y_n\}$均为线性链表示的随机变量序列，若在给定随机变量序列$X$的条件下，随机变量序列$Y$的条件概率分布$P(Y|X)$构成条件随机场，即满足马尔可夫性：</p>
<script type="math/tex; mode=display">
P(Y_i|X,Y_1,\cdots,Y_{i-1},Y_{i+1},\cdots,Y_n)=P(Y_i|X,Y_{i-1},Y_{i+1}),i=1,2,\cdots,n</script><p>则称$P(Y|X)$为线性链条件随机场。意思是$Y_i$只与$X$(观测序列)，$Y_{i-1},Y_{i+1}$有关</p>
<a id="more"></a>
<h2 id="Linear-Chain-CRF形式化表示"><a href="#Linear-Chain-CRF形式化表示" class="headerlink" title="Linear-Chain CRF形式化表示"></a>Linear-Chain CRF形式化表示</h2><h3 id="参数化形式"><a href="#参数化形式" class="headerlink" title="参数化形式"></a>参数化形式</h3><p>要先画<strong>状态路径图</strong>来理解公式</p>
<p>设$P(Y|X)$为线性链CRF，则在随机变量$X$的取值为$x$（状态序列）的情况下，随机变量$Y$取值为$y$（取值/观测值序列）的条件概率具有如下形式：</p>
<script type="math/tex; mode=display">
P(y|x)=\frac{1}{Z(x)}e^{(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i))}</script><script type="math/tex; mode=display">
Z(x)=\sum_ye^{(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i))}</script><p>$i$就是序列位置标号，见上面Linear-Chain中的定义（设$q,p∈\Gamma$，$\Gamma$为$Y_i$的取值空间（观测空间））</p>
<p>$t_k(y_{i-1}=q,y_i=p,x,i)$，也写成$t_k(y_{i-1},y_i,x,i)$：<strong>特征函数</strong>，对应的权值为$\lambda_k$，定义为<strong>边</strong>上的特征函数，称为<strong>转移特征</strong>，依赖当前和前一个位置：[表示在第$i$位置上，序列第$i$位置$x$所对应的$y$取值（观测值）为$q$，到序列第$i-1$位置$x$对应的$y$取值（观测值）为$p$这条路径的转移特征，<strong>权值</strong>为$\lambda_k$]</p>
<p>$s_l(y_i=p,x,i)$，也写成$s_l(y_i,x,i)$：<strong>特征函数</strong>，对应的权值为$\mu_l$，定义为<strong>结点</strong>上的特征函数，称为<strong>状态特征</strong>，依赖当前位置：[表示在第$i$位置上，序列第$i$位置的$x$所对应的$y$的取值（观测值）为$p$的结点的状态特征，该结点的<strong>权值</strong>为$\mu_l$]</p>
<p>$k$表示边的转移特征函数数量，$l$表示结点的状态特征函数数量</p>
<p>一般特征函数$t_k,s_l$取值为0或1</p>
<p>此求和其实就是对<strong>所有路径</strong>求和（所有可能的输出序列）</p>
<p>举例理解：</p>
<p>对于$X=(X_1,X_2,X_3),Y=(Y_1,Y_2,Y_3),Y_i$的取值集合为$\Gamma=\{1,2\}$</p>
<p>可以得到一个二维图，横坐标$i_1,i_2,i_3$表序号，纵坐标$Y=1,Y=2$表$y$的两个取值</p>
<p>则图中所有坐标点（结点）为</p>
<p>$(Y_1=2,X,1),(Y_1=2,X,2),(Y_1=2,X,3)$</p>
<p>$(Y_1=1,X,1),(Y_1=1,X,2),(Y_1=1,X,3)$</p>
<p>如$(Y_1=2,X,1)$就表示在$i=1$的$X$序列位置上，$Y$的取值（观测值）为2，也可以写成$Y(X_1)=2$</p>
<p>然后就可以通过上面的结点和边特征函数以及权重将概率图绘制出来，一切求和就得出CRF的概率分布</p>
<h3 id="其它形式"><a href="#其它形式" class="headerlink" title="其它形式"></a>其它形式</h3><p><strong>拼接</strong>$：K_1$个转移特征，$K_2$个状态特征，$K=K_1+K_2$</p>
<p><strong>求和</strong>：对同一特征在各个位置求和（<strong>同一特征在不止一个位置上有定义</strong>）</p>
<p>$f_k(y_{i-1},y_i,x,i)=\begin{cases} t_k(y_{i-1},y_i,x,i),k=1,2,\dots,K_1\\ s_l(y_i,x,i),k=K_1+l;l+1,2,\dots,K_2\end{cases}$——①</p>
<p><strong>组一：</strong></p>
<p>$f_k(y,x)=\sum_{i=1}^n f_k(y_{i-1},y_i,x,i),k=1,2,\dots,K$ （<strong>对同一特征函数$f_k$在所有位置$\sum_i^n~$上求和</strong>）</p>
<p>$F(y,x)=(f_1(y,x),f_2(y,x),\dots,f_K(y,x))^T$（上面那个求和组成的向量，方便用于下面简化形式的内积形式）——③</p>
<p><strong>组二：</strong></p>
<p>$F_i(y_{i-1},y_i|x)=(f_1(y_{i-1},y_i,x,i),f_2(y_{i-1},y_i,x,i)，\cdots,f_K(y_{i-1},y_i,x,i))$  (<strong>某一位置$i$下所有特征函数组成的向量</strong>）</p>
<p>$\therefore w·F_i(y_{i-1},y_i|x)=\sum_{k=1}^Kw_kf_k(y_{i-1},y_i,x,i)$   （与上上面的$f_k(y,x)$刚好反过来，先对一个位置$i$积分所有特征函数$f_k$，再积分所有位置$i$）——④</p>
<p>注意这一组的$F$下标和上面那组的含义是不一样的，一个是$k:$特征函数数量一个是$i:$位置数量</p>
<p>$w_k=\begin{cases} \lambda_k,k=1,2,\dots,K_1\\ \mu_l,k=K_1+l;l+1,2,\dots,K_2\end{cases}$——②</p>
<p>$w=(w_1,w_2,\dots,w_K)^T$</p>
<h3 id="简化形式—对每个特征函数求其在所有位置的和"><a href="#简化形式—对每个特征函数求其在所有位置的和" class="headerlink" title="简化形式—对每个特征函数求其在所有位置的和"></a>简化形式—对每个特征函数求其在所有位置的和</h3><p>由②③：</p>
<p>$\therefore$特征*权重可以看成求正交化（内积）可以看成是先对每个特征函数$f_k$在所有位置$i$积分，再积所有特征函数$f_k$</p>
<script type="math/tex; mode=display">
P_w(y|x)=\frac{e^{w·F(y,x)}}{Z_w(x)}</script><script type="math/tex; mode=display">
Z_w(x)=\sum_ye^{w·F(y,x)}</script><h3 id="矩阵形式—在每个位置求所有特征函数的和"><a href="#矩阵形式—在每个位置求所有特征函数的和" class="headerlink" title="矩阵形式—在每个位置求所有特征函数的和"></a>矩阵形式—在每个位置求所有特征函数的和</h3><p>由②④：</p>
<p>$W_i(y_{i-1},y_i|x)=\sum_{k=1}^Kw_kf_k(y_{i-1},y_i,x,i)$</p>
<p>$M_i(y_{i-1},y_i|x)=e^{W_i(y_{i-1},y_i|x)}$ （所以可以对每个位置$i$求$M$矩阵）</p>
<script type="math/tex; mode=display">
P_w(y|x)=\frac{1}{Z_w(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)</script><script type="math/tex; mode=display">
Z_w(x)=[M_1(x)M_2(x)\dots M_{n+1}(x)]_{start,stop}</script><p>由于矩阵表示可以很直观的表示序列上每个位置的矩阵，所以很好用，下面概率计算的时候直接累乘即可</p>
<p><strong>非矩阵形式</strong>可以表示为：（注意这个式子的$f_i(x,y)=$组二中的$F_i(y_{i-1},y_i|x)$）</p>
<script type="math/tex; mode=display">
P_w(y|x)=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{\sum_y e^{\sum_{i=1}^nw_if_i(x,y)}}</script><p><strong>此形式即最大熵模型形式！对Linear-Chain CRF参数的学习与对最大熵模型参数的学习一样</strong></p>
<h2 id="概率计算—求-max-P-Y-i-y-i-x-max-P-Y-i-1-y-i-1-Y-i-y-i-x"><a href="#概率计算—求-max-P-Y-i-y-i-x-max-P-Y-i-1-y-i-1-Y-i-y-i-x" class="headerlink" title="概率计算—求$\max\{P(Y_i=y_i|x)\},\max\{P(Y_{i-1}=y_{i-1},Y_i=y_i|x)\}$"></a>概率计算—求$\max\{P(Y_i=y_i|x)\},\max\{P(Y_{i-1}=y_{i-1},Y_i=y_i|x)\}$</h2><p><strong>与HMM一样使用前向-后向算法</strong>，递归计算，用矩阵形式表达方便</p>
<p><strong>前向算法：</strong> 前向右乘$M$矩阵（$\alpha^T$将原横着的序列变<strong>列向量</strong>与$M$矩阵相乘）</p>
<p>初始化：$\alpha_0(y|x)=\begin{cases} 1, y=start\\ 0,otherwise\end{cases}$</p>
<p>递推公式：$\alpha_i^T(y_i|x)=\alpha_{i-1}^T(y_{i-1}|x)[M_i(y_{i-1},y_i|x)],i=1,2,\dots,n+1$</p>
<p><strong>后向算法：</strong> 后向左乘$M$矩阵</p>
<p>初始化：$\beta_{n+1}(y_{n+1}|x)=\begin{cases} 1, y_{n+1}=stop\\ 0,otherwise\end{cases}$</p>
<p>递推公式：$\beta_i(y_i|x)=[M_{i+1}(y_i,y_{i+1}|x)]\beta_{i+1}(y_{i+1}|x),i=0,1,2,\dots,n+1$</p>
<p><strong>概率计算：</strong></p>
<p>$P(Y_i=y_i|x)=\frac{1}{Z(x)}\alpha_i^T(y_i|x)\beta_i(y_i|x)$</p>
<p>$P(Y_{i-1}=y_{i-1},Y_i=y_i|x)=\frac{1}{Z(x)}\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)$</p>
<p>其中$Z(x)=\alpha_n^T\vec1=\vec1\beta_1(x)$，$\vec1$为元素均为1的$m$维向量</p>
<p><strong>[O]期望值计算：</strong></p>
<p>$\mathbb E_{P(Y|X)}[f_k]=\sum_yP(y|x)f_k(y,x)=略$</p>
<p>$\mathbb E_{P(X|Y)}[f_k]=\sum_{x,y}P(x,y)\sum_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)=略$</p>
<h2 id="学习算法—求模型参数-hat-w"><a href="#学习算法—求模型参数-hat-w" class="headerlink" title="学习算法—求模型参数$\hat w$"></a>学习算法—求模型参数$\hat w$</h2><p>Linear-Chain CRF的位置求和形式，也即最大熵模型形式，于是可以<strong>用与MEM一样的方法来求模型参数</strong></p>
<h3 id="Improved-Iterative-Scaling"><a href="#Improved-Iterative-Scaling" class="headerlink" title="Improved Iterative Scaling"></a>Improved Iterative Scaling</h3><p>IIS，改进的迭代尺度法，具体略</p>
<p><strong>注意</strong>模型参数是$w$，而$f_k$（或者全局特征向量$F(y,x)$）是先验参数！</p>
<p>经验概率$\hat P(X,Y)$即为在训练集上直接通过观测法求得的经验概率，如通过多次抛硬币频数法求得概率一样</p>
<p>极大化<strong>训练数据的条件概率对数似然函数</strong>：</p>
<p>$L(w)=L_{\hat P}(P_w)=\log\prod_{x,y}P_w(y|x)^{\hat P(x,y)}=\sum_{x,y}\hat P(x,y)\log P_w(y|x)$</p>
<p>将简化形式$P_w(y|x)$代入求极大</p>
<p>对其使用IIS，略</p>
<h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><p>可以见我以前写的文章<a href="https://aisakaki.com/2019/09/26/%E7%89%9B%E9%A1%BF%E6%B3%95%E6%B1%82%E8%A7%A3%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96/">牛顿法求解非线性优化</a>和<a href="https://aisakaki.com/2019/09/26/%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/">拟牛顿法</a></p>
<p>直接对$P_w(y|x)$求对数似然得优化目标函数</p>
<p>依然使用简化形式$P_w(y|x)$</p>
<p>对其使用BFGS，略</p>
<h2 id="预测算法—求-y-arg-y-max-P-w-y-x"><a href="#预测算法—求-y-arg-y-max-P-w-y-x" class="headerlink" title="预测算法—求$y^*=\arg_y\max P_w(y|x) $"></a>预测算法—求$y^*=\arg_y\max P_w(y|x) $</h2><p>给定输入序列/观测序列$x$，求条件概率最大输出序列$y^*$，即求$\arg_y\max P_w(y|x)$</p>
<p>与HMM一样的维比特算法，动态规划，注意递推式。</p>
<p>由于要确定路径，则使用在每个位置求特征和的积分方法，即上面的非矩阵形式公式，化简后可得：</p>
<p>求$\max_y\sum_{i=1}^nw·F_i(y_{i-1},y_i,x) $  （即求$\max_y ④$)</p>
<p>$F_i(y_{i-1},y_i|x)$为局部特征向量</p>
<p>递推式：$\max\{\delta_{i-1}+w·F_i(y_{i-1},y_i,x)\}$</p>
<p>具体略</p>
<h2 id="HMM和CRF的对比区别"><a href="#HMM和CRF的对比区别" class="headerlink" title="HMM和CRF的对比区别"></a>HMM和CRF的对比区别</h2><ol>
<li><p>HMM是生成模型，CRF是判别模型</p>
<p> <img src="//aisakaki.com/2019/10/29/条件随机场/0.jpg" alt="0"></p>
<p> <em>（本图来自知乎）</em></p>
</li>
<li><p>CRF的优化函数，可以做到全局最优</p>
</li>
<li><p>定义</p>
<p>CRF可以看成一组随机变量的集合，当给每一个位置按照某种分布随机赋予一个值之后，其全体就叫做随机场。这些随机变量之间可能有依赖关系 。</p>
<p>如果给定的马尔科夫随机场（MRF）中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个MRF的分布，也就是<strong>条件分布</strong>，那么这个MRF就称为 <strong>Conditional Random Fields (CRF)</strong>。它的条件分布形式完全类似于MRF的分布形式，只不过多了一个观察集合X。所以, CRF本质上是给定了条件(观察值observations)集合的MRF。</p>
</li>
<li><p>HMM可以由CRF表达，CRF的范围更广。也就说，<strong>每一个HMM都可以被CRF表示</strong></p>
</li>
<li><p>CRF要归一化而HMM不需要 </p>
<p>因为CRF的权重值是任意的，而HMM是概率值必须满足概率约束条件</p>
</li>
<li><p>CRF可以依赖更多全局特征（Linear-Chain CRF的依赖也比较有限）</p>
<p>HMM只能依赖前一个状态和当前观测，而CRF可以定义更远的依赖关系</p>
</li>
<li><p>CRF可以看成MEMM（最大熵马尔可夫模型）在标注问题上的推广</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>海</title>
    <url>/2019/10/29/%E6%B5%B7/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/10/29/海/0.jpg" alt="0"><br><a id="more"></a></p>
<p><div style="text-align: center"><i>未来へ</i></div></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=535056564&auto=0&height=66"></iframe>

]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>隐马尔可夫模型（二）</title>
    <url>/2019/10/27/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h2 id="学习算法—预测模型参数-lambda-A-B-pi"><a href="#学习算法—预测模型参数-lambda-A-B-pi" class="headerlink" title="学习算法—预测模型参数$\lambda=(A,B,\pi)$"></a>学习算法—预测模型参数$\lambda=(A,B,\pi)$</h2><p>若知$\{(O_1,I_1),(O_2,I_2),\cdots,(O_S,I_S)\}$求$(A,B,\pi)$，用监督学习方法</p>
<p>若知$\{O_1,O_2,\cdots,O_S\}$求$(A,B,\pi)$，用非监督学习方法Baum-Welch算法</p>
<a id="more"></a>
<h3 id="监督学习方法"><a href="#监督学习方法" class="headerlink" title="监督学习方法"></a>监督学习方法</h3><p>通过转移状态与观测出现的频率来估算参数</p>
<p>但这样就需要人工标注，有时候代价过高</p>
<h3 id="Baum-Welch算法"><a href="#Baum-Welch算法" class="headerlink" title="Baum-Welch算法"></a>Baum-Welch算法</h3><p>使用MLE，由于隐变量的存在，很难求，所以用EM算法来近似MLE</p>
<p>假定训练数据为$S$个长度为$T$的观测序列$\{O_1,O_2,\cdots,O_S\}$，不知其状态序列（隐变量）</p>
<p>目标：学习$\lambda=(A,B,\pi)$</p>
<p>需要用到上一篇的公式$\gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum^N_{j=1}\alpha_t(j)\beta_t(j)}$——①   </p>
<p>$\epsilon_t(i,j)=P(i_t=q_i,i_{t+1}=q_j|O,\lambda)=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)}=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{\sum^N_{i=1}\sum^N_{j=1}P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}$——②</p>
<p>手推（键盘推O_O）Baum-Welch公式：</p>
<p>$P(O,I|\lambda)=P((\pi_{i_1} b_{i_1}(o_1)) (a_{i_1i_2}b_{i_2}(o_2)) \cdots(a_{i_{T-1}i_T}b_{i_T}(o_T))$</p>
<p>由$Q(\lambda,\hat \lambda)=\mathbb E_I(\log P(O,I|\lambda)|O,\hat \lambda)$</p>
<p>$=\sum_I \log P(O,I|\lambda)P(I|O,\hat \lambda)$</p>
<p>$=\sum_I \log P(O,I|\lambda)\frac{P(I,O|\hat \lambda)}{P(O|\hat \lambda)}$，由于$P(O|\hat \lambda)$为常数，可以省略掉</p>
<p>$\therefore Q(\lambda,\hat \lambda)=\sum_I \log P(O,I|\lambda)P(O,I|\hat \lambda)$</p>
<p>将$P(O,I|\lambda)$代入$Q$函数得：</p>
<p>$Q(\lambda,\hat \lambda)=\sum_I \log \pi_{i} P(O,I|\hat \lambda)+\sum_I\sum_{t=1}^{T-1}\log a_{i_ti_{t+1}}P(O,I|\hat \lambda)+\sum_I\sum_{t=1}^T\log b_{i_t}(o_t)P(O,I|\hat \lambda)$</p>
<p>将三个参数分布归到三个子项后，<strong>下面根据参数的需要对$P(O,I|\hat \lambda)$中的$I$变量进行对应的逆边缘化</strong>，注意时间和状态空间的关系，是独立的</p>
<p>$=\sum_{i=1}^N \log \pi_i P(O,i_1=i|\hat \lambda)$——③   </p>
<p>$+\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}\log a_{ij}P(O,i_t=i,i_{t+1}=j|\hat \lambda)$——④ 时间连续，但状态空间是任意组合的</p>
<p>$+\sum_j^N\sum_{t=1}^T\log b_j(o_t)P(O,i_t=j|\hat \lambda)$——⑤</p>
<p>由于$\lambda=(A,B,\pi)$中的三个参数变量都相互独立且求得的$Q$的三个子式只包含一个独立的变量，于是</p>
<p>$\max_{A,B,\pi}Q=\{\max_A ④,\max_B ⑤,\max_ \pi ③\}$</p>
<p><div><br></div><br><strong>于是用拉格日朗子乘数法分别对各变量求0偏导（否则算出负概率）</strong></p>
<p>Ⅰ.对于$\pi_i :\sum_{i=1}^N \log \pi_i P(O,i_1=i|\hat \lambda)$ （③式）</p>
<p>$\sum_{i=1}^N \pi_i=1$    $\pi$是初始<strong>概率</strong>向量，表初始的时候各状态的概率，别搞混了</p>
<p>$\therefore \mathbb L=\sum_{i=1}^N \log \pi_i P(O,i_1=i|\hat \lambda)+\gamma(\sum_{i=1}^N \pi_i-1)$</p>
<p>令$\frac{\partial \mathbb L}{\partial \pi_i}=0,\therefore \frac{\partial(\sum_{i=1}^N \log \pi_i P(O,i_1=i|\hat \lambda)+\gamma(\sum_{i=1}^N \pi_i-1))}{\partial \pi_i}=0$ </p>
<p>$\therefore \frac{1}{\pi_i}P(O,i_1=i|\hat \lambda)+\gamma=0$ （<strong>这里容易求错，见下面的求和函数求导</strong>）</p>
<p>$\therefore P(O,i_1=i|\hat \lambda)+\gamma\pi_i=0$  ——⑥   </p>
<p><strong>左右两边同时求和</strong>：$\sum_{i=1}^NP(O,i_1=i|\hat \lambda)+\sum_{i=1}^N\gamma\pi_i=0$ ，$I$被边缘化（求和可以使用更多信息以消去$\gamma$）</p>
<p>$\therefore P(O|\hat \lambda)+\gamma=0$</p>
<p>$\therefore \gamma=-P(O|\hat \lambda)$，代入回⑥得：</p>
<p>$P(O,i_1=i|\hat \lambda)-\pi_iP(O|\hat \lambda)=0$</p>
<p>$\therefore \pi_i=\frac{P(O,i_1=i|\hat \lambda)}{P(O|\hat \lambda)}=P(i_t=i_1|O,\lambda)$，由①得：$\pi_i=\gamma_1(i)$    ——⑦  </p>
<p>Ⅱ.对于$a_{ij}:\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}\log a_{ij}P(O,i_t=i,i_{t+1}=j|\hat \lambda)$ （④式）</p>
<p>$\sum_j^Na_{ij}=1$</p>
<p>$\therefore \mathbb L=\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}\log a_{ij}P(O,i_t=i,i_{t+1}=j|\hat \lambda)+\gamma(\sum_j^Na_{ij}-1)$</p>
<p>令$\frac{\partial \mathbb L}{\partial a_{ij}}=0,\therefore \frac{\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}\log a_{ij}P(O,i_t=i,i_{t+1}=j|\hat \lambda)+(\sum_j^Na_{ij}-1)}{\partial a_{ij}}=0$</p>
<p>$\therefore \sum_{t=1}^{T-1}\frac{1}{a_{ij}}P(O,i_t=i,i_{t+1}=j|\hat \lambda)+\gamma =0$</p>
<p>$\therefore \sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\hat \lambda)+\gamma a_{ij}=0$</p>
<p>两边同时求和：$\sum_j^N\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\hat \lambda)+\gamma \sum_j^Na_{ij}=0$</p>
<p>$\therefore\gamma=-\sum_{t=1}^{T-1}P(O,i_t=i|\hat \lambda)$，回代原式</p>
<p>$\therefore \sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\hat \lambda)-\sum_{t=1}^{T-1}P(O,i_t=i|\hat \lambda) a_{ij}=0$</p>
<p>$\therefore a_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\hat \lambda)}{\sum_{t=1}^{T-1}P(O,i_t=i|\hat \lambda)}$</p>
<p>由①②得：</p>
<p>$a_{ij}=\frac{\sum_{t=1}^{T-1}\epsilon_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}$  ——⑧  </p>
<p>Ⅲ.对于$b_j(o_t):\sum_j^N\sum_{t=1}^T\log b_j(o_t)P(O,i_t=j|\hat \lambda)$ （⑤式）</p>
<p>注意这里有所不同，$b_j(k)$的观测空间为$(o_1,o_2,\cdots,o_M)$。约束条件为$\sum_{k=1}^Mb_j(k)=1$</p>
<p>$\mathbb L=\sum_j^N\sum_{t=1}^T\log b_j(o_t)P(O,i_t=j|\hat \lambda)+\gamma(\sum_{k=1}^Mb_j(k)-1)$</p>
<p>令$\frac{\partial \mathbb L}{\partial b_j(k)}=0,\therefore \frac{\partial(\sum_j^N\sum_{t=1}^T\log b_j(o_t)P(O,i_t=j|\hat \lambda)+\gamma(\sum_{k=1}^Mb_j(k)-1)}{\partial b_j(k)}=0$</p>
<p>仅在$o_t=v_k$的时候$(b_j(o_t)=b_j(k))$，$\frac{\partial b_j(o_t)}{\partial b_j(k)}≠0$，以$I(o_t=v_k)$表示，$I(o_t=v_k)=1,I(o_t≠v_k)=0$</p>
<p>$\therefore \sum_{t=1}^T\frac{1}{b_j(o_t)}P(O,i_t=j|\hat \lambda)I(o_t=v_k)+\gamma =0$</p>
<p>$\therefore \sum_{t=1}^TP(O,i_t=j|\hat \lambda)I(o_t=v_k)+\gamma b_j(k)=0$</p>
<p>左右两边同时求和$\therefore \sum_{k=1}^M\sum_{t=1}^TP(O,i_t=j|\hat \lambda)I(o_t=v_k)+\gamma \sum_{k=1}^Mb_j(k)=0$</p>
<p>$\therefore \sum_{k=1}^M\sum_{t=1}^TP(O,i_t=j|\hat \lambda)I(o_t=v_k)+\gamma =0$</p>
<p>对这个式子进行化简，其等价于：$\sum_{t=1}^TP(O,i_t=j|\hat \lambda)+\gamma=0$ (<em>有疑问</em>)</p>
<p>$\therefore \gamma=-\sum_{t=1}^TP(O,i_t=j|\hat \lambda)$，回代</p>
<p>$\sum_{t=1}^TP(O,i_t=j|\hat \lambda)I(o_t=v_k)-\sum_{t=1}^TP(O,i_t=j|\hat \lambda)b_j(k)=0$</p>
<p>$\therefore b_j(k)=\frac{\sum_{t=1}^TP(O,i_t=j|\hat \lambda)I(o_t=v_k)}{\sum_{t=1}^TP(O,i_t=j|\hat \lambda)}$，由①得</p>
<p>$b_j(k)=\frac{\sum_{t=1,o_t=v_k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}$  ——⑨  </p>
<hr>
<p><strong>综上所述</strong>：$\frac{\partial Q}{\partial \pi_i}=⑦,\frac{\partial Q}{\partial a_{ij}}=⑧,\frac{\partial Q}{\partial b_j(k)}=⑨$</p>
<p>于是可以写出<strong>Baum-Welch算法：</strong>（仅对一个观测序列$O∈\{O_1,O_2,\dots,O_S\}$）</p>
<p>输入观测数据$O=(o_1,o_2,\dots,o_n)$，求$\lambda=(A,B,\pi)$</p>
<p>初始化：对$n=0$，选取$a_{ij}^{(0)},b_j(k)^{(0)},\pi_i^{(0)}$，得到模型$\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})$</p>
<p>递推：</p>
<script type="math/tex; mode=display">
\pi_i^{(n+1)}=\gamma_1(i)</script><script type="math/tex; mode=display">
a_{ij}^{(n+1)}=\frac{\sum_{t=1}^{T-1}\epsilon_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}</script><script type="math/tex; mode=display">
b_j(k)^{(n+1)}=\frac{\sum_{t=1,o_t=v_k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}</script><p>(每一个参数矩阵的中要计算出多个上述三式子（所有$i,j$情况）才能得到新迭代的模型参数)</p>
<p>其中各值按观测$O=(o_1,o_2,\dots,o_n)$和模型$\lambda^{(n)}=(A^{(n)},B^{(n)},\pi^{(n)})$计算</p>
<p>最终得到模型参数$\lambda^{(n+1)}=(A^{(n+1)},B^{(n+1)},\pi^{(n+1)})$</p>
<h2 id="预测算法—预测序列状态-I-arg-I-max-P-I-O-lambda"><a href="#预测算法—预测序列状态-I-arg-I-max-P-I-O-lambda" class="headerlink" title="预测算法—预测序列状态$I=\arg_I\max\{P(I|O,\lambda)\}$"></a>预测算法—预测序列状态$I=\arg_I\max\{P(I|O,\lambda)\}$</h2><h3 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h3><p><strong>思想：选出在每一个时刻最有可能出现的状态来构成状态序列，用前面正反向算法推导出的单独概率公式求解</strong></p>
<p>由在$t$时刻处于状态$q_i$的概率公式：$\gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac{\alpha_t(i)\beta_t(i)}{\sum^N_{j=1}\alpha_t(j)\beta_t(j)}$</p>
<p>在每一时刻$t$最有可能的状态$i^*_t=\arg\max_{1≤i≤N}[\gamma_t(i)],t=1,2,\dots,T$（计算出当前$t$下哪个$i$使得$\gamma_t(i)$最大）</p>
<p>以此得到状态序列$I^{<em>}=(i^{</em>}_1,i^{<em>}_2,\dots,i^{</em>}_T)$</p>
<p>但存在问题：可能存在$\gamma_t(i)=0$也就是情况不发生的情况，那么该公式就会失效了</p>
<p>但尽管如此近似算法依然是有用的</p>
<h3 id="维比特算法"><a href="#维比特算法" class="headerlink" title="维比特算法"></a>维比特算法</h3><p>动态规划，类似传统的Dijkstra算法，本文不细展开了，略</p>
<p>需要注意的就是递推公式的时候，概率最大化的是观测概率也就是$\max\{\delta_i(t)a_{ij}b_j(o_{t+1})\}$，而不是状态</p>
<h2 id="求和函数求导"><a href="#求和函数求导" class="headerlink" title="求和函数求导"></a>求和函数求导</h2><ol>
<li><p><strong>求解：</strong>$\frac{\partial(\sum_{i=1}^N\log\pi_i)}{\partial\pi_i}$</p>
<p>将$\sum_{i=1}^N\log\pi_i$展开：</p>
<p>$\sum_{i=1}^N\log\pi_i=\log \pi_1+\log \pi_2+\cdots+\log \pi_N$</p>
<p>$\therefore \frac{\partial(\sum_{i=1}^N\log\pi_i)}{\partial\pi_i}=\frac{\partial(\log \pi_1+\log \pi_2+\cdots+\log \pi_N)}{\partial \pi_i}$</p>
<p>若$i=1$，则$\frac{\partial(\log \pi_1+\log \pi_2+\cdots+\log \pi_N)}{\partial\pi_1}=\frac{1}{\pi_1}$</p>
<p>若$i=2$，则$\frac{\partial(\log \pi_1+\log \pi_2+\cdots+\log \pi_N)}{\partial\pi_2}=\frac{1}{\pi_2}$</p>
<p>若$i=i$，则$\frac{\partial(\log \pi_1+\log \pi_2+\cdots+\log \pi_N)}{\partial\pi_i}=\frac{1}{\pi_i}$</p>
<p>$\therefore \frac{\partial(\sum_{i=1}^N\log\pi_i)}{\partial\pi_i}=\frac{1}{\pi_i}$</p>
<p>同理$\frac{\partial(\sum_{i=1}^N\pi_i)}{\partial\pi_i}=1$</p>
<p><em>一次只能对一个变量求导，其余视为不相关常数</em></p>
<p>理解对$\pi_i$求导的意义，即无论被求导式为何，对其结果对$\pi_i$求导</p>
</li>
<li><p>$\sum_y×1=||y||_0×1$</p>
<p>就像$\sum_{i=1}^nm=nm,\sum_{y}^n×1=n$一样，求和的参数决定了循环求和次数</p>
<p>于是：$\sum_{y}d=\sum_{x,y}d$，只有当$||y||_0=||(x,y)||_0$才成立</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>项目管理基础</title>
    <url>/2019/10/27/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p><em>week3 项目管理基础 粗略大纲笔记</em></p>
<a id="more"></a>
<p>游戏开发模型</p>
<ol>
<li><p>产品愿景</p>
</li>
<li><p>产品线路图</p>
</li>
<li><p>发布规划</p>
</li>
<li><p>迭代计划</p>
</li>
<li><p>功能开发</p>
</li>
</ol>
<p>项目章程：项目目标 <em>定义做什么</em></p>
<ol>
<li>项目目的 </li>
<li>产品白皮书</li>
<li>里程碑计划</li>
<li>交付标准</li>
<li>职能架构 = 职责定义</li>
</ol>
<p>1234=目标定义</p>
<p>项目执行 <em>定义怎么做</em></p>
<ol>
<li>配置管理</li>
<li>需求管理</li>
<li>迭代管理</li>
<li>进度管理</li>
<li>质量关系</li>
</ol>
<p>本节课内容不止适用于游戏开发</p>
]]></content>
      <categories>
        <category>游戏设计</category>
      </categories>
  </entry>
  <entry>
    <title>隐马尔可夫模型（一）</title>
    <url>/2019/10/25/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a>Hidden Markov Model</h2><p>隐马尔可夫模型，HMM，属于生成模型</p>
<p>HMM存在两个随机过程，且两个过程是相互联系的，具体来说就是<strong>第一个随机过程为状态序列随机过程</strong>，这和一般的<strong>马尔可夫链</strong>一致（具有马尔可夫性，当前状态只与前一个状态有关，与其它状态、观测无论未来还是过去都无关），<strong>第二个随机过程为观测序列的随机过程</strong>，它的当前值（观测值）由每一时刻的状态序列随机过程下的当前状态生成，且观测序列当前值只与状态序列随机过程的当前状态有关，与随机过程中其它观测和状态无论未来还是过去无关。即：</p>
<script type="math/tex; mode=display">
P_{状态}(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,\cdots,T</script><script type="math/tex; mode=display">
P_{观测}(o_t|i_t,i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(o_t|i_t),t=1,2,\cdots,T</script><p> HMM观测序列每生成一个新的状态就生成一次新观测</p>
<a id="more"></a>
<h2 id="符号表记"><a href="#符号表记" class="headerlink" title="符号表记"></a>符号表记</h2><p>HMM中符号较多</p>
<p>$\lambda=(A,B,\pi)$表记一个HMM，$A$：状态转移概率矩阵，$B$：观测概率矩阵，$\pi$：初始状态<strong>概率向量</strong>(比如$\pi=[0.2,0,4,0,4]$即表示在$t=1$时状态为$q_1,q_2,q_3$的初始概率分别为$0.2,0,4,0,4$</p>
<p>$i_t$：一个状态，$I=(i_1,i_2,\cdots,i_T)$：状态序列，经过指定状态序列$i_1,i_2,\cdots,i_T$后到达指定最终状态$i_T$的概率</p>
<p>$o_k$：一个观测，$O=(o_1,o_2,\cdots,o_T)$：观测序列，对指定状态序列过程中产生的每一时刻状态进行观测（产生一个观测），这些观测构成指定序列的概率</p>
<p>$N$：可能的状态数，$Q=\{q_1.q_2.\cdots,q_N\}$：可能的状态集合</p>
<p>$M$：可能的观测数，$V=\{v_1,v_2,\cdots,v_M\}$：可能的观测集合</p>
<p>注意$i∈(i_1,\cdots,i_t)$但$q∈(q_1,\cdots,q_N),v∈(v_1,\cdots,v_M)$</p>
<p>$a_{ij}=P(i_{t+1}|i_t)$：$t$时刻下状态$i_t$到$t+1$时刻下状态$i_{t+1}$的转移概率</p>
<p>$b_{i_t}(o_t)$：$t$时刻下，由状态$i_t$产生的观测$o_t$的概率，<strong>具体的，$b_{q_{i_t}}(o_t)$即状态$i_t$取值为$q_{i_t}$时候，观测为$o_t$的概率</strong></p>
<p><strong>注意</strong> 对于当前状态$\sigma_t$，状态转移矩阵$A$,回到上一状态$\sigma_{t-1}=A\sigma_t$，到达下一状态$\sigma_{t+1}=\sigma_tA$</p>
<h2 id="概率计算算法—预测观测概率-max-P-O-lambda"><a href="#概率计算算法—预测观测概率-max-P-O-lambda" class="headerlink" title="概率计算算法—预测观测概率$\max\{P(O|\lambda)\}$"></a>概率计算算法—预测观测概率$\max\{P(O|\lambda)\}$</h2><h3 id="直接计算"><a href="#直接计算" class="headerlink" title="直接计算"></a>直接计算</h3><p>$T$时间后到达某状态序列概率：$P(I|\lambda)=\pi a_{i_1i_2}a_{i_2i_3},\cdots,a_{i_{T-1}i_T}$</p>
<p>在$I$状态序列下，$T$时间后输出某观测序列$O=(o_1,o_2,\cdots,o_T)$的概率：$P(O|I,\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)\cdots b_{i_T}(o_T)$</p>
<p>$\therefore$$T$时间后到达某状态序列$I$后输出某观测序列的概率为联合分布：</p>
<script type="math/tex; mode=display">
P(O,I|\lambda)=P(O|I,\lambda)P(I|\lambda)=((\pi_{i_1} b_{i_1}(o_1)) (a_{i_1i_2}b_{i_2}(o_2)) \cdots(a_{i_{T-1}i_T}b_{i_T}(o_T))</script><p>（加括号仅为表明每个观测的状态对位关系）</p>
<p>$\therefore$ <strong>为了求得$T$时间后观测得指定观测序列$O=(o_1,o_2,\cdots,o_T)$的概率</strong>，需要边缘化联合概率公式中的$I$，$I=(i_1,i_2,\cdots,i_N),i_1,i_2,\cdots,i_t=1,2,\cdots,N$</p>
<p>$\therefore P(O|\lambda)=\sum_{i_1,i_2,\cdots,i_T}P(O,I|\lambda)=\sum_{i_1,i_2,\cdots,i_T}((\pi_{i_1} b_{i_1}(o_1)) (a_{i_1i_2}b_{i_2}(o_2)) \cdots(a_{i_{T-1}i_T}b_{i_T}(o_T))$</p>
<p><strong>注意表示法$\sum_{i_1,i_2,\cdots,i_T},其中i_1,i_2,\cdots,i_t=1,2,\cdots,N$(每个状态$i_t$下都有$N$种取值)，意思是对所有可能的$I$序列取值组合，计算右边式子之后，再对所有式子结果求和</strong></p>
<p><strong>关系为$\lambda\to I\to O$，所以求$O$的时候可以看成一个全链接路径图或FC DNN</strong></p>
<p>观察这个式子，对于$,i_1=1,2,\cdots,N$：</p>
<p>$t=1: P_1(O|\lambda)=\sum_{i_1}\pi_{i_1} b_{i_1}(o_1)=\pi_{q_1}b_{q_1}(o_1)+\pi_{q_2}b_{q_2}(o_1)+\cdots+\pi_{q_N}b_{q_N}(o_1)$</p>
<p>$t=2: P_2(O|\lambda)=\sum_{i_1,i_2}((\pi_{i_1} b_{i_1}(o_1))a_{i_1i_2}b_{i_2}(o_2)$</p>
<p>$=((\pi_{q_1}b_{q_1}(o_1))a_{q_1q_1}b_{q_1}(o_2)+((\pi_{q_1}b_{q_1}(o_1))a_{q_1q_2}b_{q_2}(o_2)+\cdots+((\pi_{q_1}b_{q_1}(o_1))a_{q_1q_N}b_{q_N}(o_2)+$​</p>
<p>$((\pi_{q_2}b_{q_2}(o_1))a_{q_2q_1}b_{q_1}(o_2)+((\pi_{q_2}b_{q_2}(o_1))a_{q_2q_2}b_{q_2}(o_2)+\cdots+((\pi_{q_2}b_{q_2}(o_1))a_{q_2q_N}b_{q_N}(o_2)+$</p>
<p>$\cdots+$</p>
<p>$((\pi_{q_N}b_{q_N}(o_1))a_{q_Nq_1}b_{q_1}(o_2)+((\pi_{q_N}b_{q_N}(o_1))a_{q_Nq_2}b_{q_2}(o_2)+\cdots+((\pi_{q_N}b_{q_N}(o_1))a_{q_Nq_N}b_{q_N}(o_2)$</p>
<p>$\cdots$</p>
<p>$t=T:\cdots$</p>
<p>这样算下去就很麻烦了，路径非常多，计算量非常大</p>
<p><strong>结论：8行</strong></p>
<h3 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h3><p>观测到式子中存在大量重复子式，如$t=2$时，重复了在$t=1$中的子式$\pi_{q_1}b_{q_1}(o_1),\pi_{q_2}b_{q_2}(o_1),\cdots,\pi_{q_N}b_{q_N}(o_1)$各$N$次，于是想到可以用BP一样的解决办法，即</p>
<p><strong>核心思想</strong>：可以观察到<strong>计算图如同一张全连接的深度神经网络，状态空间维度$N$即为每层隐藏神经元个数，时间序列运行时间$T$即为网络深度</strong>，于是我们只要<strong>保存每一个结点的计算结果</strong>，就可以大幅减少计算量</p>
<p><strong>于是可以写出算法</strong>：</p>
<p>定义到时刻$t$部分观测序列为$o_1,o_2,\cdots,o_t$且状态为$q_i$的概率为前向概率：</p>
<p>$\alpha_t(i)=P(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)$</p>
<p>初始值：$\alpha_1(i)=\pi_ib_i(o_1),i=1,2,\cdots,N$</p>
<p>递推式：$\alpha_{t+1}(i)=[\sum^N_{j=1}\alpha_t(j)\alpha_{ji}]b_i(o_{t+1}),i=1,2,\cdots,N$</p>
<p>终止：$P(O|\lambda)=\sum^N_{i=1}\alpha_T(i)$</p>
<p>很像DNN的前向传递</p>
<p><strong>前向算法最小结构</strong>就是<strong>当前时刻所有状态$i_t$转移到下一时刻的某状态</strong>$i_{t+1}$，有$N$个这一时刻的状态$i_t$，1个下一时刻的<strong>某</strong>状态$i_{t+1}$和1个对下一时刻状态的观测$b_i(o_{t+1})$（观测是先验）</p>
<p><strong>注意一个理解：</strong>整个概率的计算是在时间$t$过程的前向转播中不断迭代产生的，<strong>每一个结点的计算都包含了全路径状态的转移概率和某一时刻的先验观测概率，所以完整的先验需要运行整个过程。</strong>当在$t=T$，最后一层结点计算完的时候，那么再对全状态路径求和才能求得<strong>先验$O=(o_1,o_2,\cdots,o_T)$下的概率</strong>。若对所有可能存在的观测序列$O$计算那必然$P(O|\lambda)=1$。</p>
<h3 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h3><p>同理</p>
<p>定义在时刻$t$状态为$q_i$的条件下，从$t+1$到$T$的部分观测序列$o_{t+1},o_{t+2},\cdots,o_T$的概率为后向概率：</p>
<p>$\beta_t(i)=P(o_{t+1},o_{t+2},\cdots,o_T|i_t=q_i,\lambda)$</p>
<p>初始值：$\beta_T(i)=1,i=1,2,\cdots,N$</p>
<p>递推式：$\beta_{t}(i)=\sum^N_{j=1}a_{ij}b_j(o_{t+1})\beta_{t+1}(j),i=1,2,\cdots,N$</p>
<p>终止：$P(O|\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)$</p>
<p>很像DNN的反向传递…</p>
<p><strong>后向算法最小结构</strong>就是<strong>下一时刻的所有状态$i_{t+1}$回退到当前时刻某状态</strong>$i_{t}$，有1个这一时刻的<strong>某</strong>状态$i_t$，有$N$个下一时刻的状态$i_{t+1}$，和$N$个对下一时刻状态的观测$b_j(o_{t+1})$（观测是先验）</p>
<p>同理，整个概率的计算是在时间$t$过程的后向转播中不断迭代产生的，这个先验需要运行整个过程，只不过这个过程是反着运行的。</p>
<h3 id="其它应用"><a href="#其它应用" class="headerlink" title="其它应用"></a>其它应用</h3><p><strong>利用前向概率与后向概率的定义</strong>，可以将观测序列概率统一可写成：</p>
<p>$P(O|\lambda)=\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j),t=1,2,\cdots,T-1$</p>
<p>式中$\alpha_t(i)$即为$i$状态下$t$时刻以前的概率，$\beta_{t+1}(j)$即为$i$状态下$t+1$时刻以后的概率，这个式子等于枚举出了网络中所有的路径</p>
<ol>
<li><p>给定模型$\lambda$和观测$O$，求在时刻$t$处于状态$q_i$的概率$\gamma_t(i)$</p>
<p>记$\gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}$</p>
<p>由前向概率和后向概率定义可得</p>
<p>$\alpha_t(i)\beta_t(i)=P(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)*P(o_{t+1},o_{t+2},\cdots,o_T|i_t=q_i,\lambda)=P(i_t=q_i,O|\lambda)$</p>
<p>$\therefore \gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum^N_{j=1}\alpha_t(j)\beta_t(j)}$</p>
<p>对各个时刻求和，于是可以求出在观测$O$下状态$i$出现的期望值$=\sum_{t=1}^T \gamma_t(i)$</p>
<p>同理，在观测$O$下由状态$i$转移的期望值$=\sum_{t=1}^{T-1} \gamma_t(i)$</p>
</li>
<li><p>给定模型$\lambda$和观测$O$，在时刻$t$处于状态$q_i$且在时刻$t+1$处于状态$q_j$的概率$\epsilon_t(i,j)$</p>
<p>$\epsilon_t(i,j)=P(i_t=q_i,i_{t+1}=q_j|O,\lambda)=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)}=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{\sum^N_{i=1}\sum^N_{j=1}P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}$</p>
<p>由于$P(i_t=q_i,i_{t+1}=q_j,O|\lambda)=\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)$</p>
<p>$\therefore \epsilon_t(i,j)=P(i_t=q_i,i_{t+1}=q_j|O,\lambda)=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum^N_{i=1}\sum^N_{j=1}\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}$</p>
<p>在观测$O$下由状态$i$转移到状态$j$的期望值$=\sum_{t=1}^{T-1} \epsilon_t(i,j)$</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>上帝算法：EM</title>
    <url>/2019/10/23/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="现实中的例子"><a href="#现实中的例子" class="headerlink" title="现实中的例子"></a>现实中的例子</h2><p>对于NLP中的PLSA模型，$d∈D$表示文档，$w∈V$表示词语，$z$表示隐含的主题。</p>
<p>由于隐变量的存在，</p>
<p>则$P(d_i,w_j)=P(d_i)P(w_j|d_i)=P(d_i)\sum_{k=1,\cdots,K}P(w_j|z_k)P(z_k|d_i)$</p>
<p>这里$z$就是不知道的变量=潜变量=隐变量</p>
<p>样本集分布为$P(D,V)=\prod_{i=1,\cdots,N}\prod_{j=1,\cdots,M}P(d_i,w_j)^{n(w_j,d_i)}$</p>
<p>$n(w,d)$表示$w$在$d$中出现的次数</p>
<p>于是对其求对数最大似然</p>
<p>$L(\theta)=\log P(D,V;\theta)=\sum_{i=1,\cdots,M}\sum_{j=1,\cdots,N}n(w_j,d_i)\log P(d_i,w_j)$</p>
<p>其中$\theta=\{P(w_j|z_k),P(z_k|d_i)\}$</p>
<p>要求$\theta^*=\arg\max L(\theta)$</p>
<p>则令$\frac{\partial L(\theta)}{\partial \theta}=0$以求解</p>
<p><strong>由于隐变量的存在</strong>，我们需要对隐变量求边缘化（见第一个式子），于是求解MLE需要对<strong>和的对数</strong>：$\log P(d_i,w_j)$求导（其中$P(d_i,w_j)$是一个和式）。这样先求和再求导下来表达式一长串，导致求导非常困难，所以才要用近似MLE方法</p>
 <a id="more"></a>
<h2 id="Expectation-Maximization原理"><a href="#Expectation-Maximization原理" class="headerlink" title="Expectation Maximization原理"></a>Expectation Maximization原理</h2><p>期望最大化，EM算法</p>
<p><strong>思想：通过迭代增大下界的最大对数似然的方式逐步近似极大化$L(\theta)$，每一次迭代$L(\theta)&gt;L(\theta^{i+1})&gt;L(\theta^i)&gt;\cdots$，计算迭代式的最大对数似然（最大化下界，该下界表达式里使得原式中的对和的对数求导被转化为对对数的和求导）非常容易，以此来逐渐接近$L(\theta)$；那么如何构造下界？Jensen不等式！</strong></p>
<p>设$Y$为<strong>观测数据</strong>（不完全数据），$Z$是<strong>未观测数据</strong>，$\theta^i$表示第$i$次迭代的模型参数$\theta$</p>
<p>可以理解为$\theta\to Z\to Y$</p>
<p>$L(\theta)=\log P(Y|\theta)=\log \sum_Z P(Y,Z|\theta)$(逆边缘化+贝叶斯公式)</p>
<p>$=\log (\sum_ZP(Z,\theta)P(Y|Z,\theta))$（全概率公式）</p>
<p>那么计算第$i$次迭代后的差</p>
<p>$L(\theta)-L(\theta^i)=\log (\sum_ZP(Z,\theta)P(Y|Z,\theta))-\log P(Y|\theta^i)$</p>
<p>下面拼凑一个$(Z|Y,\theta^i)$出来，<strong>拼凑的目的是为了构造出Jensen不等式的形式以创造出一个表示下界的不等式</strong>，因为显然$\sum_Z P(Z|Y,\theta^i)=1$</p>
<p>$=\log(\sum_Z P(Z|Y,\theta^i)\frac{P(Z,\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^i)})-\log P(Y|\theta^i)$</p>
<p><strong>由Jensen不等式</strong>，$\log \sum_j\lambda_jy_j≥\sum_j\lambda_j\log y_j,\lambda_j≥0,\sum_j\lambda_j=1$</p>
<p>$\therefore L(\theta)-L(\theta^i)≥=\sum_Z P(Z|Y,\theta^i)\log\frac{P(Z,\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^i)}-\log P(Y|\theta^i)$</p>
<p>$\because\sum_Z P(Z|Y,\theta^i)=1$，于是可以<strong>在负部凭空构造出此求和以合并两式</strong></p>
<p>$\therefore L(\theta)-L(\theta^i)=\sum_Z P(Z|Y,\theta^i)\log\frac{P(Z,\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^i)}-\sum_ZP(Z|Y,\theta^i)\log P(Y,\theta^i)$</p>
<p>$=\sum_Z [P(Z|Y,\theta^i)\log\frac{P(Z,\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^i)}-P(Z|Y,\theta^i)\log P(Y,\theta^i)]$</p>
<p>$=\sum_Z P(Z|Y,\theta^i)\log\frac{P(Y|Z,\theta)P(Z,\theta))}{P(Z|Y,\theta^i) P(Y|\theta^i)}$</p>
<p><strong>这里通过Jensen不等式同时也将和的对数，转换成了对数的和，$\log$被拿进了$\sum$里面，计算其导数（求最大似然）（对每个参数求偏导）就容易多了</strong></p>
<p>$\Delta =\sum_Z P(Z|Y,\theta^i)\log\frac{P(Y|Z,\theta)P(Z,\theta))}{P(Z|Y,\theta^i) P(Y|\theta^i)}$</p>
<p>令$B(\theta,\theta^i)=L(\theta^i)+ \Delta$</p>
<p>则$L(\theta)≥B(\theta,\theta^i)$，即$B(\theta,\theta^i)$为$L(\theta)$的一个<strong>下界</strong>。那么极大化$B(\theta,\theta^i)$下界自然也<strong>等同于</strong>极大化$L(\theta)$</p>
<p>$\therefore \theta^{i+1}=\arg \max_\theta B(\theta,\theta^i)≤\arg\max L(\theta)$</p>
<p>同时注意到$L(\theta^i)=B(\theta^i,\theta^i)$，很显然，如果$\theta=\theta^i$，即通过迭代$i$次之后模型参数已经完全一样，那自然是极大对数似然=近似极大对数似然</p>
<p>$\therefore \theta^{i+1}=\arg \max_\theta(L(\theta^i)+\sum_Z P(Z|Y,\theta^i)\log\frac{P(Y|Z,\theta)P(Z,\theta))}{P(Z|Y,\theta^i) P(Y|\theta^i)})$</p>
<p>该式子中$P(Z|Y,\theta^i) P(Y|\theta^i)和L(\theta)$与极大化$\theta$没有关系，所以可以直接忽略（注意$\log$外的$P(Z|Y,\theta^i)$不忽略，因为有它方便计算），$P(Z|Y,\theta^i)$是一个可以求得的<strong>潜变量后验概率</strong></p>
<p>且$\because P(Y,Z|\theta)=P(Y|Z,\theta)P(Z,\theta))$</p>
<p>$\therefore \theta^{i+1}=\arg\max_\theta(\sum_Z P(Z|Y,\theta^i)\log P(Y,Z|\theta))$，</p>
<p>记为$\arg\max_\theta Q(\theta,\theta^i)$</p>
<p>$Q(\theta,\theta^i)=\sum_Z P(Z|Y,\theta^i)\log P(Y,Z|\theta)=\mathbb E_Z[\log P(Y,Z|\theta)|Y,\theta^i]$</p>
<p>$\theta^i$是要迭代的参数，初始为$\theta^0$</p>
<p>即：<strong>完全数据的对数似然在后验潜变量上的期望</strong>，此式子为EM算法的核心</p>
<h2 id="敛散性"><a href="#敛散性" class="headerlink" title="敛散性"></a>敛散性</h2><p>定理一：设$P(Y|\theta)$为观测数据的似然函数，$\theta^i(i=1,2,\cdots)$为EM算法得到的参数估计序列，$P(Y|\theta^i)(i=1,2,\cdots)$为对应的似然函数序列，则$P(Y|\theta^i)$是单调递增的，即</p>
<script type="math/tex; mode=display">
P(Y|\theta^{i+1})≥P(Y|\theta^i)</script><p>（也就是说每一次迭代的预测必然比上一次迭代更好，这是EM算法的核心之一，从道理上能理解，这里数学证明）</p>
<p>定理二：设$L(\theta)=\log P(Y|\theta)$为观测数据的对数似然函数，$\theta^i(i=1,2,\cdots)$为EM算法得到的参数估计序列，$L(\theta^i)(i=1,2,\cdots)$为对数函数似然序列</p>
<ol>
<li>如果$P(Y|\theta)$有上界，则$L(\theta^i)=\log P(Y|\theta^i)$收敛到某一值（单调有界定理）</li>
<li>在函数$Q(\theta,\hat\theta)$与$L(\theta)$满足一定条件下，由EM算法得到的参数估计序列$\theta^i$的收敛值$\theta^*$是$L(\theta)$的稳定点</li>
</ol>
<p>证略</p>
<p><strong>显然EM算法不能保证找到全局最优值，只能收敛到稳定点，也因此初值选择非常重要</strong></p>
<h2 id="Expectation-Maximization-Algorithm"><a href="#Expectation-Maximization-Algorithm" class="headerlink" title="Expectation Maximization Algorithm"></a>Expectation Maximization Algorithm</h2><p>于是根据原理推导出来的公式可以给出EM算法：</p>
<p>对于观测变量数据$Y$，隐变量数据$Z$，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$，选择初值$\theta^0$开始迭代</p>
<p><strong>E-step：</strong>在第$i+1$步的时候，计算在第$i$次迭代的参数估计值$\theta^i$的$Q$函数：</p>
<script type="math/tex; mode=display">
Q(\theta,\theta^i)=\sum_Z P(Z|Y,\theta^i)\log P(Y,Z|\theta)=\mathbb E_Z[\log P(Y,Z|\theta)|Y,\theta^i]</script><p><strong>M-step：</strong></p>
<script type="math/tex; mode=display">
\theta^{i+1}=\arg\max_\theta Q(\theta,\theta^i)</script><p>迭代直到$||\theta^{i+1}-\theta^i||或||Q(\theta^{i+1},\theta^i)-Q(\theta^{i},\theta^i)||$足够小</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p><strong>边缘化</strong>：实际上就是求出边缘概率以消去变量 $p(x)=\int_{-∞}^{+∞}p(y)dy$，那么$y$就被消去了，通过边缘化可以获得我们想要的任何属性（推理） </p>
<p><strong>概率论中的条件期望</strong>：$\mathbb E_Z[P(X)|C]=\int_ZP(X)P(Z|C)dZ$</p>
<p><strong>在推导中常犯的一个马虎错误：</strong>看清楚积分的作用域，比如$\sum_xf(x)g(x)$，如果$\sum_x f(x)=1$，别直接就给求和了= =，后面还有一个带积分变量$x$的函数呢！</p>
<p><strong>求极大值</strong>可以用求<strong>0导数法</strong>也可以用<strong>拉格朗日乘数法将边界条件限制（经常需要如此！）</strong>进去</p>
<p><strong>算Q</strong>可以借助期望运算的性质来减小运算量</p>
<p>EM算法实在是非常的美妙，很多人称其为“上帝的算法”；更多扩展可以看知乎上的这篇文章：<a href="https://www.zhihu.com/question/40797593/answer/275171156" target="_blank" rel="noopener">EM算法存在的意义是什么？</a>，包含了诠释EM算法的九层境界</p>
<p><strong>为什么要构造下界？</strong>是因为我们想要消去和的对数这种很难计算的东西使用了Jensen不等式，而Jensen不等式的副作用就是产生了不等式，所以才会想着通过迭代容易计算的下界的方法来逼近</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks</title>
    <url>/2019/10/21/NETWORKS/</url>
    <content><![CDATA[<blockquote>
<p>《Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks》<br>Alec Radford&amp;Luke Metz ,Soumith Chintala<br>ICLR 2016</p>
</blockquote>
<h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><ul>
<li><p>Unsupervised representation learning 无监督表示学习领域（聚类，AE，DBN）</p>
</li>
<li><p>Generative image models生成模型（参数化模型，非参数化模型）</p>
<p>Variational sampling approach (2013) 模糊</p>
<p>GAN(2014) 噪点多，难以理解</p>
<p>A recurrent network approach(Gregor et al.,2015, a deconvolution network approach(Dosovitskiy et al.,2014)  未用生成器监督</p>
</li>
<li><p>VISUALIZING THE INTERNALS OF CNNS 通过反卷积和最大池化可以找到网络中每个卷积核学到的内容 (Zeiler &amp; Fergus, 2014) ，对输入使用梯度下降可以推断出卷积核的理想子集(Mordvintsev et al.).</p>
</li>
</ul>
<a id="more"></a>
<h2 id="APPROACH-AND-MODEL-ARCHITECTURE"><a href="#APPROACH-AND-MODEL-ARCHITECTURE" class="headerlink" title="APPROACH AND MODEL ARCHITECTURE"></a>APPROACH AND MODEL ARCHITECTURE</h2><p>Core to our approach is adopting and modifying three recently demonstrated changes to CNN architectures. </p>
<p>模型采用CNN</p>
<ol>
<li><p>all convolutional net (Springenberg et al., 2014) </p>
<p><strong>不使用池化层</strong></p>
<p>在判别模型中用<strong>步长卷积 (strided convolutions )</strong>替代</p>
<p>在生成模型中用<strong>转置卷积/微步卷积（fractionally-strided convolutions）</strong>替代</p>
<p>这样可以允许生成和判别模型学习自己空间的下采样，使得网络可微。</p>
</li>
<li><p>trend towards eliminating fully connected layers on top of convolutional features <strong>不使用FC层</strong>，替而代之使用<strong>global pooling（全局池化层）</strong></p>
<p>目的是减轻运算量，提高了模型的稳定性，但不利于收敛速度。 </p>
</li>
<li><p>Batch Normalization <strong>批归一化</strong> (Ioffe &amp; Szegedy, 2015)  （BN现在已经是非常常用的工具了）</p>
<p>有效解决GANs训练中常遇到的梯度消失问题，也解决初始化问题，并防止生成模型把所有样本都收敛到一个点</p>
<p>但不要对生成模型的输出层和判别模型的输入层使用，否则会导致模型不稳定</p>
</li>
<li><p>activation 激活函数的选择</p>
<p>生成模型中使用ReLU，除了输出层用tanh</p>
<p>判别模型使用Leaky ReLU，尤其在高分辨率模型中效果非常好</p>
</li>
</ol>
<p><strong>生成模型结构：</strong></p>
<p><img src="//aisakaki.com/2019/10/21/NETWORKS/0.png" alt="0"></p>
<h2 id="实验结论与方法总结"><a href="#实验结论与方法总结" class="headerlink" title="实验结论与方法总结"></a>实验结论与方法总结</h2><ol>
<li><p><strong>对于无监督表示学习算法通常的验证方法</strong>： apply them as a feature extractor on supervised datasets and evaluate the performance of linear models ﬁtted on top of these features. 在监督学习数据集中，将训练好的模型当成特征提取器，使用线性模型用来匹配特征以衡量性能（相当于将监督学习训练好的模型作为判别器）</p>
<p>作者使用在ImageNet-1K上训练得到的生成模型，使用所有层的所有CNN特征作为输入，将每一层的CNN特征使用最大池化的方式降到4×4，扁平化成一个28672维的向量，输入到L2-SVM中，明显DCGAN效果优于其他无监督表示模型</p>
</li>
<li><p>对于隐空间，可以对经过生成模型之前的初始向量进行差值的方式探索隐空间</p>
</li>
<li><p>如果图像急剧变化，可能是因为没有学习到特征，只是记住了图像；如果缓慢渐变，说明学习到了特征</p>
</li>
<li><p>在标签数据较少的情况下，DCGAN效果最好。 </p>
</li>
<li><p>使用 guided backpropagation ( Striving for Simplicity: The All Convolutional Net  Springenberg et al.,2014)可视化判别模型中的特征 </p>
</li>
<li><p>GAN中可以类似word2vec一样进行向量运算，直接对输入生成模型的那个向量进行加减</p>
<p>如vector(”King”)-vector(”Man”)+vector(”Woman”)</p>
</li>
<li><p>去除掉某物体</p>
<p>对窗户特征进行dropout</p>
</li>
</ol>
<hr>
<p>补充：</p>
<p><strong>全局平均池化：global average pooling（GAP）</strong></p>
<p>$W×H×C\to1×1×C$</p>
<p>即将一个Channel变成成一个值，原Channel每个位置的权值为$\frac{1}{W×H}$</p>
<p>GAP的真正意义是:<strong>对整个网路在结构上做正则化防止过拟合。</strong>其直接剔除了全连接层中黑箱的特征，直接赋予了每个channel实际的内别意义。 </p>
<p><strong>反卷积/转置卷积/微步卷积（fractionally-strided convolutions）</strong></p>
<p>判别模型输入维度&gt;输出维度，需要用卷积，下采样；</p>
<p>但生成模型输入维度&lt;输出维度，就需要用卷积的逆过程，也就是反卷积，也就是上采样</p>
<p><strong>反卷积的输出公式：</strong> </p>
<p>$O=S<em>(I-1)+K-2</em>P$</p>
<p>(O=output, S=Stride,I=Input,K=Kernel,P=Padding)</p>
<p><strong>上采样(upsampling)</strong></p>
<p>一般两种方法：</p>
<p>①图像缩放，如双线性插值</p>
<p>②逆卷积/转置卷积</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>游戏中的心理学</title>
    <url>/2019/10/20/%E6%B8%B8%E6%88%8F%E4%B8%AD%E7%9A%84%E5%BF%83%E7%90%86%E5%AD%A6/</url>
    <content><![CDATA[<p><em>week2-游戏中的心理学 课堂总结</em></p>
<h2 id="乐趣原理"><a href="#乐趣原理" class="headerlink" title="乐趣原理"></a>乐趣原理</h2><p>人为什么需要玩游戏？为什么在漫长的进化过程中，只有人类有对游戏的需要？</p>
<p>游戏的本质：人类对现实或超现实世界里 <strong>某种模式</strong> （对事物的规律的抽象提炼）的仿真和模仿</p>
<p>掌握和学习模式其实是人类进化的需求。人类的大脑有一种其他生物不具备的能力和喜好，就是从现实环境中不断<strong>识别和仿真模式</strong></p>
<a id="more"></a>
<p><del>少年，你爱玩游戏的原因其实是你的基因里被赋予了重大的种族使命！</del></p>
<p><strong>WTCAF心理循环</strong>（W循环）</p>
<p>W（watch）：玩家会观察目标事件，观察并尝试理解相关事务的运行，尝试从中发现潜在的<strong>模式</strong></p>
<p>T（Think）：玩家理解模式后会尝试发现其中存在的<strong>规则</strong>，以分析思考如何对其进行仿真</p>
<p>C（Choose）：玩家需要找到一个<strong>目标</strong>来进行判断和自己的行为选择</p>
<p>A（Action）：玩家决定付诸行动，向系统<strong>输出</strong>的自己选择的行为</p>
<p>F（Feedback）：系统根据玩家输入继续运行，并输出给玩家一个<strong>反馈</strong>来告诉玩家游戏结果</p>
<p>=》乐趣（多巴胺）</p>
<p>当WTCAF循环到极限状态：<strong>心流</strong>（技能与挑战都到达对等极限，最佳游戏状态）</p>
<p><img src="//aisakaki.com/2019/10/20/游戏中的心理学/0.png" alt="0"></p>
<p>成瘾原理</p>
<p><strong>一个好的游戏模式至少需要具备几个特点：</strong></p>
<p>具有可以被识别并易于模拟的模式</p>
<p>这个模式具有比较多样的结果，不容易被玩家穷尽所有可能</p>
<p>模式具有一定的变化性，让玩家始终需要调整和学习</p>
<p>其他的：目标，奖惩机制</p>
<h2 id="构建基本乐趣循环"><a href="#构建基本乐趣循环" class="headerlink" title="构建基本乐趣循环"></a>构建基本乐趣循环</h2><p><strong>三项基本要素（缺一不可）</strong>——W三角：<strong>模式、目标、奖惩</strong></p>
<h2 id="五大核心乐趣元素"><a href="#五大核心乐趣元素" class="headerlink" title="五大核心乐趣元素"></a>五大核心乐趣元素</h2><p>基本乐趣循环</p>
<p>内容探索发现</p>
<p>角色环境代入</p>
<p>成长目标追求</p>
<p>社群交互价值</p>
<p>一般来说，一个游戏不会拥有所有五大核心，如果真的要做这样一个五边形游戏，那必然极其复杂且小众，因为五大核心之间存在矛盾</p>
<p>所以一个游戏的基本玩法需要有一个可被玩家理解的玩法模式，并提供可以供玩家理解和思考的规则和目标，并提供可以输入行为决策的入口，并根据玩家输入和系统规则给出玩家胜负的反馈</p>
<h2 id="乐趣空间"><a href="#乐趣空间" class="headerlink" title="乐趣空间"></a>乐趣空间</h2><p>依靠难度和奖惩机制，我们可以为一个玩法模式构建出一个乐趣空间</p>
<p>这个空间的衡量尺度是用户能力，我们一般用主要目标市场的平均水平来衡量</p>
<p>乐趣空间的范围：思维与操作（大脑与小脑的能力）</p>
<p><img src="//aisakaki.com/2019/10/20/游戏中的心理学/1.png" alt="1"></p>
<h2 id="庸众的胜利"><a href="#庸众的胜利" class="headerlink" title="庸众的胜利"></a>庸众的胜利</h2><p>举例：FPS进化史</p>
<p>①<em>雷神之锤</em> 10把武器，3维地图，必须掌控全场，“空战”FPS，门槛极高</p>
<p>②<em>三角洲</em>  </p>
<p>③<em>CS</em> CS的每一个版本都在削弱上限</p>
<p>④<em>CF</em> 复活时间短，降低WTCAF</p>
<p>⑤<em>PUBG</em> 枪法已经不重要了</p>
<p><em>难度不断降低，门槛不断降低，受众不断变广</em></p>
<p>这个趋势会持续下去吗？不会，看图</p>
<p>如何寻找合适的目标市场，并且找到他们的Point点，圈住最主流的用户，一直是制作人们最头疼的问题</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>今天选出了PM，真是各路大佬各显神通….人均做过游戏，人均拿奖到手软，人均编程美术策划全能….甚至有个负责的游戏DAU 0.5亿的大佬，还有上来就展示导过的电影的…….</p>
<p>瑟瑟发抖……….</p>
<p>下周开始就要正式分组进行Mini-game制作了，我也是加把劲骑士！</p>
]]></content>
      <categories>
        <category>游戏设计</category>
      </categories>
  </entry>
  <entry>
    <title>且听风吟</title>
    <url>/2019/10/19/%E4%B8%94%E5%90%AC%E9%A3%8E%E5%90%9F/</url>
    <content><![CDATA[<p>尝试着给主页换了很多副标题，但最后还是用了「且听风吟」这四个字。</p>
<p>「且听风吟」来自村上春树的同名小说《且听风吟》</p>
<p>原文是「 風の歌を聴け」，其实字面意思与原意有一些差别。</p>
<p> 「 且 」字给原意加入了一层浪漫与释然的情感，</p>
<p>但我觉得这是符合书意的。</p>
<p>其实我不是很喜欢村上春树讲故事的风格，充满后现代主义风格和西式的文风，少了些日本传统文学的物哀之情。但《且听风吟》的故事性本身就不是很强，或者说故事本身并不重要。</p>
<p>村上春树在他20岁的尽头写下了这本书</p>
<blockquote>
<p>好久没有感觉出夏日的气息了。海潮的清香，遥远的汽笛，女孩肌体的感触，洗发香波的气味，傍晚的和风，缥缈的憧憬，以及夏日的梦境……然而，这一切宛若移动过的复写纸，无不同原有位置有着少许然而却是无可挽回的差异。 </p>
</blockquote>
<p>21岁的村上经历了百无聊赖的夏天，遇到了三个女孩，和鼠在酒吧里一起喝着酒聊着各种各样的话题···</p>
<p>但夏天结束了，就像一场梦境一样。他还会去那个阶梯看看，去聆听那个夏天的声音，闻一闻海潮的清香，去感受夏天的和风。可是无论如何，都不能回到那个夏天了。</p>
<p>时光就是这样流逝着，如穿过指尖的沙一般，无论如何焦急地想要挽留，于己，无论是14岁那年，还是17岁那年，还是现在。</p>
<a id="more"></a>
<p><div><br></div><br>门外已经听到了冬天沉重的脚步声</p>
<p>又要到失去色彩的季节，人们穿着笨重的衣服，戴着大耳机，在仅属于自己的世界里蜷缩着</p>
<p>世界变得很小</p>
<p>我讨厌这样。</p>
<p><div><br></div><br>想起在云村看到的一段话</p>
<blockquote>
<p>日语里“夏天结束了”这句话，包含了多少不可言说的含义，是一夜长大的意思，是恋爱无疾而终的预兆，是青春消失殆尽的季节，是从梦想跌入到现实的分界点，是失去童真变成大人的夜晚，也是人生从充满期待的未知陷落到无可改变的已知的无所适从。</p>
</blockquote>
<p>纵然夏天已经结束了罢，再也回不到曾经的夏天了罢，</p>
<p>无论是热闹还是孤独，坚定还是彷徨，或懊悔，或怀念，或思忆，或迷茫，都终将幻化成风</p>
<p>可是穿越了秋冬春，终会迎来下一个新生的夏天，无论它是否还会如此绚烂</p>
<p>我还会回到那条夕阳下的街道去，胸口还会因为当时突如其来的偶遇而震动；</p>
<p>我还会回到午后惬意的校园，捂住耳朵，能听到放学后活动室内的喧嚣；</p>
<p>在夜幕之下抬头，闭上眼睛，我还会回忆起第一次穿着浴衣看到满天花火在夜幕中绽放的热泪盈眶。</p>
<p>也许这种情感本身，早已成为生命的一部分。</p>
<p><div><br></div><br>且静静地倾听着时光如风般流逝着，与生命交汇所发出的轻轻吟唱</p>
<p><strong>且听风吟</strong></p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>村上春树</tag>
      </tags>
  </entry>
  <entry>
    <title>配分函数</title>
    <url>/2019/10/18/%E9%85%8D%E5%88%86%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p><em>本文为 Deep Learning Ch.18 学习后总结的笔记</em></p>
<h2 id="配分函数"><a href="#配分函数" class="headerlink" title="配分函数"></a>配分函数</h2><p>配分函数是统计物理学中常使用的概念，在前面<a href="https://aisakaki.com/2019/10/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B/">深度学习的结构化概率模型</a>中出现了配分函数：$p(x)=\frac{1}{Z}\hat{p}(\vec x)$</p>
<p>$x$为模型中的所有参数组成的向量，记为：</p>
<script type="math/tex; mode=display">
p(x)=\frac{1}{Z}\hat{p}(x;\theta)</script><p>为归一化此式子我们需要等比缩放，将$\hat{p}(x)$缩放到1，即：</p>
<script type="math/tex; mode=display">
Z=\int \hat{p}(x;\theta)dx</script><script type="math/tex; mode=display">
or,Z=\sum_{\vec x}\hat{p}(x;\theta)</script><p>对其求对数似然梯度：</p>
<script type="math/tex; mode=display">
\therefore \nabla_{MLE}=\nabla_\theta \log \hat{p}(x;\theta)=\nabla_\theta \log \frac{\hat{p}(x)}{\int \hat{p}(x;\theta)dx}</script><script type="math/tex; mode=display">
=\nabla_\theta\log \hat{p}(x)-\nabla_\theta\log \int \hat{p}(x;\theta)dx</script><p>前半部分分为正相，后半部分称为负相</p>
<a id="more"></a>
<p>正相很好解决，对于一个mini-batch中的样本直接计算即可</p>
<p>负相就非常难或者无法计算了，可以看到$\int \hat{p}(x;\theta)dx$实际上就是对概率图网络中的所有可能出现的$\vec x$求积分</p>
<h2 id="MCMC求负相"><a href="#MCMC求负相" class="headerlink" title="MCMC求负相"></a>MCMC求负相</h2><p>从离散表达式开始推导</p>
<script type="math/tex; mode=display">
\nabla_\theta \log Z=\frac{\nabla_\theta Z}{Z}</script><script type="math/tex; mode=display">
=\frac{\nabla_\theta\sum_{\vec x}\hat{p}(x;\theta)}{Z}</script><script type="math/tex; mode=display">
=\frac{\sum_{\vec x}\nabla_\theta\hat{p}(x;\theta)}{Z}</script><p>显然$e^{\log \hat{p}(x;\theta)}=\hat{p}(x;\theta)$，这里要这样变换的目的是为了凑出$\mathbb{E}_{x～p(x)}=\int p(x)f(x)$形式，以使用MCMC算法来模拟</p>
<script type="math/tex; mode=display">
\therefore \nabla_\theta \log Z=\frac{\sum_{\vec x}e^{\log \hat{p}(x;\theta)}\nabla_\theta\hat{p}(x;\theta)}{Z}</script><script type="math/tex; mode=display">
=\frac{\sum_{\vec x}\hat{p}(x)\nabla_\theta\log \hat{p}(x)}{Z}</script><script type="math/tex; mode=display">
=\sum_{\vec x}p(x)\nabla_\theta\log \hat{p}(x)</script><script type="math/tex; mode=display">
=\mathbb{E}_{\vec x～p(\vec x)}\nabla_\theta\log \hat{p}(\vec x)</script><p>这样就可以用MCMC方法对一个mini-batch的数据运用MCMC算法来求$\nabla_\theta \log Z$了，连续情况同理</p>
<p>在深度学习中，经常采用能量函数$p(x)=e^{-E(x)}$来参数化$p(x)$，这种情况下正相可以解释为压低训练样本的能量，负相可以解释为提高模型抽取出的样本的能量。确切地说就是正相推高了数据点未归一化的概率，负相压低了数据点未归一化的概率，当数据分布和模型分布相等时，正相推高数据点和负相压低数据点的机会相等，这时候就不会有任何梯度了，训练停止。</p>
<p>于是就可以求得$\nabla_\theta \log \hat{p}(x;\theta)$然后使用梯度算法更新模型参数$\theta$</p>
<h2 id="随机最大似然和对比散度"><a href="#随机最大似然和对比散度" class="headerlink" title="随机最大似然和对比散度"></a>随机最大似然和对比散度</h2><p>如果对每个batch都做MCMC算法直到马尔科夫过程收敛才开始采样，磨合过程太花时间。</p>
<p>以下两种算法都是基于一个思想：初始化马尔科夫链为一个非常接近模型分布的分布，从而大大减小磨合过程 </p>
<p><strong>对比散度</strong>（contrastive divergence）：在每个步骤中初始化马尔科夫链为采样自数据分布中的样本（具有$k$个Gibbs步骤的叫CD-$k$）。在开始的时候随着正相学习到模型的概率越来越精准，负相也会相应有所提高。</p>
<p><strong>随机最大似然</strong>（stochastic maximum likelihood）也叫 <strong>持续对比发散</strong>（persistent contrastive divergence）：在每个梯度步骤中初始化马尔科夫链为先前梯度步骤的状态值（每个更新中具有$k$个Gibbs步骤的叫PCD-$k$）</p>
<p>一般来说PCD的方法好于CD</p>
<p><strong>以下几种方法是绕过了对配分函数进行求值</strong></p>
<h2 id="伪似然"><a href="#伪似然" class="headerlink" title="伪似然"></a>伪似然</h2><p>无向图模型是非贝叶斯网络，所以在计算中会出现$\frac{p(a)}{p(b)}=\frac{\frac{1}{Z}\hat{p}(a)}{\frac{1}{Z}\hat{p}(b)}=\frac{\hat{p}(a)}{\hat{p}(b)}$形式，抵消掉了配分函数</p>
<p>假设$x=\{a,b,c\}$，$a$包含想要的条件分布变量，$b$包含所有想要条件化的变量，$c$包含除此之外的变量。于是根据贝叶斯公式，$p(a|b)$可以表示为概率相除的形式：</p>
<script type="math/tex; mode=display">
p(a|b)=\frac{p(a,b)}{p(b)}=\frac{p(a,b)}{\sum_{a,c}p(a,b,c)}=\frac{\frac{1}{Z}\hat{p}(a,b)}{\sum_{a,c}\frac{1}{Z}\hat{p}(a,b,c)}=\frac{\hat{p}(a,b)}{\sum_{a,c}\hat{p}(a,b,c)}</script><p>最理想的情况是$a=1,c=0$，这样计算起来就洒洒水了。但实际在计算对数似然$\log p(x)$的时候，即使使$a$足够小，$c$依然可非常以大。那么一个假设就是<strong>将$c$移动到$b$中以减少计算代价</strong>。Marlin and de Freitas(2011)证得了这种方法的有效性。</p>
<h2 id="得分匹配"><a href="#得分匹配" class="headerlink" title="得分匹配"></a>得分匹配</h2><p>在前面的文章<a href="https://aisakaki.com/2019/10/09/Autoencoder/">Autoencoder</a>中用到过</p>
<p>定义得分（score）：$\nabla_x\log p(x)$</p>
<p>其<strong>训练策略是最小化模型对数密度和数据对数密度关于输入的导数之间的平方差期望</strong></p>
<p>这样就可以<strong>避免了$Z$带来的问题，因为$Z$不是$x$的函数</strong>，所以$\nabla_xZ=0$</p>
<p>$L(x,\theta)=\frac{1}{2}||\nabla_x\log p_{model}(x;\theta)-\nabla_x\log p_{data}(x)||_2^2$</p>
<p>$J(\theta)=\frac{1}{2}\mathbb{E}_{p_{data(x)}}L(x;\theta)$</p>
<p>$\theta^*=\min_{\theta}J(\theta)$</p>
<h2 id="比率匹配"><a href="#比率匹配" class="headerlink" title="比率匹配"></a>比率匹配</h2><p>将得分匹配推广到离散情况，主要运用在二元模型</p>
<script type="math/tex; mode=display">
L^{RM}(x,\theta)=\sum^n_{j=1}(\frac{1}{1+\frac{p_{model}(x;\theta)}{p_{model}(f(x,j);\theta)}})^2</script><p>$f(x,j)$为$j$处位置取反的$x$</p>
<p>配分函数会在两个概率的比例中被抵消</p>
<p>比例匹配还可以作为处理高维稀疏数据的基础（如词计数向量）</p>
<h2 id="噪声对比估计"><a href="#噪声对比估计" class="headerlink" title="噪声对比估计"></a>噪声对比估计</h2><p>Noise Contrastive Estimation, NCE</p>
<p>思路：用一个变量来代替负相</p>
<script type="math/tex; mode=display">
\nabla_{\theta}\log p_{model}(x)=\nabla_{\theta}\log \hat{p}_{model}(x)+c</script><p>$c$作为模型训练学习的参数之一，问题在于如何训练</p>
<p>不能使用最大似然估计方法了，因为$c$会被无限扩大。于是将问题转化为一个<strong>概率二分类器的监督学习问题</strong>，通俗地说就是通过训练可以让模型分得清噪声和数据。</p>
<p>引入等概率二值分布$p_{noise}$，使得$p_{joint}(x|y=1)=p_{model}(x),p_{joint}(x|y=0)=p_{noise}(x)$</p>
<script type="math/tex; mode=display">
\therefore p_{joint}(y=1|x)\frac{p_{model}(x)}{p_{model}(x)+p_{noise}(x)}</script><script type="math/tex; mode=display">
=\frac{1}{1+\frac{p_{noise}(x)}{p_{model}(x)}}=\frac{1}{1+e^{\log \frac{p_{noise}(x)}{p_{model}(x)}}}</script><script type="math/tex; mode=display">
=\sigma(\log p_{model}(x)-\log p_{noise}(x))</script><p>可以看到实际上是最大化模型与噪声分布之间的对数概率之差，也就是使得真实数据与噪声之间的差异最大，这便是好的模型，<strong>也是GAN的基本思想</strong></p>
<h2 id="估计配分函数"><a href="#估计配分函数" class="headerlink" title="估计配分函数"></a>估计配分函数</h2><p>在评估模型相对性能的时候，可能需要计算$\sum_i\log p_A(\vec x;\theta_A)&gt;\sum_i\log p_B(\vec x;\theta_B)$是否成立，如果成立则A更优，这时候经过变化可以求得一个比值，求得该比值就可以判断谁大谁小。于是如果知道配分函数的比值，就可以判断该式子的大小关系。</p>
<p>如何获得配分函数的值？根据<strong>重要采样</strong>，使用提议分布$p(x)=\frac{1}{Z_0}\hat{p}_0(x)$</p>
<p>$Z_1=\int\hat{p}_1(x)dx$</p>
<p>$=\int\frac{p_0(x)}{p_0(x)}\hat{p}_1(x)dx$</p>
<p>$=Z_0\int p_0(x)\frac{\hat{p}_1(x)}{\hat{p}_0(x)}dx$</p>
<p>于是使用蒙特卡罗方法：</p>
<script type="math/tex; mode=display">
Z_1=\frac{Z_0}{K}\sum^K_{k=1}\frac{\hat{p}_1(x^{(k)})}{\hat{p}_0(x^{(k)})}</script><p>可以计算得到配分函数比值为$\frac{1}{K}\sum^K_{k=1}\frac{\hat{p}_1(x^{(k)})}{\hat{p}_0(x^{(k)})}$</p>
<p>当$KL(p_0||p_1)$较小的时候，可以有效估计出比值。但实际情况是 但实际情况中$p_1$通常是高维空间的复杂分布，很难找到一个简单的分布$p_0$既易于评估又接近$p_1$的分布，如果$p_0$与$p_1$不接近，那么$p_0$的大多数采样将在$p_1$中出现的概率较低，此时由于$\frac{\hat{p}_1(x^{(k)})}{\hat{p}_0(x^{(k)})}$很大，导致方差很大，导致估计结果很差（方差大，采样欠估计）。</p>
<p><strong>退火重要采样</strong>（Annealed Importance Sampling AIS）</p>
<p>目前是估计无向概率模型的配分函数的最常用方法</p>
<p>思想是引入中间分布来缩小$p_0$与$p_1$的差距</p>
<p>考虑分布序列$p_{\eta_0},\cdots,p_{\eta_n},0=\eta_0&lt;\eta_1&lt;\cdots&lt;\eta_{n-1}&lt;\eta_n=1$</p>
<p>$\therefore \frac{Z_1}{Z_0}=\frac{Z_1}{Z_0}\frac{Z_{\eta_1}}{Z_{\eta_1}}\cdots\frac{Z_{\eta_{n-1}}}{Z_{\eta_{n-1}}}$</p>
<p>$=\frac{Z_{\eta_{1}}}{Z_0}\frac{Z_{\eta_{2}}}{Z_{\eta_{1}}}\cdots\frac{Z_{\eta_{n-1}}}{Z_{\eta_{n-2}}}\frac{Z_{\eta_{1}}}{Z_{\eta_{n-1}}}$</p>
<p>$=\prod^{n-1}_{j=0}\frac{Z_{\eta_{j+1}}}{Z_{\eta_j}}$</p>
<p>只要保证每个中间$p$之间足够近，就可以用低方差的重要采样来求得每一个$\frac{Z_{\eta_{j+1}}}{Z_{\eta_j}}$，上式也就求解了。</p>
<p>通用的流行选择是用目标分布$p_1$的加权几何平均，$p_0$为$p_{\eta_j}∝p_1^{\eta_j}p_0^{1-\eta_j}$</p>
<p><strong>桥式采样</strong>（Bridge Sampling）</p>
<p>相比AIS，只使用一个中间分布$p$</p>
<p>最优的桥为$p_*^{(opt)}(x)∝\frac{\hat{p}_0(x)\hat{p}_1(x)}{r\hat{p}_0(x)+\hat{p}_1(x)}$</p>
<p>$r=\frac{Z_1}{Z_2}$</p>
<p>我们可以从粗糙的$r$开始迭代来估计最好的$r$</p>
<hr>
<p>本章各种五花八门的方法真是复杂，看得我眼花缭乱….</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>统计物理学</tag>
      </tags>
  </entry>
  <entry>
    <title>绽放之后</title>
    <url>/2019/10/17/%E7%BB%BD%E6%94%BE%E4%B9%8B%E5%90%8E/</url>
    <content><![CDATA[<div style="text-align: center"><i>如花火般短暂，如花火般绚烂</i></div>
<a id="more"></a>

<div align="MIDDLE">
<iframe src="//player.bilibili.com/player.html?aid=71516775&cid=123923582&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="564"> 
</iframe>
</div>

]]></content>
      <categories>
        <category>Anime</category>
      </categories>
      <tags>
        <tag>青春</tag>
        <tag>新海诚式</tag>
      </tags>
  </entry>
  <entry>
    <title>失眠飞行</title>
    <url>/2019/10/14/%E5%A4%B1%E7%9C%A0%E9%A3%9E%E8%A1%8C/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/10/14/失眠飞行/0.jpg" alt="0"></p>
<p><div style="text-align: center"><i>我想和你 一起闯进森林潜入海底</i></div></p>
<p><div style="text-align: center"><i>我想和你 一起看日出到日落天气</i></div></p>
<p><div style="text-align: center"><i>我想和你穿过格林威治和时间飞行</i></div></p>
<p><div style="text-align: center"><i>我想见你 穿过教堂和人海拥抱你</i></div></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1394847947&auto=0&height=66"></iframe>

]]></content>
      <categories>
        <category>Music</category>
      </categories>
      <tags>
        <tag>hanser</tag>
        <tag>kb</tag>
      </tags>
  </entry>
  <entry>
    <title>寒露</title>
    <url>/2019/10/13/%E5%AF%92%E9%9C%B2/</url>
    <content><![CDATA[<p>眼眸里流动着窗外的街景，打了一个寒战之后才意识到，深秋的冰冷正从自己倚靠在车窗上的头侧侵袭而来。</p>
<p>四面八方依稀传来的的轮胎与湿漉漉的地面的拍打声不绝于耳，淅淅沥沥的雨点与溅起的水花构成车外寂寥的白噪，在白茫茫的天际下，这座庞大的城市显得格外的孤独。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>秋</tag>
      </tags>
  </entry>
  <entry>
    <title>腾讯游戏策划培训课堂笔记-week1</title>
    <url>/2019/10/13/%E8%85%BE%E8%AE%AF%E6%B8%B8%E6%88%8F%E7%AD%96%E5%88%92%E5%9F%B9%E8%AE%AD%E8%AF%BE%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0-week1/</url>
    <content><![CDATA[<p><em>很幸运能通过选拔参与腾讯的策划培训，时间是每周日下午，除此之外也打算顺便玩玩虚幻引擎</em></p>
<p><strong>week 1 揭开游戏设计师的神秘面纱—开课典礼</strong></p>
<p>下周开始组队研发一款mini-game</p>
<h2 id="策划"><a href="#策划" class="headerlink" title="策划"></a>策划</h2><p>策划：制定游戏规则，怎么玩</p>
<ol>
<li><strong>想好</strong>要做什么样的游戏</li>
<li><strong>沟通</strong>获得各种制作资源</li>
<li><p><strong>组合</strong>资源成为想要的游戏</p>
<a id="more"></a>
</li>
</ol>
<h2 id="电子游戏研发流程综述"><a href="#电子游戏研发流程综述" class="headerlink" title="电子游戏研发流程综述"></a>电子游戏研发流程综述</h2><ol>
<li><p>提炼游戏创意</p>
<ul>
<li><p>语言简洁精炼</p>
</li>
<li><p>眼前一亮，令人深刻</p>
</li>
<li><p><strong>聚焦用户体验</strong></p>
</li>
</ul>
<p>方法：头脑风暴</p>
<p>然后分类整理，进行评估，初步筛选可行，进一步评估</p>
</li>
<li><p>设计游戏概念</p>
<p>基于游戏创意进行概念设计</p>
<p>可以组成游戏概念设计小组，1~2人</p>
<p>概念：平台，目标用户，类型，描述，玩法，卖点，竞品，美术概念等</p>
</li>
<li><p>制作纸面原型/物理原型</p>
<p>帮助思考，高效沟通，快速试错</p>
<p>纸面原型相比数字原型优点在于成本低，直观，效率高，便于试错</p>
</li>
<li><p>自己试玩原型</p>
<ul>
<li><p>设定规则集</p>
</li>
<li><p>确保运行良好</p>
<p>游戏逻辑合乎常理，所有游戏事件，结果，游戏挑战的平衡，游戏乐趣，顺序呈现</p>
</li>
<li><p>修正结构性问题</p>
</li>
</ul>
</li>
<li><p>模拟测试易用性和游戏性</p>
</li>
<li><p>数字化开发</p>
<ul>
<li><p>构建内容</p>
<p>将纸面原型的逻辑和内容进行数字化</p>
<p>构建内容不要超过游戏内容的20%</p>
<p>创建的游戏内容，未必会很好玩</p>
<p>少构建些内容，万一不好玩造成损失也能够让损失最小化</p>
</li>
<li><p>替代素材</p>
<p>图像，人物简单化</p>
<p>关卡素材足够简单</p>
</li>
<li><p>保证运作</p>
</li>
<li><p>团队协作</p>
<p>策划团队（枢纽），美术团队，程序团队，测试团队</p>
</li>
<li><p>版本管理</p>
<p>vs,SVN,git</p>
</li>
<li><p>需求管理</p>
<p>TAPD(腾讯敏捷项目管理平台)</p>
</li>
<li><p>游戏引擎是如何工作的</p>
<p>略</p>
</li>
</ul>
</li>
<li><p>数字测试易用性和游戏性</p>
<ul>
<li>半结构化测试</li>
<li>检测任务完成情况</li>
<li>收集测试数据</li>
</ul>
<p>技巧：开放式提问，提前设问，有声思维</p>
</li>
<li><p>修正游戏玩法和内容</p>
<ul>
<li>基于用户测试</li>
<li>关注系统问题</li>
<li>从大到小修正</li>
<li>完成第一个迭代，可以玩的数字版本</li>
</ul>
</li>
<li><p>创建下一个迭代</p>
<p>进入<strong>螺旋开发模型</strong> The Spiral Model（3~9循环）</p>
<p>持续迭代过程中，也在不断对游戏的玩法，美术进行迭代</p>
</li>
<li><p>打磨游戏</p>
<ul>
<li>打磨核心元素</li>
<li>打磨细节</li>
<li>提升美术表现品质（所有的UI，素材，人物，图像，场景，道具，动画效果等等）</li>
<li>检查文字</li>
</ul>
</li>
<li><p>继续打磨</p>
<ul>
<li>自己当新手试玩</li>
<li>找旁人试玩</li>
</ul>
</li>
<li><p>再三打磨，直到出类拔萃</p>
<ul>
<li>将自己当作某个别人</li>
<li>终于有一天，看吐了…</li>
</ul>
</li>
<li><p>完整测试</p>
<ul>
<li>真实用户</li>
<li>用户调研</li>
</ul>
</li>
<li><p>修正调优</p>
<ul>
<li>修正遗留漏洞</li>
<li>微调提升玩法，手感</li>
<li>do best</li>
</ul>
</li>
<li><p>版本发布（运营策划这个时候最忙）</p>
<ul>
<li>发布完整版本</li>
<li>版本跟踪</li>
</ul>
</li>
<li><p>打磨和打补丁</p>
</li>
</ol>
<h2 id="案例学习：尼山萨满（By-Next-Studio）"><a href="#案例学习：尼山萨满（By-Next-Studio）" class="headerlink" title="案例学习：尼山萨满（By Next Studio）"></a>案例学习：尼山萨满（By Next Studio）</h2><p>略</p>
<h2 id="电子游戏类型介绍-Video-Game-Genres"><a href="#电子游戏类型介绍-Video-Game-Genres" class="headerlink" title="电子游戏类型介绍(Video Game Genres)"></a>电子游戏类型介绍(Video Game Genres)</h2><p>Genres 来自法语</p>
<p>游戏类型，游戏流派</p>
<p><strong>基于游戏玩法交互的差异来分类</strong></p>
<p>不是基于视觉或叙事差异，不像电影或者小说按照题材内容来分类</p>
<p>类型的本质：<strong>世上本没有路，走的人多了，就成了路………</strong></p>
<p>分类并无统一标准，只是约定俗成方便沟通</p>
<p><strong>几大类：</strong></p>
<ul>
<li><p><strong>动作</strong> <em>Action</em></p>
<p>强调物理挑战，手眼协调，反应时间</p>
<p>细分：平台，射击，格斗，清版，生存</p>
</li>
<li><p><strong>冒险</strong> <em>Adventure</em></p>
<p>嵌入在叙事框架中的谜题，战斗和行动挑战有限</p>
<p>细分：文字冒险，视觉小说，图形冒险，互动电影，混合冒险</p>
</li>
<li><p><strong>动作冒险</strong> <em>Action-Adventures</em></p>
<p>物理挑战，手眼协调，反应时间 + 故事情节，众多角色，储存系统</p>
<p>细分：潜行，恐怖，类GTA，银河恶魔城，通常</p>
</li>
<li><p><strong>角色扮演</strong> <em>Role-Playing</em></p>
<p>玩家扮演一个或多个角色</p>
<p>沉浸在给定世界中</p>
<p>细分：动作，回合，大型多人在线，类rogue，战术，沙盒，地下城……</p>
</li>
<li><p><strong>模拟</strong> <em>Simulation</em></p>
<p>从现实生活中复制各种活动，用于各种目的</p>
<p>细分：经营，生活，交通工具</p>
</li>
<li><p><strong>策略</strong> <em>Strategy</em></p>
<p>强调思维技巧，计划性，针对1~N个对手，很少涉及物理挑战</p>
<p>细分：4X（eXplore,eXpand,eXploit,eXterminate)（如文明），炮术，即时战略，即时战术，多人在线战术竞技，塔防，回合制策略（TBS），回合制战略，战争，大型战略</p>
</li>
<li><p><strong>运动</strong> <em>Sports</em></p>
<p>细分：竞速，体育，竞技，体育格斗</p>
</li>
<li><p><strong>其他</strong> <em>Others</em></p>
<p>聚会，卡牌，益智，等等等等</p>
</li>
</ul>
<p><em>Tips:</em> </p>
<ol>
<li><p>文字冒险与视觉小说的区别</p>
<p>前者如BatMUD（鼻祖级MUD）</p>
<p>后者如一票日本Galgame（<em>演示：交响乐之雨</em>），是以各种多媒体为主，图片为主的</p>
</li>
<li><p>类银河恶魔城</p>
<p>突破了传统平台游戏的局限性，在传统平台游戏的基础上，任天堂的银河战士引入了非线性卷轴设计，KONOMI的恶魔城引入了RPG的部分系统，以此诞生的经典游戏类型。 <em>演示 : Cave Story By Pixel Studio</em></p>
</li>
<li><p>炮术游戏</p>
<p>俗称打炮游戏（误），如百战天虫WMD （2016 By Team17）</p>
</li>
</ol>
<hr>
<p>课后作业略</p>
<p>书籍推荐：<em>The Art of Game Design</em></p>
<p>《全景探秘 游戏设计艺术》（中文版）</p>
]]></content>
      <categories>
        <category>游戏设计</category>
      </categories>
      <tags>
        <tag>游戏设计</tag>
      </tags>
  </entry>
  <entry>
    <title>蒙特卡罗方法</title>
    <url>/2019/10/12/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="采样与蒙特卡罗方法"><a href="#采样与蒙特卡罗方法" class="headerlink" title="采样与蒙特卡罗方法"></a>采样与蒙特卡罗方法</h2><p>为什么要采样？比如假设我们要计算积分$\int_a^bh(x)dx$，我们需要枚举$x∈[a,b]$，这是非常困难的；又比如使用mini-batch进行模型训练的时候需要采样</p>
<p>蒙特卡罗采样的思想就是在这种很难进行枚举的时候，使用采样来近似它。这种想法<strong>把和或者积分视作某分布下的期望，然后通过估计对应的平均值来近似这个期望</strong>（也就是说把原函数$h(x)$分解成某个函数与概率密度函数$p(x)$的乘积，即$f(x)$在$p(x)$分布上的均值）</p>
<script type="math/tex; mode=display">
s = \int_a^bh(x)dx=\int_a^bp(x)f(x)dx=E_p(f(x))</script><p>从样本集中抽取$n$个样本$(x_1,\cdots,x_n)～p(x)$（同期望证明略，使用大数定理可证得）来近似之，则</p>
<script type="math/tex; mode=display">
\int_a^bh(x)dx=E_p(f(x))≈\frac{1}{n}\sum_{i=1}^nf(x_i)</script><p>蒙特卡罗方法是一种随机模拟技术</p>
<h2 id="马尔科夫链蒙特卡罗方法（MCMC）"><a href="#马尔科夫链蒙特卡罗方法（MCMC）" class="headerlink" title="马尔科夫链蒙特卡罗方法（MCMC）"></a>马尔科夫链蒙特卡罗方法（MCMC）</h2><p>Markov Chain Monte Carlo</p>
<p>马尔科夫链是<em>随机过程</em>中的重要角色，不多赘述</p>
<p>马尔科夫过程<strong>各态遍历性</strong>需要满足：①非周期  ②不可约</p>
<p>非周期即存在某个取值从它出发转移回自身所需要的转移次数总是整数$d(&gt;1)$的倍数，使得能够“连续”转移，这保证了马尔科夫过程的连续性，否则必须走特定步长（即周期）才能转移，使得马尔科夫过程不连续</p>
<p>不可约即为任意两个取值之间总是能以非零的概率相互转移</p>
<p>若马尔科夫过程是各态遍历的，无论初始值为何，随机变量的最终取值分布会收敛于一个唯一的平稳分布$\pi^*$</p>
<script type="math/tex; mode=display">
\lim_{t\to ∞ }\pi^{(0)}P^t=\pi^*</script><p><strong>意味着马尔科夫过程经过多次转以后，随机变量的分布会一直逼近该平稳分布</strong></p>
<p>可以从这个角度证明：</p>
<script type="math/tex; mode=display">
\pi_t=P^t\pi_0=(Pdiag(\lambda)P^{-1})^t\pi_0=Pdiag(\lambda)^tP^{-1}\pi_0</script><p>可见这个过程将导致$P$中不为1的特征值全部衰减到0，因此该过程就收敛到平稳分布</p>
<script type="math/tex; mode=display">
\pi^*=\pi P</script><p>得到一个特征向量方程，收敛之后的$\pi^*$是特征值为1所对应的特征向量</p>
<p>于是就可以<strong>利用马尔科夫链来进行蒙特卡罗估计</strong>，这类算法被称为<strong>马尔科夫链蒙特卡罗方法（MCMC）</strong></p>
 <a id="more"></a>
<p>由于要保证各态遍历性，于是MCMC方法<strong>最适用于基于能量的模型</strong>（见上一篇笔记）</p>
<p>马尔科夫链的<strong>磨合过程</strong>：运行马尔科夫过程直到收敛</p>
<p>马尔科夫过程中的<strong>混合时间</strong>：在未收敛前的那段时间</p>
<p>连续的马尔科夫链也叫<strong>哈里斯链</strong></p>
<p>难以预测马尔科夫链是否收敛，目前方法：</p>
<p>①看$P$的特征值是否趋近于0 （但通常$P^t$计算难度极大，难以表示）</p>
<p>②启发式方法（手动检查样本；衡量样本间的相关性）</p>
<p><strong>但注意一个问题：在一个马尔科夫链的一个抽样序列无法完全表达均衡分布！</strong></p>
<p>因为在一个马尔科夫链达到平稳状态后，两个连续样本之间会<strong>高度相关</strong>。（在马尔科夫过程平稳后，$P$使得每次转移使得样本的分布不变，但是$P$会改变每一次转移后的样本的形态，但同一时间下的样本形态是高度相关的）</p>
<p>解决办法之一是间隔抽样，另一个是使用<strong>多条马尔科夫链</strong>，每个样本从不同的马尔科夫链抽样，在深度学习中的通用实践是选取的马尔科夫链数目和小批量中的样本数相近。</p>
<h2 id="如何采样？（采样方法）"><a href="#如何采样？（采样方法）" class="headerlink" title="如何采样？（采样方法）"></a>如何采样？（采样方法）</h2><p>对于$uniform(0,1)$，我们可以使用<strong>线性同余发生器算法</strong>LCG：$R=(A*R+B)\%M$，推导略</p>
<p>对于常见的概率分布，可以使用<strong>逆采样</strong>方法，将其他概率分布<strong>映射到</strong>$uniform(0,1)$，推导略（大体思路是将CDF的反函数代入$uniform(0,1)$的概率密度函数）</p>
<p>这需要是常见概率分布，因为必须要求累计概率分布CDF可以求逆，所以</p>
<p>对于非常见概率分布，可以使用<strong>接受—拒绝采样</strong>，但对MCMC使用接受—拒绝采样效果并不好</p>
<p>（以上的采样方法可以以后单独写篇笔记）</p>
<p><strong>那MCMC该如何采样呢？</strong>我们需要采样目标概率分布$p(x)$，而目标概率分布是由转移概率决定的，所以也就是说我们要构造一个转移概率矩阵$P$，使得$p(x)$恰好是我们想要的目标概率分布</p>
<h2 id="Metropolis-Hasting采样"><a href="#Metropolis-Hasting采样" class="headerlink" title="Metropolis Hasting采样"></a>Metropolis Hasting采样</h2><p>为了满足能够构造一个转移概率矩阵$P$，使得$p(x)$<strong>恰好是我们想要的目标概率分布</strong>，我们设定马尔科夫链满足<strong>细致平稳条件</strong></p>
<script type="math/tex; mode=display">
\pi(i)P_{ij}=\pi(j)P_{ji}, \forall i,j</script><p>若满足，则马尔科夫链为平稳分布。</p>
<p>细致平稳条件可理解为从$i$状态转移到$j$状态的付出消耗与从$j$转移回$i$的吸收消耗相同，所以状态$i$上的概率质量$\pi(i)$是稳定的。</p>
<p>那么一般情况下</p>
<script type="math/tex; mode=display">
p(i)q(i,j)≠p(j)q(j,i)</script><p>$Q$为转移矩阵，$q$为转移概率。为了强行让它相等，即满足细致平稳条件，则设计一个$\alpha$（称作跳转的<strong>接受率</strong>）使得$\alpha(i,j)=p(j)q(j,i),\alpha(j,i)=p(i)q(i,j)$（对称性）</p>
<script type="math/tex; mode=display">
p(i)q(i,j)\alpha(i,j)=p(j)q(j,i)\alpha(j,i)</script><p>$\hat{Q}(i,j)=q(i,j)\alpha(i,j),\hat{Q}(j,i)=q(j,i)\alpha(j,i)$</p>
<p>$q$称为<strong>提议概率</strong>，$Q$称为提议转移矩阵，<strong>自己选择一种简单的分布即可</strong></p>
<p>于是原来的转移矩阵$Q$变成了现在可以保证细致平稳条件的转移矩阵$\hat{Q}$，此转移矩阵的平稳分布就是$p(x)$了</p>
<p>以上称为Metropolis抽样</p>
<p><strong>那么$\alpha$该如何更好地取值呢？</strong>太小的话会导致拒绝率太高，收敛速度太慢。发现如果在等式两边同时扩大相同的倍数，等式依然成立，于是想到将等式两边同比例放大使得最大的一边放大到1（概率最大为1），即</p>
<script type="math/tex; mode=display">
\alpha^*(i,j)=\min\{\frac{\alpha(j,i)}{\alpha(i,j)},1\}=\min\{\frac{p(j)q(j,i)}{p(i)q(i,j)},1\}</script><p>（这里的推导：当$\alpha(i,j)&gt;\alpha(j,i),则\alpha(i,j)先到1，\alpha^<em>(j,i)=\frac{1}{\alpha(i,j)}·\alpha(j,i)&lt;1$ <em>*缩放</em></em></p>
<p>当$\alpha(j,i)&gt;\alpha(i,j),则\alpha(j,i)先到1，\alpha^<em>(i,j)=\frac{\alpha(i,j)}{\alpha(j,i)}$ <em>*缩放</em></em></p>
<p>整合上两式记得上面的等式）</p>
<p><strong>于是</strong>$p(x)$是<strong>目标平稳概率分布，自设已知</strong>；$q(x)$为<strong>提议概率分布，自设已知</strong>，那么就可以求出<strong>转移概率</strong>$\alpha$了</p>
<p>从$uniform(0,1)$中采样$u$，如果$u&lt;\alpha(i,j)$，（相当于在拒绝采样中，随机采样点落入函数范围内），则该跳转成功，否则该跳转失败，以此迭代（这里使用<strong>接受-拒绝采样算法的思想</strong>，用提议转移矩阵$Q$去求得转移矩阵$\hat{Q}$）</p>
<p>这就是Metropolis Hasting采样算法</p>
<p>算法如下：</p>
<p><img src="//aisakaki.com/2019/10/12/蒙特卡罗方法/0.png" alt="0"></p>
<h2 id="Gibbs采样（深度学习中的最佳选择）"><a href="#Gibbs采样（深度学习中的最佳选择）" class="headerlink" title="Gibbs采样（深度学习中的最佳选择）"></a>Gibbs采样（深度学习中的最佳选择）</h2><p>针对高维的情形，发现在高维情况下自然成立一个式子使得细致平稳条件成立，也就可以不拒绝，使得收敛迅速。Gibbs采样每一步都只更新变量的一个小部分</p>
<p>对平面上任意两点$X,Y$</p>
<script type="math/tex; mode=display">
p(X)=Q(X,Y)=p(Y)Q(Y,X)</script><p><img src="//aisakaki.com/2019/10/12/蒙特卡罗方法/1.jpg" alt="0"></p>
<p>​    <img src="//aisakaki.com/2019/10/12/蒙特卡罗方法/2.jpg" alt="0"></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
      <tags>
        <tag>采样</tag>
        <tag>数学</tag>
        <tag>统计数学</tag>
        <tag>马尔科夫链</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习中的结构化概率模型</title>
    <url>/2019/10/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p><em>本文为 Deep Learning Ch.16 学习后整理的笔记</em></p>
<h2 id="结构化概率模型"><a href="#结构化概率模型" class="headerlink" title="结构化概率模型"></a>结构化概率模型</h2><p>概率图模型（PGM）描述概率有助于减少表示概率分布，学习和推断的成本。间接表示间接关系而不是直接表示</p>
<h2 id="有向图模型-信念网络-贝叶斯网络"><a href="#有向图模型-信念网络-贝叶斯网络" class="headerlink" title="有向图模型=信念网络=贝叶斯网络"></a>有向图模型=信念网络=贝叶斯网络</h2><p>方向仅表示依赖关系而不表明如何依赖</p>
<p>通过有向无环图和一系列<strong>局部条件概率分布</strong>定义</p>
<script type="math/tex; mode=display">
p(x)=\prod_ip(x_i|P(x_i))</script><p>其中$P(x_i)$表示结点$x_i$的所有父节点。</p>
<h2 id="无向图模型-马尔可夫随机场-马尔科夫网络"><a href="#无向图模型-马尔可夫随机场-马尔科夫网络" class="headerlink" title="无向图模型=马尔可夫随机场=马尔科夫网络"></a>无向图模型=马尔可夫随机场=马尔科夫网络</h2><p>对无向模型$M$中的一个团$C$称为一个<strong>因子</strong>$\phi(C)$，也叫<strong>团势能</strong>，<strong>势能函数</strong></p>
<p>它们一起定义了<strong>未归一化概率函数</strong>：</p>
<script type="math/tex; mode=display">
p(\hat{x})=\prod_{C∈M}\phi(C)</script><p>(图的一个团是图中结点的一个子集，并且其中的点是全连接的)</p>
<p>为了使得未归一化概率函数归一化，我们需要使用对应的归一化概率分布</p>
<script type="math/tex; mode=display">
p(x)=\frac{1}{Z}\hat{p(x)}</script><p>其中$Z$是使得所有概率之和或者积分为1的常数，且满足</p>
<script type="math/tex; mode=display">
Z=\int \hat{p}(x)dx</script><p>归一化常数$Z$被称为<strong>配分函数</strong>。计算$Z$需要对$x$的所有可能状态求联合概率分布，所以通常非常难计算，于是可以用一些近似方法求得。</p>
<p>但有时候积分是发散的，不存在一个$F(x)$，这时候$Z$就不存在</p>
<p>但注意如果指定发散函数的某定义域内是收敛的，那是OK的</p>
<p>无向图在<strong>表示上</strong>存在模糊性，即“多大的团？”，这时候可以引入<strong>因子图</strong>解决，它将随机变量绘为圆形，将未归一化概率函数因子$\phi$绘为方形，仅当随机变量对应该函数因子的时候存在连接，连接该函数的随机变量构成一个团</p>
<h2 id="基于能量的模型-Energy-Based-model-EBM"><a href="#基于能量的模型-Energy-Based-model-EBM" class="headerlink" title="基于能量的模型(Energy-Based model,EBM)"></a>基于能量的模型(Energy-Based model,EBM)</h2><p>无向模型依赖一个假设$\forall x,\hat{p}(x)&gt;0$，使得这个条件满足的一个简单方式是使用EBM</p>
<script type="math/tex; mode=display">
\hat{p}(x)=e^{-E(x)}</script><p>服从该形式的任意分布都是<strong>玻尔兹曼分布</strong>的一个实例，也因此我们把许多基于能量的模型成为<strong>玻尔兹曼机</strong></p>
<p>$E(x)$被称作<strong>能量函数</strong>(energy function)</p>
<p>由于$e^{(a+b)}=e^a·e^b$，于是可以发现无向模型中不同的团对应于能量函数的不同项，也就是说<strong>EBM只是一种特殊的马尔科夫网络</strong>，这种网络满足求幂使能量函数中的每项对应于不同团的一个影子。</p>
<p>比如对于一个MAP d-a-b-e,b-e,e-f(只写了连接关系)，通过为每个团选择适当的能量函数$E(a,b,c,d,e,f)$可以写成$E_{a,b}(a,b)+E_{b,c}(b,c)+E_{a,d}(a,d)+E_{b,e}(b,e)+E_{e,f}(e,f)$</p>
<p>能量函数可以视为<strong>专家之积</strong>，其中每一项看作决定一个特定的软约束是否满足的“专家”</p>
<p>公式中幂的负号只是为了保持机器学习文献与物理学文献之间的兼容性（能量无符号），对于机器学习来说可以自由决定符号</p>
<p>很多对概率模型进行操作的算法是计算$\log p(x)$的，于是对于具有潜变量$h$的EBM，这些算法有时会将该量的负数称为<strong>自由能</strong>(free energy)：</p>
<script type="math/tex; mode=display">
F(x)=-\log \sum_h e^{-E(x,h)}</script> <a id="more"></a>
<h2 id="分离与d-分离—条件独立"><a href="#分离与d-分离—条件独立" class="headerlink" title="分离与d-分离—条件独立"></a>分离与d-分离—条件独立</h2><p>无向图中隐含的条件独立性称为<strong>分离</strong>，有向图中则被称为<strong>d-分离</strong>。（d代表依赖）</p>
<p>如何判断哪些变量子集（d-）分离/彼此条件独立？</p>
<p> ①两个随机变量之间没有路径，或</p>
<p> ②两个随机变量之间<strong>所有路径</strong>都包含<strong>可观测变量</strong></p>
<p>举个例子：一个房子的好坏（C）是由大小（A）和装潢（B）决定的，此时的图结构为A-&gt;C,B-&gt;C，如果我们知道了这个房子是好房子，也就是C为可观测变量（条件），那么<strong>A和B是C条件下的独立随机变量</strong></p>
<p>记$a⊥b|c$表示给定$c$条件下$a$与$b$条件独立</p>
<blockquote>
<p>关于图模型条件独立的推导可以看这篇文章 <a href="https://my.oschina.net/liyangke/blog/2986515" target="_blank" rel="noopener">https://my.oschina.net/liyangke/blog/2986515</a></p>
</blockquote>
<h2 id="有向图和无向图的转化"><a href="#有向图和无向图的转化" class="headerlink" title="有向图和无向图的转化"></a>有向图和无向图的转化</h2><ol>
<li><p>有向模型和无向模型的一个重要区别在于有向模型是通过从起始点的概率分布直接定义的，而无向模型的定义显得更加宽松，通过$\phi$函数转化为概率分布定义</p>
</li>
<li><p>有向图可以使用<strong>不道德图结构</strong>而无向图不行</p>
<p>将有向模型图转换为无向模型图：①有向边变无向边   ②对于<strong>不道德</strong>（immorality)情况(a$\to$c，b$\to$c）需要在不道德的节点互相连线，变成<strong>道德图</strong>。由于道德化的过程中会加入很多边，因而会损失一些独立性条件</p>
</li>
<li><p>无向图可以使用<strong>环</strong>结构而有向图不行</p>
<p>将环结构的无向图转化为有向图需要添加<strong>弦</strong>（环序列中任意两个非连续变量的连接），转化后的图称为<strong>弦图</strong>或<strong>三角形化图</strong>。但如果存在长度大于3的环，则转化过来会损失独立性条件。（注意不能出现有向环，否则无法定义有效的有向概率模型）</p>
</li>
</ol>
<h2 id="从图模型中采样"><a href="#从图模型中采样" class="headerlink" title="从图模型中采样"></a>从图模型中采样</h2><ol>
<li><p>有向图：原始采样，根据拓扑排序顺序采样。</p>
<p>缺点：不是每次采样都是条件采样操作</p>
</li>
<li><p>无向图：①转化为有向图作原始采样</p>
<p>​                ②Gibbs采样</p>
</li>
</ol>
<hr>
<p>有已知值的变量被称为显变量，而值未被观察到的变量被称为隐变量</p>
<p>深度学习模型可以看作一类特殊的图概率模型，拥有大量的<strong>潜变量</strong>，且在设计的时候不表示任何特定含义；而传统模型大多使用高阶项和结构学习来捕获变量之间复杂的非线性作用，即使有潜变量，数量也通常很少，且会被赋予特定的含义</p>
<p><strong>潜变量=隐变量=未观测变量</strong></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>Autoencoder</title>
    <url>/2019/10/09/Autoencoder/</url>
    <content><![CDATA[<h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><p>AE可以看为一个单隐藏层的NN</p>
<p>目标函数为重构误差（reconstruction error)，目标是使得解码器的输出与编码器的输入同分布</p>
<p><strong>隐藏层单元数&lt;输入层单元数</strong>即可用于降维，如果编码的激活函数为线性函数就成为了<strong>PCA</strong></p>
<p>但现在一般<strong>隐藏层单元数&gt;=输入层单元数</strong>比较多，这样可以挖掘更多高维特征，但容易造成神经网络的输出为输入的直接“复制”，为了解决这个问题，引入了<strong>正则</strong>和<strong>随机性</strong>等等，且可以组合使用</p>
<h2 id="Regularized-Autoencoder"><a href="#Regularized-Autoencoder" class="headerlink" title="Regularized Autoencoder"></a>Regularized Autoencoder</h2><p>正则项自编码器</p>
<p>神经网络的输入直接“复制”输出的一个具体表现就在于一些神经元权重过大，使得神经网路直接变成直连通路</p>
<p>于是可以直接向损失函数正则项</p>
<script type="math/tex; mode=display">
L(x,g(f(x)))+\Omega(h)</script><p><strong>实现方法</strong>：使用传统的加入正则项实现方法</p>
<h2 id="Sparse-Autoencoder"><a href="#Sparse-Autoencoder" class="headerlink" title="Sparse Autoencoder"></a>Sparse Autoencoder</h2><p>稀疏自编码器</p>
<p>使得参数变得稀疏，避免过多神经元被激活</p>
<p>引入稀疏度公式</p>
<script type="math/tex; mode=display">
\hat{\rho}_j = \frac{1}{N} \Sigma_{i=1}^N h_j(x^{(i)})</script><p>此公式计算了第$j$个神经元在训练集$\{x^{i}\}$上激活度的平均值，那么就可以取一个足够小的$\rho$，保证$\hat{\rho}_j=\rho,j=1,2,3,\cdots$</p>
<p>对于$\hat{\rho}_j&gt;&gt;\rho$，对其使用KL散度进行惩罚，KL散度可以用于衡量两个分布的相似度（在前面这篇论文的阅读笔记：<a href="https://aisakaki.com/2019/07/30/Examples/">Explaining and Harnessing Adversarial Examples</a>中使用到了这个数学工具。</p>
<p>向损失函数中加入KL散度</p>
<script type="math/tex; mode=display">
J=L(x,g(f(x)))+\Sigma_{j=1}^mKL(\hat{\rho}_j||\rho)</script><script type="math/tex; mode=display">
KL(\hat{\rho}_j||\rho)=\rho·\log \frac{\rho}{\hat{\rho}_j}+(1-\rho)·\log \frac{1-\rho}{1-\hat{\rho}_j}</script><p><strong>实现方法</strong>：可以设置一个三维矩阵，第一维为$||\{x^{(i)}\}||$，第二和第三维对应神经网络每一个神经元。然后在数据流经一次模型后抽取出每一层神经元的参数，作平均。抽取方法可以用我以前写的<a href="https://aisakaki.com/2019/07/10/pytorch获取中间层参数、输出与可视化/">pytorch获取中间层参数、输出与可视化/</a>一文中提到的中间层参数可视化方法</p>
<h2 id="Denoising-Autoencoder"><a href="#Denoising-Autoencoder" class="headerlink" title="Denoising Autoencoder"></a>Denoising Autoencoder</h2><p>降噪自编码器（DAE）</p>
<p>向输出加入噪声$\hat{x}$，并设置损失函数尽量减少输出的噪声，目标使得模型学习<strong>重构分布</strong>$p_{decoder}(x|\hat{x})$，增强模型健壮性</p>
<p>最小化</p>
<script type="math/tex; mode=display">
L(x,g(f(\hat{x})))</script><p>也就是要<strong>使得正确输入与加噪声的输入经过模型后还原得到的输出尽量一样</strong></p>
<p><strong>实现方法：</strong>加入噪声的方法可以有多种，比如增加高斯分布随机扰动$x\to x+\epsilon,\epsilon ～ N(μ,\sigma^2)$，也可以使用掩码方法遮蔽部分特征</p>
<blockquote>
<p>补充：得分匹配是最大似然的代替。它提供了概率分布的一致性估计，使得模型在各个数据点$x$上获得与数据分布相同的得分，在这种情况下，得分是一个特定的梯度场：</p>
</blockquote>
<script type="math/tex; mode=display">
\nabla_x\log p(x)</script><p>DAE的训练目标可以使得AE学到能估计数据分布得分的向量场$(g(f(x))-x)$</p>
<h2 id="Contractive-Autoencoder"><a href="#Contractive-Autoencoder" class="headerlink" title="Contractive Autoencoder"></a>Contractive Autoencoder</h2><p>收缩自编码器(CAE)</p>
<p>是正则自编码器的一个变形，将正则项设置为惩罚导数——隐藏层函数对输入求导，也即其Jacobi矩阵$J_h$</p>
<script type="math/tex; mode=display">
J=L(x,g(f(x)))+\lambda\Sigma_i||\nabla_xh_i||_F^2</script><p>注意这里是矩阵的F-范数，即矩阵每项平方求和</p>
<p>可以看出收缩惩罚目标是使得学习到的参数在所有方向上不变，而是在整体上进行参数的缩放，其使得函数梯度很小，所以能够很好的学习到流形结构，（而使得$J_x$很大的方向$x$，会快速改变$h$，则很可能是近似流形切平面的方向）</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title>行って、振り向かないで</title>
    <url>/2019/10/03/%E4%B8%8D%E8%A6%81%E5%9B%9E%E5%A4%B4%EF%BC%8C%E4%B8%80%E7%9B%B4%E5%90%91%E5%89%8D/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/10/03/不要回头，一直向前/0.png" alt="Snipaste_2019-10-04_08-27-03"></p>
<p><div style="text-align: center"><i>“不要回头，一直向前 ”</i></div></p>
<p><div style="text-align: right"><i>——《千与千寻的神隐》</i></div></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=578090&auto=0&height=66"></iframe>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>另一种提取中间层输出的办法与风格迁移的改进</title>
    <url>/2019/09/30/%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%8F%90%E5%8F%96%E4%B8%AD%E9%97%B4%E5%B1%82%E8%BE%93%E5%87%BA%E7%9A%84%E5%8A%9E%E6%B3%95%E4%B8%8E%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%9A%84%E6%94%B9%E8%BF%9B/</url>
    <content><![CDATA[<h2 id="提取中间层输出"><a href="#提取中间层输出" class="headerlink" title="提取中间层输出"></a>提取中间层输出</h2><p>除了在我前面的文章<a href="https://aisakaki.com/2019/07/10/pytorch获取中间层参数、输出与可视化/">pytorch获取中间层参数、输出与可视化</a></p>
<p>提到的使用hook函数获取中间层输出以外，还可以用更直接的办法，不使用hook函数</p>
<p><strong>方法一：将model中的每层拆解出来依次在监控下让数据流过</strong><br>可以借助<code>isinstance(layer, nn.Conv2d)</code>判断层类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_layers</span><span class="params">(layers,img,model)</span>:</span></span><br><span class="line">    layer_index = <span class="number">0</span></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> model:</span><br><span class="line">        <span class="keyword">if</span> layer_index == <span class="number">0</span>:</span><br><span class="line">            out = layer(img)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out = layer(out)</span><br><span class="line">        <span class="keyword">if</span> layer_index <span class="keyword">in</span> layers:</span><br><span class="line">            res.append(out)</span><br><span class="line">        layer_index += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p><strong>方法二：直接在定义模型的时候输出中间层数据</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">32</span>,<span class="number">16</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></span><br><span class="line">        linear1_out = self.linear1(input)</span><br><span class="line">        linear2_out = self.linear2(linear1_out)</span><br><span class="line">        <span class="keyword">return</span> linear1_out,linear2_out</span><br></pre></td></tr></table></figure>
<h2 id="对于噪点的改进-降噪"><a href="#对于噪点的改进-降噪" class="headerlink" title="对于噪点的改进-降噪"></a>对于噪点的改进-降噪</h2><p>修改方程，向方程中加入总变差损失(total variation denoising)</p>
<script type="math/tex; mode=display">
L_{total}(S,C,G) = \alpha L_{content}(C,G)+\beta L_{style} (S,G)+\gamma L_{noise}(C,G)</script><script type="math/tex; mode=display">
L_{noise}(C,G)=\Sigma_{i,j}|x_{i,j}-x_{i+1,j}|+|x_{i,j}-x_{i,j+1}|</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TotalVariationLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,weight)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = weight</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,inputs)</span>:</span></span><br><span class="line">        <span class="comment">#矩阵计算，一步到位</span></span><br><span class="line">        loss = <span class="number">0.5</span>*((inputs[:, :, <span class="number">1</span>:, :]-inputs[:, :, :<span class="number">-1</span>, :]).abs().mean()+(inputs[:, :, :, <span class="number">1</span>:]-inputs[:, :, :, :<span class="number">-1</span>]).abs().mean())</span><br><span class="line">        <span class="keyword">return</span> loss * self.weight</span><br></pre></td></tr></table></figure>
<p> 在<code>loss function</code>中对应作修改</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#总变差损失计算</span></span><br><span class="line">    noise_loss = TotalVariationLoss(noise_weight)</span><br><span class="line">    noise_loss_this = noise_loss(input_param)</span><br><span class="line">    </span><br><span class="line">    loss = style_loss_this + content_loss_this + noise_loss_this</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="风格迁移的其它损失函数优化算法"><a href="#风格迁移的其它损失函数优化算法" class="headerlink" title="风格迁移的其它损失函数优化算法"></a>风格迁移的其它损失函数优化算法</h2><p>我最先采用的LBFGS算法是因为原论文是使用的该算法，使用LBFGS的优点在于能够快速收敛，但缺点也很明显，非常耗显存，图片大一点的话泰坦的12G显存迅速被耗尽</p>
<p>另外以下为两种优化算法对同样风格层迁移效果的对比</p>
<p>原图：<br><img src="//aisakaki.com/2019/09/30/另一种提取中间层输出的办法与风格迁移的改进/原图.jpg" alt="原图"><br>风格图：<img src="//aisakaki.com/2019/09/30/另一种提取中间层输出的办法与风格迁移的改进/style.png" alt="style"></p>
<p>LBFGS：<br><img src="//aisakaki.com/2019/09/30/另一种提取中间层输出的办法与风格迁移的改进/result_LBFGS.jpg" alt="result_LBFGS"><br>Adam：<br><img src="//aisakaki.com/2019/09/30/另一种提取中间层输出的办法与风格迁移的改进/result3_Adam_10020.jpg" alt="result3_Adam_10020"></p>
<p>Denoise Adam：</p>
<p>加入降噪训练之后效果好很多</p>
<p><img src="//aisakaki.com/2019/09/30/另一种提取中间层输出的办法与风格迁移的改进/result_Adam_TVDnoise.jpg" alt="result_Adam_TVDnoise"></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>风格迁移</tag>
      </tags>
  </entry>
  <entry>
    <title>拟牛顿法</title>
    <url>/2019/09/26/%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
    <content><![CDATA[<h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>牛顿法中最困难的部分在于计算$H$和$H^{-1}$</p>
<p>拟牛顿法的思想就是不用二阶偏导数而构造出可以<strong>近似</strong>$H$和$H^{-1}$的正定对称矩阵，在拟牛顿的条件下优化目标函数。但是不可能随便一个矩阵都能近似$H$，所以我们要给近似矩阵开一个条件，也就是下面的拟牛顿条件</p>
<h2 id="拟牛顿条件"><a href="#拟牛顿条件" class="headerlink" title="拟牛顿条件"></a>拟牛顿条件</h2><p>由上一篇我写的文章可知（依然省略掉高阶导数无穷项，实际上应该用$≈$，为了方便这里都用$=$了</p>
<script type="math/tex; mode=display">
\nabla f(x)=g_{k+1}+H_{k+1}(x-x_{k+1})</script><p>设$x=x_k$</p>
<p>记$s_k=x_{k+1}-x_k,y_k=g_{k+1}-g_k$</p>
<p>$\therefore y_k=H_{k+1}\cdot s_k$</p>
<p>或$s_k=H^{-1}_{k+1}\cdot y_k$</p>
<p>此即为拟牛顿条件</p>
<p>选择合适的$B_{k+1}$做$H_{k+1}$的近似，合适的$D_{K+1}$做$H^{-1}_{k+1}$的近似，使他们满足拟牛顿条件即可</p>
<p>相当于给拟合$H，H^{-1}$做了一个约束，使得在$f(x_k)与f(x_{k+1})$拟合矩阵与真实矩阵的一阶导相等</p>
<p>以下不同的算法实际上就是<strong>设计拟合矩阵</strong>的算法</p>
<h2 id="DFP算法"><a href="#DFP算法" class="headerlink" title="DFP算法"></a>DFP算法</h2><p>DFP算法通过迭代的方法对$D_{K+1}$做$H^{-1}_{k+1}$的近似</p>
<p>主要设置$D_k$待定形式为</p>
<script type="math/tex; mode=display">
\Delta D_k=\alpha uu^T+\beta vv^T</script><p>推导过程类似BFGS算法，略，见下</p>
<script type="math/tex; mode=display">
D_{k+1}=D_{k}+\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k},k=0,1,2,\cdots</script><p>一般$D_0=I$</p>
<h2 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h2><p>BFGS算法通过迭代的方法对$B_{K+1}$做$H_{k+1}$的近似，直接逼近Hessian矩阵</p>
<p>采用迭代法</p>
<script type="math/tex; mode=display">
B_{k+1}=B_k+\Delta B_k,k=0,1,2,\cdots</script><p>一般$B_0=I$</p>
<p>设置$B_k$的待定形式</p>
<script type="math/tex; mode=display">
\Delta B_k=\alpha uu^T+\beta vv^T</script><p>$uu^T$和$vv^T$正好构成对称矩阵</p>
<p>代入$y_k=H_{k+1}\cdot s_k$中（$B_{k+1}$逼近$H_{k+1}$）得</p>
<script type="math/tex; mode=display">
y_k=(B_k+\alpha uu^T+\beta vv^T)\cdot s_k</script><script type="math/tex; mode=display">
=B_ks_k+\alpha u^Ts_k\cdot u+\beta v^ts_k\cdot v</script><p>$u^Ts_k$和$v^Ts_k$都是常数，不妨令$\alpha u^Ts_k=1,\beta v^ts_k=-1,u=y_k,v=B_ks_k$,</p>
<script type="math/tex; mode=display">
\therefore \alpha=\frac{1}{y_k^Ts_k},\beta=-\frac{1}{s_k^TB_ks_k}</script><script type="math/tex; mode=display">
\therefore \Delta B_k=\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}</script><p>但由于必须储存$D$，使得储存开销很大，不适用大数据大型模型</p>
<h2 id="LBFGS算法"><a href="#LBFGS算法" class="headerlink" title="LBFGS算法"></a>LBFGS算法</h2><p>通过避免储存完整的Hessian逆运算近似$D$,降低BFGS的储存代价</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>牛顿法求解非线性优化</title>
    <url>/2019/09/26/%E7%89%9B%E9%A1%BF%E6%B3%95%E6%B1%82%E8%A7%A3%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p><em>前面在风格迁移的时候用到了LBFGS，很好奇只需要设置一个闭包迭代即可，不需要像动量算法等一阶优化算法一样设置步长学习率等，于是系统地去了解了一下牛顿法与拟牛顿法</em></p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>首先回想起泰勒展开，此式子即为$f(x)$在$x_k$附近的二阶泰勒展开式</p>
<script type="math/tex; mode=display">
\phi(x) =f(x_k)+f'(x_k)(x-x_k)+\frac{1}{2}f''(x_k)(x-x_k)^2+O</script><p>令导数为0即可求得最值</p>
<script type="math/tex; mode=display">
\phi'(x)=0</script><p>省略掉高阶项，则</p>
<script type="math/tex; mode=display">
f'(x_k)+f''(x_k)(x-x_k)=0</script><p>$\therefore$</p>
<script type="math/tex; mode=display">
x = x_k-\frac{f'(x_k)}{f''(x_k)},k=0,1,2,···</script><p>于是我们就可以从初始$x_0$值开始用此式进行迭代来逼近极小值点，其逼近就是<strong>在每次迭代的时候直接跳到近似函数的最小点</strong></p>
<p>那么对于高维度的情形，原泰勒公式就可以表示为</p>
<script type="math/tex; mode=display">
\phi(X) = \nabla f(X_k)·(X-X_k)+\frac{1}{2}(X-X_k)^T·\nabla^2f(X_k)·(X-X_k)</script><p>这里的$\nabla^2 f(X_k)$即为Hessian矩阵，其实就是二阶梯度</p>
<script type="math/tex; mode=display">
\nabla^2 f(X_k)=
\begin{bmatrix}
{\frac{\partial^2 f}{\partial x_1^2}}&{\frac{\partial^2 f}{\partial x_1\partial x_2}}&{\cdots}&{\frac{\partial^2 f}{\partial x_1\partial x_n}}\\
{\frac{\partial^2 f}{\partial x_2\partial x_1}}&{\frac{\partial^2 f}{\partial x_2^2}}&{\cdots}&{\frac{\partial^2 f}{\partial x_2\partial x_n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{\frac{\partial^2 f}{\partial x_n\partial x_1}}&{\frac{\partial^2 f}{\partial x_2^2}}&{\cdots}&{\frac{\partial^2 f}{\partial x_n\partial x_n}}\\
\end{bmatrix}</script><p>$\nabla f(X_k)$就是一阶梯度，不展开了</p>
<p>因为Hessian矩阵是实对称矩阵，我们可以将其分解为一组实特征值和一组特征向量的正交基。在特定方向$d$的二阶导数可以写成$d^THd$。</p>
<p>当$d$是$H$的一个特征向量的时候，这个方向的二阶导数就是对应的特征值。对于其他方向$d$，方向二阶导数是所有特征值的加权平均，权重在0和1之间，且与$d$夹角越小的特征值权重越大。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。</p>
<p>记$g_k=\nabla f(X_k)$,$H_k=\nabla^2 f(X_k)$，</p>
<script type="math/tex; mode=display">
\nabla \phi(X)=0</script><p>则</p>
<script type="math/tex; mode=display">
g_k+H_k(X-X_k)=0</script><p>$H_k$必为非奇异矩阵（满秩矩阵）</p>
<p>$\therefore$</p>
<script type="math/tex; mode=display">
X = X_k-H_k^{-1}·g_k ,k=0,1,2,\cdots</script><p>此即为牛顿法的迭代式，$d_k=-H_k^{-1}·g_k$称为<strong>牛顿方向</strong></p>
<p>牛顿法求出的方向是一个逼近，因为其省略了泰勒的高阶展开项（但如果是二次函数，那就不是逼近了，这时候Hessian矩阵退化为常数矩阵，只需要一次运算即可到达最优点）</p>
<h2 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h2><ol>
<li><p>使用牛顿法可以比一阶优化算法更快的收敛，因为每次它都是直接跳到近似函数的最小点（对于二次函数，直接一步收敛），但是这种特性在<strong>鞍点</strong>附近会造成病态。</p>
</li>
<li><p>会造成<strong>局部收敛</strong></p>
</li>
<li><p>Hessian矩阵的计算量是其它一阶优化方法的平方倍，所以对硬件计算负担要求极高</p>
</li>
<li><p>由于步长是定的，所以牛顿方向$-H_k^{-1}·g_k$并不能保证稳定下降，其并不是梯度下降算法</p>
</li>
<li><p>函数必须二阶可导，Hessian矩阵必须为正定矩阵</p>
</li>
</ol>
<p>   <em>深度学习通常都有很多局部极值，鞍点，且计算量巨大，所以很少在深度学习中使用牛顿法</em></p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ol>
<li><p>针对第四个缺陷，可以对其进行改进，加入<strong>线搜索</strong>，称作<strong>阻尼牛顿法</strong>，这个思想和动量算法思想一样</p>
<p>仍然用$d_k$在方向上进行迭代，但是每次迭代的时候需要进行一次线搜索，寻求最优步长因子$\lambda_k $</p>
<p>$d_k\to\lambda_k d_k$</p>
<p>$\lambda_k=arg\min f(x_k+\lambda_k d_k)$</p>
</li>
<li><p>可以通过多种手段避免直接对Hessian矩阵求逆</p>
<p>比如共轭梯度法（PCG），代数多重网格法（AMG）等</p>
<p>补充PCG思想：结合梯度下降与牛顿法，在一个方向上用牛顿法，一次性迭代完，理论上N个方向N次即可收敛</p>
</li>
<li><p>拟牛顿法</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
      <tags>
        <tag>非线性优化</tag>
      </tags>
  </entry>
  <entry>
    <title>骚动时节的少女们啊</title>
    <url>/2019/09/25/%E9%AA%9A%E5%8A%A8%E6%97%B6%E8%8A%82%E7%9A%84%E5%B0%91%E5%A5%B3%E4%BB%AC%E5%95%8A/</url>
    <content><![CDATA[<p>应该是老马失蹄的最惨的一次</p>
<p>没想到后半段开始一路血崩…</p>
<p>这就是部青春期性教育重要性的宣传片.. 前八集非常不错，冈妈将细腻与敏感的初春体现的淋漓尽致</p>
<p>但是第八集之后，至少我是理解不能…… </p>
<p>冈妈把关于性和爱的模糊不清的这种情感无限夸大 ，然后非常魔幻地表达出来，造成一种非常尴尬的效果，把非常抽象的东西用现实的手段生硬地表现出来造成了极强的违和感</p>
<p>还有白毛和矮子这俩能不能删了orz</p>
<p>前8集9分 ，后几集2分 ，加权一下，6.7分</p>
]]></content>
      <categories>
        <category>Anime</category>
      </categories>
      <tags>
        <tag>新番</tag>
      </tags>
  </entry>
  <entry>
    <title>IOS开发小记</title>
    <url>/2019/09/25/XCODE%E5%BC%80%E5%8F%91%E5%B0%8F%E8%AE%B0/</url>
    <content><![CDATA[<p>最近老师想要给一个去中心链的IM开发几个功能，安卓端有人做，但我们实验室没人开发过IOS，老师想起我面试的时候提到过我大二做过IOS开发，就让我这个月搞出来，然后…</p>
<p>不愧是世界上最难用的IDE，回想起了大二被XCODE支配的恐惧</p>
<p>因为当时没用pods管理第三方库…以下踩坑记录</p>
<ol>
<li><p>使用CocoaPod管理依赖的项目，XCode只能使用workspace编译项目，如果还只打开以前的xcodeproj文件进行开发，编译会失败。新增的workspace文件会引用原有的应用主工程，还会引用新增的Pods工程</p>
</li>
<li><p>如果   PODS生成的静态链接库没有被主工程target链接，要自己手动建立链接</p>
</li>
<li><p>在头文件搜索路径里 xxx/xxx/<em> re… 别忘了加\</em>并设置迭代搜索</p>
</li>
<li><p>build后修改代码，需要build clean再重新build，不然会报上次一样的错</p>
</li>
<li><p>要检查头文件路径是否正确<xxx xxx="">，或者直接写”.h”，则xcode会直接在头文件搜索路径里搜索</xxx></p>
</li>
<li><p>mac自带的ruby权限较小，开发不要用这个。我们要自己安装rvm并创建ruby环境，随后我们的cocoapods是在这个环境使用的</p>
</li>
<li><p>respondsToSelector 用来判断某一个方法时候实现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#如果destroy这个方法实现了，就执行之</span><br><span class="line">if ([subViewController respondsToSelector:@selector(destroy)])</span><br><span class="line">&#123;</span><br><span class="line">	[subViewController destroy];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>UINavigationController 导航控制器</p>
<p>开发的时候一般都是多控制器，由一个父控制器管理多个子控制器</p>
<p>一般都是在Appdelegate中生成（可以将Nav写在一个VC中，然后在Appdelegate中实例化这个VC）</p>
<p>使用方法：</p>
<ul>
<li><p>初始化UINavigationController实例</p>
</li>
<li><p>在AppDelegate中设置一个根控制器，即设置<code>self.window.rootViewController=UINavigationController</code>（在AppDelegate中设置,self指的appdelegate）</p>
</li>
<li><p>通过push方法添加需要的子控制器</p>
<p>在初始化UINavagationController的时候调用构造方法<code>initWithRootViewController:</code>就会自动push自己设定的根控制器，但其它view就要创建后手动push</p>
</li>
</ul>
<p>当前view所属控制器<code>self.navigationController</code></p>
<p>控制器跳转：<code>[self.navigationController pushViewController:xxxViewController animated:YES]</code></p>
<p>压入(push)控制器：<code>-(void)pushViewController:(UIViewController *)viewController animated:(BOOL)animated;</code></p>
<p>移除(pop)栈顶控制器:<code>-(UIViewController *)popViewControllerAnimated:(BOOL)animated;</code>即可返回上一层控制器</p>
<p>回到(popTo)指定子控制器:<code>-(NSArray *)popToViewController:(UIViewController *)viewController animated:(BOOL)animated;</code></p>
<p>回到根(popToRoot)控制器（栈底）:<code>-(NSArray *)popToRootViewControllerAnimated:(BOOL)Animated;</code></p>
<p>一次压入多个控制器<code>setViewController</code></p>
<p>栈顶控制器<code>nav.topViewController</code></p>
<p>当前可见视图控制器<code>nav.visibleViewController</code></p>
</li>
<li><p>ViewController相关</p>
<p>根据名字获取VC：<code>_homeViewController = self.viewControllers[TABBAR_HOME_INDEX];</code></p>
</li>
<li><p>账户登入状态等信息可以用单例模式来管理</p>
</li>
<li><p>AppDelegate负责管理程序周期，是程序入口</p>
<p>如<code>-(BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions</code></p>
<p>这里就是程序launch完后做的事情，这时候就可以初始化Root VC了</p>
</li>
<li><p>协议 接口 委托</p>
<p>委托：A委托B去做某事，B如果没有这个方法，B从C继承，C有这个方法，就会执行C中的该方法（与java一样的向上转型）</p>
<ul>
<li>A是B的委托对象</li>
<li>B引用一个A</li>
<li>A将实现B的委托方法</li>
<li>B通过委托方法通知</li>
</ul>
<p>接口：接口就相当于一个头文件，告诉外部可以用，联想生物细胞的构造</p>
<p>在h文件里写@interfacae，在m文件里实现该interface，就相当于建立了一个类，只有interface暴露的东西才能被外部使用，这种设计是为了更好的安全性</p>
<p>java类和oc类的核心区别在于，java类的共有私用是由类里面自己实现的，可以设置共私有变量和方法，而OC中每个类都是由一个接口interface和实现该interface的类来完成，只有interface中的东西暴露给了外部</p>
<pre><code> 协议：协议可以类比java中的interface
</code></pre></li>
<li><p><code>Universal Link</code>, 中文是通用链接. 可以通过<code>http(s)</code>来唤醒<code>App</code></p>
</li>
<li><p>prepareForSegue:sender: </p>
<p>为了区分视图的跳转，可以用上一个、下一个来表示，也可以用源视图、目标视图来表示。 即： sourceViewController 和destinationViewController。  目标视图控制器是指：即将显示（加载）的视图， 而源视图控制器是指：即将被取代的视图控制器。</p>
</li>
<li><p>跳转总结</p>
<p><a href="https://www.jianshu.com/p/ceaf978f9dfe" target="_blank" rel="noopener">https://www.jianshu.com/p/ceaf978f9dfe</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>开发</category>
        <category>IOS开发</category>
      </categories>
      <tags>
        <tag>IOS</tag>
        <tag>XCODE</tag>
      </tags>
  </entry>
  <entry>
    <title>神经风格迁移</title>
    <url>/2019/09/21/%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<h2 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h2><script type="math/tex; mode=display">
L_{total}(S,C,G) = \alpha L_{content}(C,G)+\beta L_{style} (S,G)</script><p>内容图像（C）,生成的图像（G ）,样式图像（S）</p>
<p>合成图像为唯一需要学习（更新）的参数（视为模型参数），也是生成的结果，而不是VGG</p>
<p>输入：C</p>
<p>初始化：可以选择让G=C，也可以随机初始化一个G</p>
<p>这个算法里面的参数(也就是是合成图片里面的每个像素点，我们可以将内容图片直接 copy 成合成图片，<strong>然后训练使得他的风格和我们的风格图片相似，同时也可以随机化一张图片作为合成图片（两种初始化）</strong>，然后训练他使得他与内容图片以及风格图片具有相似性。特征的提取</p>
<h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><p>这里不训练模型，直接使用预训练的vgg来提取特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    vgg = models.vgg19(pretrained=<span class="keyword">True</span>).features</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> vgg.parameters():</span><br><span class="line">        param.requires_grad = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">return</span> vgg</span><br></pre></td></tr></table></figure>
<h2 id="extract-loss"><a href="#extract-loss" class="headerlink" title="extract loss"></a>extract loss</h2><p>这里使用以前用过的register_forward_hook()函数来hook</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerActivations</span><span class="params">()</span>:</span></span><br><span class="line">    features = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,model,layer_nums)</span>:</span></span><br><span class="line">        <span class="comment">#这里要hook多层，要保存多层钩子</span></span><br><span class="line">        self.hooks = []</span><br><span class="line">        <span class="keyword">for</span> layer_num <span class="keyword">in</span> layer_nums:</span><br><span class="line">            self.hooks.append(model[layer_num].register_forward_hook(self.hook_fn))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hook_fn</span><span class="params">(self,module,input,output)</span>:</span></span><br><span class="line">        self.features.append(output)</span><br><span class="line">    <span class="comment">#捕捉完输出后不能忘掉remove方法，否则所有输入都累加在一起会内存溢出 </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> hook <span class="keyword">in</span> self.hooks:</span><br><span class="line">            hook.remove()</span><br><span class="line"></span><br><span class="line"><span class="comment">#该函数用于将图片输入进模型，经过钩子获取指定多层的特征输出      </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_layers</span><span class="params">(layers,img,model)</span>:</span></span><br><span class="line">    la = LayerActivations(model=model,layer_nums=layers)</span><br><span class="line">    <span class="comment">#清空缓存</span></span><br><span class="line">    la.features=[]</span><br><span class="line">    <span class="comment">#运行模型，开钩</span></span><br><span class="line">    out = model(img)       </span><br><span class="line">    <span class="comment">#已经获取到特征，这是我们关注的东西，然后注销钩子</span></span><br><span class="line">    la.remove()</span><br><span class="line">    <span class="comment">#注意这里返回的是列表，一次性钩了多层</span></span><br><span class="line">    <span class="keyword">return</span> la.features</span><br></pre></td></tr></table></figure>
<h2 id="定义损失"><a href="#定义损失" class="headerlink" title="定义损失"></a>定义损失</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># content loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContentLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,weight)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.mseloss = nn.MSELoss()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,inputs,targets)</span>:</span></span><br><span class="line">        out = self.mseloss(inputs,targets)</span><br><span class="line">        <span class="keyword">return</span> out * self.weight</span><br></pre></td></tr></table></figure>
<p>衡量风格损失使用的是Gram Matrix，对于$k$个向量$\alpha_1,\alpha_2,\cdots,\alpha_k$</p>
<script type="math/tex; mode=display">
G=
\begin{bmatrix}
{(\alpha_1,\alpha_1)}&{(\alpha_1,\alpha_2)}&{\cdots}&{(\alpha_1,\alpha_k)}\\
{(\alpha_2,\alpha_1)}&{(\alpha_2,\alpha_2)}&{\cdots}&{(\alpha_2,\alpha_k)}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{(\alpha_k,\alpha_1)}&{(\alpha_k,\alpha_2)}&{\cdots}&{(\alpha_k,\alpha_k)}\\
\end{bmatrix}</script><p>把特征提取输出变换为$k$行$h*w$列的矩阵$X$，那么$X=&lt;\alpha_1,\alpha_2,\cdots,\alpha_k&gt;$，其中向量$x_i$代表了通道$i$上的样式特征，于是Gram矩阵实际上计算出了各个通道的两两相关性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># style loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GramMatrix</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,inputs)</span>:</span></span><br><span class="line">        b,c,h,w = inputs.size()</span><br><span class="line">        <span class="comment">#hw维度扁平化</span></span><br><span class="line">        features = inputs.view(b,c,h*w)</span><br><span class="line">        <span class="comment">#bmm(A) =  AxA^T</span></span><br><span class="line">        gram_matrix = torch.bmm(features,features.transpose(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#为防止值过大，做归一化</span></span><br><span class="line">        gram_matrix.div_(h*w)</span><br><span class="line">        <span class="keyword">return</span> gram_matrix</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StyleLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,weight)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.gramfunc = GramMatrix()</span><br><span class="line">        self.mseloss = nn.MSELoss()</span><br><span class="line">        self.weight = weight</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,inputs,targets)</span>:</span></span><br><span class="line">        gram_inputs = self.gramfunc(inputs)</span><br><span class="line">        gram_targets = self.gramfunc(targets)</span><br><span class="line">        out = self.mseloss(gram_inputs,gram_targets)</span><br><span class="line">        <span class="keyword">return</span> out * self.weight</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(content_layers,style_layers,content_weight,style_weight,content_img,style_img,model,input_param,times)</span>:</span></span><br><span class="line">    loss = <span class="number">0</span> </span><br><span class="line">    <span class="comment">#提取风格图片的特征并计算,注意这里是提取input（合成）和风格/内容的，都要提取</span></span><br><span class="line">    style_features = extract_layers(style_layers,style_img,model)  </span><br><span class="line">    input_features = extract_layers(style_layers,input_param,model) </span><br><span class="line">    <span class="keyword">for</span> style_layer_index <span class="keyword">in</span> range(len(style_layers)):</span><br><span class="line">        style_loss = StyleLoss(style_weight)</span><br><span class="line">        style_loss_this = style_loss(input_features[style_layer_index],style_features[style_layer_index])      </span><br><span class="line">        </span><br><span class="line">    <span class="comment">#提取内容图片的特征并计算   </span></span><br><span class="line">    content_features = extract_layers(content_layers,content_img,model)</span><br><span class="line">    input_features = extract_layers(content_layers,input_param,model)     </span><br><span class="line">    <span class="keyword">for</span> content_layer_index <span class="keyword">in</span> range(len(content_layers)):</span><br><span class="line">        content_loss = ContentLoss(content_weight)</span><br><span class="line">        content_loss_this = content_loss(input_features[content_layer_index],content_features[content_layer_index])    </span><br><span class="line">    </span><br><span class="line">    loss = style_loss_this+content_loss_this</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> times % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'style loss:&#123;:.4f&#125; , content loss:&#123;:.4f&#125;'</span>.format(style_loss_this,content_loss_this))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="TRAIN"><a href="#TRAIN" class="headerlink" title="TRAIN"></a>TRAIN</h2><p>使用LBGFS作为优化函数，LBFGS使用的时候就需要用到闭包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(Epoch,input_img,</span></span></span><br><span class="line"><span class="function"><span class="params">          content_weight,style_weight,content_img,style_img,</span></span></span><br><span class="line"><span class="function"><span class="params">          content_layers=[<span class="number">21</span>],style_layers=[<span class="number">1</span>,<span class="number">6</span>,<span class="number">11</span>,<span class="number">20</span>,<span class="number">25</span>])</span>:</span></span><br><span class="line">    <span class="comment">#默认vgg模型</span></span><br><span class="line">    model = get_model().cuda()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始化合成图片，将input_img作为初始的合成图片并参数化</span></span><br><span class="line">    input_param = nn.Parameter(input_img.data)    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始化优化器</span></span><br><span class="line">    optimizer = torch.optim.LBFGS([input_param])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(Epoch):</span><br><span class="line">        print(<span class="string">'epoch:&#123;&#125;'</span>.format(epoch))</span><br><span class="line">        <span class="comment">#开始训练</span></span><br><span class="line">        <span class="keyword">global</span> times</span><br><span class="line">        times=<span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">closure</span><span class="params">()</span>:</span></span><br><span class="line">            optimizer.zero_grad() <span class="comment">#勿忘</span></span><br><span class="line">            <span class="keyword">global</span> times</span><br><span class="line">            times+=<span class="number">1</span></span><br><span class="line">            loss = loss_fn(content_layers,style_layers,content_weight,style_weight,content_img,style_img,model,input_param,times)</span><br><span class="line">            loss.backward() </span><br><span class="line">            <span class="keyword">return</span> loss</span><br><span class="line">        optimizer.step(closure) </span><br><span class="line">    <span class="keyword">return</span> input_param.data </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showImg</span><span class="params">(res)</span>:</span></span><br><span class="line">    img = res[<span class="number">0</span>]</span><br><span class="line">    img = img.cpu()</span><br><span class="line">    img = transforms.ToPILImage()(img) </span><br><span class="line">    plt.imshow(img)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>风格迁移</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>Adversary Resistant Deep Neural Networks with an Application to Malware Detection</title>
    <url>/2019/09/19/toMalwareDetection/</url>
    <content><![CDATA[<blockquote>
<p>《Adversary Resistant Deep Neural Networks with an Application to Malware Detection》<br>    Qinglong Wang ,Wenbo Guo,Kaixuan Zhang,AlexanderG.OrorbiaII, Xinyu Xing,Xue Liu,C.LeeGiles<br>    KDD 2017（CCF-A）</p>
</blockquote>
<h2 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h2><ul>
<li><p>deep neural networks(DNNs) could help turn the tide in the war against malware infection</p>
</li>
<li><p>However, DNNs are vulnerable to adversarial samples</p>
</li>
<li>Past research in developing defense mechanisms relies on strong assumptions,which typically do not hold in many real-world scenarios. Also,these proposed techniques can only be empirically validated and do not provide any theoretical guarantees. This is particularly disconcerting when they are applied to security-critical applications such as malware detection. </li>
</ul>
<h2 id="Why-It-Works？"><a href="#Why-It-Works？" class="headerlink" title="Why It Works？"></a>Why It Works？</h2><ul>
<li>随机性的引入使得attackers不容易发现DNN的”blind spots”（也就是AEs）</li>
<li>这个adversary-resistant DNNs 只需要一点微小的工作，且可以维持分类的表现</li>
<li>从理论上来说，本文的方法可以保证对AE的抵抗性</li>
</ul>
<p>该方法也可以应用在图像等其他DNN模型适用效果较好的领域</p>
<h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><ul>
<li><p><strong>Data Augmentation</strong></p>
<p>Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).</p>
<p>Towards deep neural network architectures robust to adversarial examples. arXiv:1412.5068 [cs] (2014). </p>
<p>Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization. arXiv:1601.07213 [cs] (2016).</p>
<p>增强数据，主要是通过将潜在AEs与普通样本进行训练（<strong>对抗训练</strong>）以增加对AEs的鲁棒性，对抗训练已被证明很有用</p>
<p>作者指出的问题：blind spots空间太大，不可能去覆盖一个infinite space，且attackers也可以对对抗训练模型本身进行攻击（<em>how</em>），考虑到无限空间，每一次遇到AEs就必须再次训练对抗训练模型，如此反复迭代</p>
</li>
<li><p><strong>Enhancing Model Complexity</strong></p>
<p>增加模型复杂度</p>
<p>Towards deep neural network architectures robust to adversarial examples. arXiv:1412.5068 [cs] (2014).</p>
<p>Distillation as a defense to adversarial perturbations against deep neural networks. arXiv preprint arXiv:1511.04508 (2015).  （防御蒸馏机制，将第一个深度神经网络输出的软标签输入到第二个网络中进行训练，降低模型对微小扰动的敏感度。第一个模型的软标签熵编码了类之间的相对差异）</p>
<p>作者指出的问题：攻击者可以使用两个近似性能的DNN来拟合整个机制（该论文作者承认了此机制很容易被拟合）</p>
<p>同时该机制实际上是一个<strong>梯度掩码</strong>模型，并无法抵抗JSMA的攻击</p>
</li>
</ul>
<h2 id="Random-Feature-Nullification"><a href="#Random-Feature-Nullification" class="headerlink" title="Random Feature Nullification"></a>Random Feature Nullification</h2><p>  we introduce random feature nullification to both the training and testing phases of DNN models, making the architectures non-deterministic. </p>
<p>  引入随机性，对行为特征进行随机失活，看起来像是一种特殊的dropout正则</p>
<p>  但区别在于dropout仅仅在训练的时候对神经元随即失活，而本文的方法是在train和test的时候都执行</p>
<p><img src="//aisakaki.com/2019/09/19/toMalwareDetection/1.png" alt="1"></p>
<p>作者在input与hidden之间加了一层<strong>Nullification层</strong></p>
<p>超参数：$μ_p,\sigma^2_p $</p>
<h2 id="Model-Description"><a href="#Model-Description" class="headerlink" title="Model Description"></a>Model Description</h2><p>$X\in R^{N*M}$ ($N$ 个样本，$M$维特征)</p>
<p>$\hat{I}_p\in R^{N*M}$ (mask matrix)</p>
<p>Nullification来源于$X$与$\hat{I}_p$按位乘</p>
<p>经验证在Random Nullification中会损失有用的分类特征信息，于是为了解决这个问题，本文为每个样本引入了<strong>Nullification Rate</strong>：$p^i$，且不仅哪个神经元失活是随机的，失活神经元的数量也是随机的</p>
<p>单个输入$x_i$对应一个$I_{p^i}$，后者是一个二进制向量，0的数量取决于$p^i$，且随机分布。作者使用了高斯分布和均匀分布。</p>
<p>$⌈M·p^i⌉$ :$I_{p^i}$中随机分布的0的个数，$p^i$是从高斯分布$N(μ_p,\sigma^2_p)$中的一次采样样本</p>
<p>于是DNN的目标函数定义为</p>
<script type="math/tex; mode=display">
\min_\theta \Sigma^N_{i=1} L(f(x_i,I_{p^i};\theta),y_i)</script><p>随机特征失活过程（random feature nullification process）表示为$q(x_i,I_{p^i})=x^i⊙I_{p^i}$</p>
<p>(⊙为Hadamard product，一种特殊的矩阵乘法，同阶矩阵，$c_{ij}=a_{ij}*b_{ij}$)</p>
<p>于是$f(x_i,I_{p^i};\theta)=f(q(x_i,I_{p^i});\theta)$</p>
<p>在训练中使用SGD求解目标函数，但这里不同之处在于$I_{p^i}$在<strong>一个</strong>样本的前向传播和反向传播过程中是固定的，这使得容易计算$L(f(x_i,I_{p^i};\theta),y_i)$的导数</p>
<p>在测试中由于参数固定，使用高斯分布$N(μ_p,\sigma^2_p)$的期望作为辅助的随机变量$p^i$</p>
<h2 id="Analysis-Model-Resistance-to-Adversaries"><a href="#Analysis-Model-Resistance-to-Adversaries" class="headerlink" title="Analysis: Model Resistance to Adversaries"></a>Analysis: Model Resistance to Adversaries</h2><p><strong>攻击防御：</strong></p>
<p>对抗攻击需要求解如下导数($\hat{x}$是任一测试样本)</p>
<script type="math/tex; mode=display">
J_L(\hat{x})=\frac{\partial L(f(x_i,I_{p^i};\theta),\hat{y})}{\partial \hat{x}}</script><script type="math/tex; mode=display">
=J_L(q)·\frac{\partial q(\hat{x},I_p)}{\partial \hat{x}}</script><p>where $J_L(q)=\partial L(f(x_i,I_{p^i};\theta),\hat{y})/\partial q(\hat{x},I_p)$，$I_p$是在测试中使用的mask matrix</p>
<p>只要此式子求解，攻击者便可以生成AE发动攻击$\hat{x} \to\hat{x}+ \phi sign(J_L(\hat{x}))$</p>
<p>但此时由于乘随机变量$I_p$的存在，使得攻击者无法轻易算出$J_L(q)$</p>
<p>这里作者说明了如果$I_p$是加随机变量的话$J_L(q)$将可以被轻易求解</p>
<p><strong>模拟攻击：</strong></p>
<p>因此，攻击者要想攻击此模型的最佳办法就算去拟合$I_p$.</p>
<p>作者假设 $I^*_p$ 为 $I_p$ 的最佳拟合，DNNs为黑盒，最佳的扰动为 $\delta\hat{x}$ </p>
<p>则最佳合成攻击样本为 $\hat{x}+\delta\hat{x}⊙I^*_p$ </p>
<p>假设攻击者用此来攻击下面图2中的系统</p>
<p><img src="//aisakaki.com/2019/09/19/toMalwareDetection/2.png" alt="2"></p>
<p>如图可以看到，<strong>攻击必须经过feature nullification layer才能抵达实际的DNN</strong>，由如下式子表示</p>
<script type="math/tex; mode=display">
(\hat{x}+\delta\hat{x}⊙I^*_p)⊙I_p=(\hat{x}⊙I_p)+\delta\hat{x}⊙I^*_p⊙I_p</script><p>式子中 $\hat{x}⊙I_p$ 即为一个真实的nullified样本，$\delta\hat{x}⊙I^*_p⊙I_p$ 就是发动攻击的添加的扰动</p>
<p>这里可以看到，尽管 $\delta\hat{x}$ 是非常有效的扰动，但 $I^*_p⊙I_p$ 的存在使得该扰动大幅减弱</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>对抗样本</tag>
        <tag>智能对抗</tag>
      </tags>
  </entry>
  <entry>
    <title>云雀</title>
    <url>/2019/09/18/%E4%BA%91%E9%9B%80/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/09/18/云雀/64633930_p0.png" alt="64633930_p0"></p>
<div style="text-align: center"><i>懐かしくあどけない　悲しみを捨ててゆこう</i></div>
<div style="text-align: center"><i>ひとすじ空へ舞い上がる　翼に心をのせて</i></div>
<div style="text-align: center"><i>ねえ　本当はいつだって　一人は寂しいからね</i></div>
<div style="text-align: center"><i>大事なものは　ひとつじゃないの</i></div>
<div style="text-align: center"><i>呼び合っているような　雲雀の声だけ遠く</i></div>
<div style="text-align: center"><i>雲の向こうへ　草原に優しい影を残して</i></div>
<div style="text-align: center"><br> <br>  </div>
<div style="text-align: center"><i>将让人留恋的天真与悲伤都舍弃掉吧</i></div>
<div style="text-align: center"><i>任凭羽翼在碧空中划出舞蹈般的痕迹</i></div>
<div style="text-align: center"><i>其实无论何时 孤独一人终会感到寂寞</i></div>
<div style="text-align: center"><i>珍贵的宝物并非是唯一的</i></div>
<div style="text-align: center"><i>如同呼唤彼此的云雀般 只有遥远的啼鸣</i></div>
<div style="text-align: center"><i>在云之彼方的草原上 留下优雅的掠影</i></div>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>some papers about GAN and AEs in model security</title>
    <url>/2019/09/18/security/</url>
    <content><![CDATA[<p><div style="text-align: center"><i>mainly collected from security conferences and journals，aiming at machine learning model security</i></div><br><a id="more"></a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>year</th>
<th>conf/jour</th>
<th>zone</th>
<th>title</th>
<th>topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017</td>
<td>ESORICS</td>
<td>CCF-B</td>
<td>Adversarial examples for malware detection</td>
<td>AE</td>
</tr>
<tr>
<td>2017</td>
<td></td>
<td></td>
<td>Evading machine learning malware detection</td>
<td></td>
</tr>
<tr>
<td>2017</td>
<td></td>
<td></td>
<td>Generating adversarial malware examples for black-box attacks   based on GAN</td>
<td>GAN</td>
</tr>
<tr>
<td>2017</td>
<td>KDD</td>
<td>CCF-A</td>
<td>Adversary resistant deep neural networks with an application   to malware detection</td>
<td>AE</td>
</tr>
<tr>
<td>2018</td>
<td></td>
<td></td>
<td>Adversarial Malware   Binaries: Evading Deep Learning for Malware Detection in Executables</td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>CoRR</td>
<td></td>
<td>Deceiving end-to-end deep learning malware detectors using   adversarial examples</td>
<td>AE</td>
</tr>
<tr>
<td>2017</td>
<td>RAID</td>
<td>CCF-B</td>
<td>Generic Black-Box   End-to-End Attack Against State of the Art API Call Based Malware   Classifiers</td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td></td>
<td>CCF-B</td>
<td>Zero-day malware   detection using transferred generative adversarial networks based on deep   autoencoders</td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep   Learning</td>
<td>GAN</td>
</tr>
<tr>
<td>2019</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>Misleading Authorship Attribution of Source Code using   Adversarial Learning</td>
<td>AE</td>
</tr>
<tr>
<td>2019</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>Why Do Adversarial   Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks</td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>AttriGuard: A Practical   Defense Against Attribute Inference Attacks via Adversarial Machine Learning</td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>A4NT: Author Attribute   Anonymity by Adversarial Training of Neural Machine Translation</td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>Formal Security Analysis of Neural Networks using Symbolic   Intervals</td>
<td>AE</td>
</tr>
<tr>
<td>2016</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>Stealing Machine Learning Models via Prediction APIs</td>
<td></td>
</tr>
<tr>
<td>2019</td>
<td>USENIX Securit</td>
<td>CCF-A</td>
<td>Seeing is Not Believing: Camouflage Attacks on Image Scaling   Algorithms</td>
<td>AE,IMAGE</td>
</tr>
<tr>
<td>2019</td>
<td>TIFS</td>
<td>CCF-A</td>
<td>GANobfuscator: Mitigating Information Leakage Under GAN via   Differential Privacy.</td>
<td>GAN,PI</td>
</tr>
<tr>
<td>2019</td>
<td>TIFS</td>
<td>CCF-A</td>
<td>FV-GAN: Finger Vein Representation Using Generative   Adversarial Networks</td>
<td>GAN,IMAGE</td>
</tr>
<tr>
<td>2018</td>
<td>TIFS</td>
<td>CCF-A</td>
<td>CNN-Based Adversarial Embedding for Image Steganography</td>
<td>AE,IMAGE</td>
</tr>
<tr>
<td>2017</td>
<td>TIFS</td>
<td>CCF-A</td>
<td>No Bot Expects the   DeepCAPTCHA! Introducing Immutable Adversarial Examples, With Applications to   CAPTCHA Generation</td>
<td></td>
</tr>
<tr>
<td>2017</td>
<td>TIFS</td>
<td>CCF-A</td>
<td>A Game-Theoretic Analysis of Adversarial Classification</td>
<td>AE</td>
</tr>
<tr>
<td>2018</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Yet Another Text Captcha   Solver: A Generative Adversarial Network Based Approach</td>
<td></td>
</tr>
<tr>
<td>2018</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Machine Learning with Membership Privacy using Adversarial   Regularization</td>
<td>AE,PI</td>
</tr>
<tr>
<td>2018</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Tutorials:Wild Patterns: Ten Years After the Rise of   Adversarial Machine Learning</td>
<td>AE</td>
</tr>
<tr>
<td>2018</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Towards Understanding the Dynamics of Adversarial Attacks</td>
<td>AE</td>
</tr>
<tr>
<td>2018</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Adversarial Product Review Generation with Word Replacements</td>
<td>AE,NLP</td>
</tr>
<tr>
<td>2018</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Spartan Networks:   Self-Feature-Squeezing Networks for Increased Robustness in Adversarial   Settings</td>
<td></td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>DolphinAttack: Inaudible Voice Commands</td>
<td>AE</td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Evading Classifiers by Morphing in the Dark</td>
<td>AE</td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>MagNet: A Two-Pronged Defense against Adversarial Examples</td>
<td>AE</td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Practical Attacks Against Graph-based Clustering</td>
<td>AE,IMAGE</td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Automated Crowdturfing Attacks and Defenses in Online Review   Systems</td>
<td>AE,NLP</td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>POISED: Spotting Twitter Spam Off the Beaten Paths</td>
<td>AE,NLP</td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Poster: Adversarial Examples for Classifiers in   High-Dimensional Network Data.</td>
<td>AE</td>
</tr>
<tr>
<td>2017</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Deep Models Under the   GAN: Information Leakage from Collaborative Deep Learning</td>
<td></td>
</tr>
<tr>
<td>2016</td>
<td>CCS</td>
<td>CCF-A</td>
<td>Tutorials:Adversarial Data Mining: Big Data Meets Cyber   Security.</td>
<td>AE</td>
</tr>
<tr>
<td>2016</td>
<td>S&amp;P</td>
<td>CCF-A</td>
<td>Distillation as a Defense to Adversarial Perturbations Against   Deep Neural Networks</td>
<td>AE</td>
</tr>
<tr>
<td>2019</td>
<td>AAAI</td>
<td>CCF-A</td>
<td>MIGAN: Malware Image Synthesis Using GANs.</td>
<td>GAN</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>AE</tag>
        <tag>model-security</tag>
      </tags>
  </entry>
  <entry>
    <title>个人NAS计划</title>
    <url>/2019/08/28/%E4%B8%AA%E4%BA%BANAS%E8%AE%A1%E5%88%92/</url>
    <content><![CDATA[<p>眼看着google drive容量即将撑爆..而且9月13号到期<img src="//aisakaki.com/2019/08/28/个人NAS计划/1.png" alt="1"><br>然后一看续费扩容计划…<br> <a id="more"></a><br><img src="//aisakaki.com/2019/08/28/个人NAS计划/2.png" alt="2"></p>
<p>99.99$/1T emmm，家境贫寒，告辞</p>
<p>于是目光投向微软<img src="//aisakaki.com/2019/08/28/个人NAS计划/3.png" alt="3"></p>
<p>。。。。。<br><img src="//aisakaki.com/2019/08/28/个人NAS计划/7.png" alt="7"><br>。<br>。<br>。<br>。<br>。<br>。<br>。<br>。</p>
<p><img src="//aisakaki.com/2019/08/28/个人NAS计划/4.png" alt="4"><br>谷歌我不做人辣！！！</p>
<p>那。。那就组个NAS吧！<br><img src="//aisakaki.com/2019/08/28/个人NAS计划/5.png" alt="5"></p>
<p>学校里有千兆网，还可以秒掉gdrive和onedrive的龟速<br>正好用上以前大二买的台式机淘汰下来的硬件，先记录一下各个组件</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>组件</th>
<th>具体型号</th>
<th>价格(元)</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>9100f</td>
<td>503</td>
</tr>
<tr>
<td>GPU</td>
<td>亮机卡</td>
<td>18</td>
</tr>
<tr>
<td>主板</td>
<td>影驰b360m.2</td>
<td>280</td>
</tr>
<tr>
<td>固态</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>内存</td>
<td>8g+8g(ddr4 2400)</td>
<td>现成</td>
</tr>
<tr>
<td>散热器</td>
<td>ID-COOLING</td>
<td>现成</td>
</tr>
<tr>
<td>硬盘支架</td>
<td></td>
<td>要买</td>
</tr>
<tr>
<td>机箱</td>
<td>Q300L</td>
<td>现成</td>
</tr>
<tr>
<td>电源</td>
<td>VS450</td>
<td>现成</td>
</tr>
<tr>
<td>初始硬盘</td>
<td>WD1T</td>
<td>现成</td>
</tr>
<tr>
<td>数据硬盘</td>
<td>东芝 P300</td>
<td>448</td>
</tr>
<tr>
<td>网线</td>
<td>千兆</td>
<td>现成</td>
</tr>
<tr>
<td>合计</td>
<td></td>
<td>1249</td>
</tr>
</tbody>
</table>
</div>
<p>然后就可以享受千兆上传下载，无缝同步，超大容量的个人云端硬盘了，爽到<br><img src="//aisakaki.com/2019/08/28/个人NAS计划/9.png" alt="9"></p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>高达升级</title>
    <url>/2019/08/16/%E9%AB%98%E8%BE%BE%E5%8D%87%E7%BA%A7/</url>
    <content><![CDATA[<p>这个月又对我的台式机PC电脑的几个零件进行了升级，主要是更换了CPU，添加了固态硬盘，而且把原来CPU的单风扇120水冷换成了双风扇夹汉堡。</p>
<p>9900k这货实在是个大火炉，发热量太大了。后来我才发现我忘了设置SYS_FAN的温度检测区域，导致CPU或GPU都热爆了风扇却不全速转起来。</p>
<p>以下为升级后现在的配置：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>硬件</th>
<th></th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>Inter i9 9900K</td>
<td>主频3.6GHz，睿频5GHz</td>
</tr>
<tr>
<td>CPU散热</td>
<td>恩杰 海妖 X62</td>
<td>280冷排，双AER2 风扇</td>
</tr>
<tr>
<td>内存</td>
<td>海盗船LPX 32G</td>
<td>16G*2  3000GHz</td>
</tr>
<tr>
<td>主板</td>
<td>华擎 Z390 Gaming Phantom</td>
<td>ITX主板</td>
</tr>
<tr>
<td>显卡</td>
<td>Inno RTX 2080Ti</td>
<td>-</td>
</tr>
<tr>
<td>显卡散热</td>
<td>Alphacool 北极狼水冷</td>
<td>240冷排，四be quiet风扇</td>
</tr>
<tr>
<td>硬盘</td>
<td>NVME 固态硬盘：三星970evo Plus</td>
<td>1T</td>
</tr>
<tr>
<td></td>
<td>NVME 固态硬盘：海康威视C2000Pro</td>
<td>2T</td>
</tr>
<tr>
<td></td>
<td>SATAIII 固态硬盘：西部数码 蓝盘</td>
<td>2T</td>
</tr>
<tr>
<td></td>
<td>SATAIII 固态硬盘：英睿达 BX300</td>
<td>0.5T    (MLC颗粒)</td>
</tr>
<tr>
<td></td>
<td>机械硬盘：希捷酷鱼</td>
<td>1T</td>
</tr>
<tr>
<td></td>
<td>机械硬盘：西部数码</td>
<td>1T</td>
</tr>
<tr>
<td>电源</td>
<td>振华 冰山金蝶GX650</td>
<td>650W</td>
</tr>
<tr>
<td>机箱</td>
<td>NZXT MANTA 白色</td>
<td>-</td>
</tr>
<tr>
<td>其他风扇</td>
<td>乔思伯 日食 Plus</td>
<td>后置位一个</td>
</tr>
<tr>
<td>显示器</td>
<td>DELL U2718QM</td>
<td>4K 27英寸</td>
</tr>
<tr>
<td>鼠标</td>
<td>罗技 G903</td>
<td>-</td>
</tr>
<tr>
<td>键盘</td>
<td>IKBC W200</td>
<td>红轴，霜冻之蓝键帽</td>
</tr>
<tr>
<td>机箱手办</td>
<td>美柑</td>
<td>老婆</td>
</tr>
<tr>
<td>耳机</td>
<td>1000XM2</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p>以后考虑上sunmilo T03那款机箱，那个就得改用itx板子（必须双m2位），电源改sfx（+1000），CPU风扇用风冷。现在暂时先不折腾了</p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
  </entry>
  <entry>
    <title>再见，夏天</title>
    <url>/2019/08/11/summer/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/08/11/summer/F845CF3DE17D87ACAEB421026508A8A1.jpg" alt="F845CF3DE17D87ACAEB421026508A8A1"></p>
<div style="text-align: center">朝着大海的列车，在书香，旋律与橘子芬香中，驶向夏天的终点 </div>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>夏天</tag>
      </tags>
  </entry>
  <entry>
    <title>夏休</title>
    <url>/2019/08/05/JSY/</url>
    <content><![CDATA[<p>这个暑假应该是人生最短的一次暑假了</p>
<p>傍晚回到了中学时期住过的小区去看看，保卫大叔竟然一眼认出了我，还和我聊了些以前的事情</p>
<p>我很惊讶那些小事他都记得，</p>
<p>而且明明5年都没有回去过了。</p>
<p>走出大门，蝉鸣声依然密密麻麻。渐行渐远，不一会儿变得窸窸窣窣。</p>
<p>大叔叫我多回来玩，可我回来能去哪呢？</p>
<p>蝉鸣声声响，可夏日并不长。</p>
<p><img src="//aisakaki.com/2019/08/05/JSY/E9C4B851D57299086E6779C064F6C5B6.png" alt="E9C4B851D57299086E6779C064F6C5B6"></p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>Explaining and Harnessing Adversarial Examples</title>
    <url>/2019/07/30/Examples/</url>
    <content><![CDATA[<blockquote>
<p>《Explaining and Harnessing Adversarial Examples》<br>   Ian J. Goodfellow, Jonathon Shlens &amp; Christian Szegedy<br>   ICLR 2015</p>
</blockquote>
<h2 id="为什么会产生对抗样本？"><a href="#为什么会产生对抗样本？" class="headerlink" title="为什么会产生对抗样本？"></a>为什么会产生对抗样本？</h2><p>以前的观点： extreme nonlinearity of DNN and insufﬁcient model averaging and insufﬁcient regularization of the purely supervised learning problem</p>
<p>本文的观点：<strong>Linear behavior</strong> in <strong>high-dimensional spaces</strong> is sufﬁcient to cause adversarial examples</p>
<h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><ol>
<li><p>Box-constrained L-BFGS can reliably ﬁnd adversarial examples.</p>
</li>
<li><p>On some datasets, such as ImageNet (Deng et al., 2009), the adversarial examples were so close to the original examples that the differences were indistinguishable to the human eye.</p>
</li>
<li><p>The same adversarial example is often misclassiﬁed by avariety of classiﬁers with different architectures or trained on different subsets of the training data.</p>
</li>
<li><p>Shallow softmax regression models are also vulnerable to adversarial examples. </p>
</li>
<li><p>Training on adversarial examples can regularize the model —however , this was not practical at the time due to the need for expensive constrained optimization in the inner loop</p>
</li>
</ol>
<p>这就引起我们怀疑即便当前最好的模型是否只是虚有其表，并没有真正学习到内在的语义信息。仅仅能够fit现有样本，无法泛化。</p>
<h2 id="对抗样本的线性解释"><a href="#对抗样本的线性解释" class="headerlink" title="对抗样本的线性解释"></a>对抗样本的线性解释</h2><p>本文提出了对对抗样本产生原因的线性解释.设置扰动$\eta,\epsilon$为一个小到可以被分类器忽略的值</p>
<p>$\hat{x} = x+\eta$ ，若$||\eta||_\infty&lt;\epsilon$，分类器将无法将$x$与$\hat{x}$分开</p>
<p>则$w^T\hat{x}=w^T(x+\eta)=w^Tx+w^T\eta$</p>
<p>若维度很高，则即使$\eta$很小，也会使得$w^T\eta$非常大，影响分类器的判断</p>
<p>令$\eta=sign(w)$即可使得$w^T\eta$最大</p>
<h2 id="非线性模型的线性扰动"><a href="#非线性模型的线性扰动" class="headerlink" title="非线性模型的线性扰动"></a>非线性模型的线性扰动</h2><p>对于现在的神经网络都有很强的线性性质以至于无法抵抗对抗攻击，如 LSTMs (Hochreiter &amp; Schmidhuber, 1997), ReLUs (Jarrett et al., 2009; Glorot et al., 2011),  maxout networks (Goodfellow et al., 2013c) 都设计得具有很强的线性性质，所以也方便优化。FGSM将损失函数近似线性化，从而获取$||\eta||_\infty&lt;\epsilon$的最优扰动。</p>
<p><strong>Fast Gradient Sign Method(FGSM)</strong></p>
<script type="math/tex; mode=display">
\eta = \epsilon sign(\nabla_xJ(\theta,x,y))​</script><script type="math/tex; mode=display">
\hat{x} = x+\epsilon sign(\nabla_xJ(\theta,x,y))</script><p>注意这里$y=y_{true}$</p>
<p>通过实验证明，作者的假设产生对抗样本的原因是由于模型的线性特性是正确的。这种方法也可以作为一种快速生成对抗样本的方法（即FGSM）</p>
<h2 id="线性模型的对抗训练"><a href="#线性模型的对抗训练" class="headerlink" title="线性模型的对抗训练"></a>线性模型的对抗训练</h2><p>在Logistics Regression上应用FGSM方法 ，$label\in\lbrace1,-1\rbrace$</p>
<p>$P(y=1|x)=\sigma(w^Tx+b),\sigma(z)=softmax(z)$   </p>
<p>$\therefore J = E{x,y \tilde{} p_{data}}[(-\frac{1+y}{2})\ln P(y=1|x)-\frac{1-y}{2}\ln P(y=-1|x)]$</p>
<p>设$p=P(y=1|x),f=w^Tx+b$</p>
<p>$\therefore J=-\frac{1}{2}\ln p-\frac{y}{2}\ln p-\frac{1}{2}\ln (1-p)+\frac{y}{2}\ln (1-p)$</p>
<p>$=-\frac{1}{2}\ln p（1-p)-\frac{y}{2}\ln \frac{p}{1-p} $</p>
<p>$= \frac{1}{2}(f-yf) + \ln(1+e^{-f})$</p>
<p>$=\begin{cases} \ln(1+e^{-f}),y=1\\ f+\ln(1+e^{-f})=\ln e^f+\ln(1+e^{-f})=\ln(1+e^f),y=-1\end{cases}$</p>
<p>$= \ln(1+e^{-yf})$</p>
<p>$=\zeta(-yf)=E{x,y \tilde{} p_{data}}\zeta(-y(w^Tx+b)),\zeta(z) = \ln(1+e^z)$  </p>
<p>$\therefore \eta=\epsilon sign(\nabla_xJ(\theta,x,y))$</p>
<p>$=\epsilon sign(\nabla_x\zeta(-y(w^Tx+b)))$</p>
<p>$= \epsilon sign(\frac{\partial \ln(1+e^{-y(w^Tx+b)})}{\partial x})$</p>
<p>$=\epsilon sign(\frac{1}{1+e^{-y(w^Tx+b)}} ×(-e^{-y(w^Tx+b)})×(yw^T))$</p>
<p>$=\epsilon sign(\frac{e^{-yf}}{1+e^{-yf}}×(-y)w)$</p>
<p>$=\epsilon sign(-w)=-\epsilon sign(w)$</p>
<p>$\because w^Tsign(w)=||w||_1，\hat{x}=x+\eta,\eta=\epsilon-sign(w)$</p>
<p>$\therefore \hat{J}=E{x,y \tilde{} p_{data}}\zeta(y(\epsilon||w||_1 -w^Tx-b))$</p>
<p>可以看到类似于$L^1$正则，不过是减去$L^1$惩罚项。当置信度很高，$w^Tx+b$足够大的时候，$\epsilon||w||_1$几乎不起作用，但是当模型欠拟合的时候，则会更加欠拟合</p>
<h2 id="深度网络的对抗训练"><a href="#深度网络的对抗训练" class="headerlink" title="深度网络的对抗训练"></a>深度网络的对抗训练</h2><p>相比于纯线性模型，深度网络可以在训练网络过程中来抵御对抗攻击。</p>
<script type="math/tex; mode=display">
\hat{J}= \alpha J(\theta,x,y)+(1-\alpha)J(\theta,x+\epsilon sign(\nabla_xJ(\theta,x,y)))</script><p>这种方法在训练中不断更新对抗样本，同时训练。</p>
<h2 id="对抗样本的泛化原因"><a href="#对抗样本的泛化原因" class="headerlink" title="对抗样本的泛化原因"></a>对抗样本的泛化原因</h2><p>根据线性解释，FGSM可以在连续空间上生成对抗样本，而不是特定的点。作者通过取不同的$\epsilon$值证实了这一点。</p>
<p>为什么不同的分类器会将对抗样本误分到同一类？因为作者假设的模型都在训练集的不同子集上训练，模型泛化后学得的参数差不多，具有一定的稳定性，导致对抗样本也具有稳定性。</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>对抗样本</tag>
        <tag>智能对抗</tag>
      </tags>
  </entry>
  <entry>
    <title>Pray for Kyoto Animation</title>
    <url>/2019/07/27/Animation/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/07/27/Animation/QQ图片20190726230738.jpg" alt="QQ图片20190726230738"></p>
<div style="text-align: center">你们一直伴随着我的青春时光，生命却止步于此。 </div>

<div style="text-align: center">R.I.P. 为动画世界奉献了青春的你们。 </div>



]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>京阿尼</tag>
      </tags>
  </entry>
  <entry>
    <title>mini-batch 在网络里的并行计算</title>
    <url>/2019/07/26/mini-batch/</url>
    <content><![CDATA[<p>使用小批量可以</p>
<p>①<strong>提高模型的训练速度，不用过于频繁地计算参数</strong>。通过经验风险最小化由一组样本并行计算并共同决定这批数据的梯度方向</p>
<p> ②<strong>引入随机性以避免陷入局部极值</strong></p>
<p>具体到设计网络，以RNN为例（RNN与CNN相比的输入是反常的）</p>
<p>输入节点为$x$,隐藏层节点为$h$，输出节点为$o$。设vec_size = v, hidden_size = h, batch_size = 2,则设定参数矩阵(省略偏移)</p>
<script type="math/tex; mode=display">W_{xh}=W_{v*h} , W_{hh}=W_{h*h}, W_{ho}=W_{h*o} ， seq_a = [a_0,a_1,a_2],seq_b = [b_0,b_1,b_2]</script><p>于是一个mini-batch为<script type="math/tex">[[a_0,b_0],[a_1,b_1],[a_2,b_2]]_{3*2*v}</script></p>
<p>对其进行迭代，则$t_0$时刻输入矩阵<script type="math/tex">X^{t_0}=[a_0,b_0]=[a^{t_0}_{1*v},b^{t_0}_{1*v}]</script></p>
<p>注意这里在python中的表示用数学表示形式是一个二维矩阵，</p>
<script type="math/tex; mode=display">
Input=
X^{t_0} = X_{2*v} = 
\begin{bmatrix}
{a_{1*v}}\\{b_{1*v}}
\end{bmatrix}</script><p>于是一次循环：</p>
<script type="math/tex; mode=display">
Hidden=
\sigma (X^{t_0}×W_{xh} + H^{(t_0-1)}×W_{hh})=\sigma(X_{2*v}×W_{v*h}+ H^{(t_0-1)}_{2*h}×W_{h*h})</script><script type="math/tex; mode=display">
= \sigma(
\begin{bmatrix}
a^{t_0}_{1*v}×W_{v*h}\\  
b^{t_0}_{1*v}×W_{v*h}
\end{bmatrix}
+
\begin{bmatrix}H^{(t_0-1)}_{1*h}×W_{h*h}\\ 
H^{(t_0-1)}_{1*h}×W_{h*h}
\end{bmatrix}
)</script><script type="math/tex; mode=display">
=
\begin{bmatrix}
{h^{a_0}_{1*h}}\\{h^{b_0}_{1*h}}
\end{bmatrix}
=H^{(t_0)}_{2*h}</script><script type="math/tex; mode=display">
Output=
H^{(t_0)}_{2*h}×W_{h*o} = 
\begin{bmatrix}
{h^{a_0}_{1*h}×W_{h*o}}\\
{h^{b_0}_{1*h}×W_{h*o}}
\end{bmatrix}</script><script type="math/tex; mode=display">
=
\begin{bmatrix}
{o^{a_0}_{1*o}}\\{o^{b_0}_{1*o}}
\end{bmatrix}
=O^{t_0}_{2*o}</script><p>由此可看出，在矩阵相乘的规则下，相当于将不同的$seq$样本同时加载进网络中同时并行计算。</p>
<p>通过一段时间将mini-batch全部加载进循环神经网络，通过loss function再平均作为mini-batch的loss，再作BPTT，就可以求出此mini-batch的梯度。</p>
<p>对于CNN，也是一样的原理同时处理一个mini-batch的所有图片，只不过其输入方式与处理方式有所不同。</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>mini-batch</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN做批</title>
    <url>/2019/07/25/RNN%E5%81%9A%E6%89%B9/</url>
    <content><![CDATA[<p>输入RNN的是时间序列，与CNN是<strong>反过来</strong>的。CNN的输入是输入(batch_size,$C_{in},H_{in},W_{in}$)，而RNN的输入是(seq_len, batch_size, input_size)，batch_size位于第二维度。</p>
<p>在CNN中我们是要在同一时间里输入batch里每张图，而在RNN里我们是要在同一时刻输入一个batch里每个序列的同一个位置。</p>
<p>训练每个batch后，使用ERM进行BP计算，与CNN一样</p>
<p>以下画图理解，注意紫色是实体结构</p>
<p><img src="//aisakaki.com/2019/07/25/RNN做批/0.png" alt="0"></p>
<p>于是对于一个可迭代的dataloader</p>
<p><img src="//aisakaki.com/2019/07/25/RNN做批/RNN输入结构2.png" alt="RNN输入结构2"></p>
<p>对语料矩阵进行转置后可以更快捷方便生批</p>
<p><img src="//aisakaki.com/2019/07/25/RNN做批/RNN快捷生批.png" alt="RNN快捷生批"></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>我永远喜欢间桐樱</title>
    <url>/2019/07/15/%E6%88%91%E6%B0%B8%E8%BF%9C%E5%96%9C%E6%AC%A2%E9%97%B4%E6%A1%90%E6%A8%B1-jpg/</url>
    <content><![CDATA[<p><img src="//aisakaki.com/2019/07/15/我永远喜欢间桐樱-jpg/53357546_p0.png" alt="53357546_p0"></p>
]]></content>
      <categories>
        <category>Anime</category>
      </categories>
      <tags>
        <tag>老婆</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch获取中间层参数、输出与可视化</title>
    <url>/2019/07/10/pytorch%E8%8E%B7%E5%8F%96%E4%B8%AD%E9%97%B4%E5%B1%82%E5%8F%82%E6%95%B0%E3%80%81%E8%BE%93%E5%87%BA%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<h2 id="获取模型中间层的权重和其他参数"><a href="#获取模型中间层的权重和其他参数" class="headerlink" title="获取模型中间层的权重和其他参数"></a>获取模型中间层的权重和其他参数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先设定如下网络</span></span><br><span class="line"><span class="comment">#定义网络结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">	    self.c1 = nn.Sequential(</span><br><span class="line">	    nn.Conv2d(<span class="number">3</span>,<span class="number">16</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>), </span><br><span class="line">	    nn.ReLU()</span><br><span class="line">	        ) </span><br><span class="line">	    self.c2 = nn.Sequential(</span><br><span class="line">	        nn.Conv2d(<span class="number">16</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>), </span><br><span class="line">	        nn.ReLU(),</span><br><span class="line">	    )</span><br><span class="line">	    self.fc = nn.Linear(<span class="number">2097152</span>,<span class="number">2</span>)</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">	    x = self.c1(x)</span><br><span class="line">	    x = self.c2(x)</span><br><span class="line">	    x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) </span><br><span class="line">	    x = self.fc(x) </span><br><span class="line">	    <span class="keyword">return</span> x    </span><br><span class="line">model = net()</span><br><span class="line">model = model.cuda()</span><br><span class="line"><span class="comment">#载入先前训练好并保存的模型权重</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">'model_wts.pkl'</span>))</span><br><span class="line"><span class="comment">#此时网络的结构为</span></span><br><span class="line">model</span><br><span class="line">&gt;&gt;</span><br><span class="line">net(</span><br><span class="line">  (c1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (c2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (fc): Linear(in_features=<span class="number">2097152</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">)</span><br><span class="line">	</span><br><span class="line"><span class="comment">#获取某一层</span></span><br><span class="line">model.c1</span><br><span class="line">&gt;&gt;</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">)</span><br><span class="line">model.fc</span><br><span class="line">&gt;&gt;</span><br><span class="line">Linear(in_features=<span class="number">1048576</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#获取Sequential里的子层</span></span><br><span class="line">model.c1[<span class="number">0</span>]</span><br><span class="line">&gt;&gt;</span><br><span class="line">Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment">#获得某层的权重</span></span><br><span class="line">model.c1[<span class="number">0</span>].weight</span><br><span class="line">&gt;&gt;</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">2.7182e-03</span>, <span class="number">-8.7767e-03</span>,  <span class="number">3.2988e-02</span>, <span class="number">-1.0006e-01</span>, <span class="number">-1.1177e-01</span>],</span><br><span class="line">          [<span class="number">-2.9155e-02</span>, <span class="number">-6.2152e-02</span>,  <span class="number">4.1465e-02</span>, <span class="number">-4.5812e-02</span>,  <span class="number">6.7885e-02</span>],</span><br><span class="line">          [<span class="number">-1.0680e-01</span>, <span class="number">-1.0023e-01</span>, <span class="number">-1.7158e-02</span>, <span class="number">-1.3828e-02</span>,  <span class="number">5.7319e-02</span>],</span><br><span class="line">          [ <span class="number">5.1668e-02</span>, <span class="number">-4.2982e-02</span>,  <span class="number">2.7770e-02</span>, <span class="number">-1.1801e-01</span>,  <span class="number">7.9863e-02</span>],</span><br><span class="line">          [ <span class="number">1.1050e-01</span>,  <span class="number">2.4979e-02</span>,  <span class="number">5.1047e-03</span>, <span class="number">-4.6120e-02</span>, <span class="number">-9.9121e-02</span>]],</span><br><span class="line">··························<span class="comment">#省略</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#参数</span></span><br><span class="line">model.c1[<span class="number">0</span>].parameters()为该层的参数，包含梯度等等</span><br><span class="line">parameters()输出的参数在训练的时候需要传入优化器，比如在训练网络的时候，model的所有参数</span><br><span class="line">都要传入优化器，则是 optimizer = torch.optim.Adam(model.parameters(),lr=LR)</span><br><span class="line">又如下面一条迁移学习，只要训练最后一层，就只将最后一层的参数传入了优化器</span><br><span class="line"></span><br><span class="line"><span class="comment">#层的迭代器</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.SequentialName.children():</span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"><span class="comment">#应用方法可以见下面迁移学习部分</span></span><br></pre></td></tr></table></figure>
 <a id="more"></a>
<h2 id="所以如果要进行迁移学习"><a href="#所以如果要进行迁移学习" class="headerlink" title="所以如果要进行迁移学习"></a>所以如果要进行<strong>迁移学习</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#以VGG举例</span></span><br><span class="line">VGG的结构为</span><br><span class="line">VGG&#123;</span><br><span class="line">(features):Sequential(....略....)</span><br><span class="line">(classifier):Sequntial(....略.........(6)Linear(4096-&gt;1000))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#这里采用预训练的，[创建vgg网络实例]</span></span><br><span class="line">vgg = models.vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#[设置冻结层](只改变classifier里的最后一个线性层，特征提取层不变）</span></span><br><span class="line"><span class="comment">#冻结住特征提取层(vgg取名叫features)的参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> vgg.features.parameters():</span><br><span class="line">	param.requires_grad=<span class="keyword">False</span></span><br><span class="line"><span class="comment">#[微调模型]（只改变最后一层fc层，将1000类分类变为我想要的2类分类）</span></span><br><span class="line"><span class="comment">#注意如果用vgg.classifier[6].out_features = 2的话以后会遇到问题，输出还是1000种类</span></span><br><span class="line">   vgg.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#优化器设置 （由于我们直接用预训练的权重，所以只需要训练分类器的参数，所以只将最后一个分类器层的classifier.parameters传入优化器）</span></span><br><span class="line">optimizer = optim.SGD(vgg.classifier.parameters(),lr=<span class="number">0.0001</span>,momentum=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#还可以继续对模型进行一些修改，比如修改最后一层中的dropout</span></span><br><span class="line"><span class="comment">#这里使用了迭代器</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> vgg.classifier.children():</span><br><span class="line">	<span class="keyword">if</span> (type(layer) == nn.Dropout):</span><br><span class="line">		layer.p=<span class="number">0.2</span></span><br></pre></td></tr></table></figure>
<h2 id="使用pytorch-hook进行中间层输出与可视化"><a href="#使用pytorch-hook进行中间层输出与可视化" class="headerlink" title="使用pytorch hook进行中间层输出与可视化"></a>使用pytorch hook进行中间层输出与可视化</h2><p>（这里hook上面第一节中创建的网络）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义用于hook的类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerActivations</span><span class="params">()</span>:</span></span><br><span class="line">	<span class="comment">#定义这个变量用于储存结果</span></span><br><span class="line">	features = <span class="keyword">None</span></span><br><span class="line">	<span class="comment">#类初始化。当前向传播的时候（即图像数据通过层传输的时候），调用register_forward_hook方法。</span></span><br><span class="line">	<span class="comment">#register_forward_hook方法即为[钩子]，此方法返回一个句柄保存到self.hook</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,model,layer_num)</span>:</span></span><br><span class="line">		self.hook = model[layer_num].register_forward_hook(self.hook_fn)</span><br><span class="line">	<span class="comment">#hook函数具体执行的方法，即hook方法</span></span><br><span class="line">	<span class="comment">#register_forward_hook将三个参数传入hook_fn方法内</span></span><br><span class="line">	<span class="comment">#module:允许访问层本身 input:流进层的数据 output:层变换后的流出的数据或激活</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">hook_fn</span><span class="params">(self,module,input,output)</span>:</span></span><br><span class="line">		<span class="comment">#将输出保存到[自己设置的features变量中]</span></span><br><span class="line">		self.features = output.cpu()</span><br><span class="line">	<span class="comment">#注销句柄self.hook</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.hook.remove()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义hook类实例</span></span><br><span class="line">conv_out = LayerActivations(model.c1,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#运行模型</span></span><br><span class="line">output = model(img)</span><br><span class="line"><span class="comment">#注销函数</span></span><br><span class="line">conv_out.remove()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在hook class中被保存到了features变量的即为输出，自己定义的</span></span><br><span class="line">activations = conv_out.features</span><br><span class="line"></span><br><span class="line"><span class="comment">#activations 即为层输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对其进行可视化</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>,<span class="number">50</span>))</span><br><span class="line">fig.subplots_adjust(left=<span class="number">0</span>,right=<span class="number">1</span>,bottom=<span class="number">0</span>,top=<span class="number">0.8</span>,hspace=<span class="number">0</span>,wspace=<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">ax = fig.add_subplot(<span class="number">12</span>,<span class="number">5</span>,i+<span class="number">1</span>,xticks=[],yticks=[])</span><br><span class="line">ax.imshow(activations[<span class="number">0</span>][i].detach().numpy())</span><br></pre></td></tr></table></figure>
<h2 id="中间层参数可视化"><a href="#中间层参数可视化" class="headerlink" title="中间层参数可视化"></a>中间层参数可视化</h2><p>有两种方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#方法一</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> nn.parameters():</span><br><span class="line">	xxx</span><br><span class="line"><span class="comment">#方法二</span></span><br><span class="line">weight = model.state_dict()[<span class="string">'features.0.weight'</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Adversarial Nets</title>
    <url>/2019/06/01/GenerativeAdversarialNets/</url>
    <content><![CDATA[<blockquote>
<p>《Generative Adversarial Nets》</p>
<p>Ian J.Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair†, Aaron Courville, Yoshua Bengio</p>
<p>ICLR 2014</p>
<p>GAN开山之作</p>
</blockquote>
<h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2>]]></content>
      <categories>
        <category>人工智能</category>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>n-gram实践</title>
    <url>/2019/05/29/n-gram%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>对于加固的应用，可以用通过在沙箱穷举各种可能操作返回的API操作序列来建立模型，处理API序列可以用N-gram来分析语义。</p>
<p>发现网上好像很少有用中文说明具体使用，所以记录一下。</p>
<p>n-gram处理之后，实际上是把句子<strong>划分为</strong>不同的gram！</p>
<p>以2元词 2-gram（bigram为例）</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">class Phrases(sentences=None, min_count=5, threshold=10.0, max_vocab_size=40000000, delimiter=None, progress_per=10000, scoring="default", common_terms=frozenset)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">corpusList = [<span class="string">'我'</span>,<span class="string">'爱'</span>,<span class="string">'你'</span>,<span class="string">'爱'</span>,<span class="string">'你'</span>,<span class="string">'我'</span>,<span class="string">'爱'</span>],[<span class="string">'我'</span>,<span class="string">'也'</span>,<span class="string">'爱'</span>,<span class="string">'你'</span>]</span><br><span class="line">bigram = Phrases(corpusList, min_count=<span class="number">1</span>, threshold=<span class="number">0.01</span>, delimiter=<span class="string">b'~'</span>)</span><br><span class="line"></span><br><span class="line">texts = [bigram[line] <span class="keyword">for</span> line <span class="keyword">in</span> corpusList]</span><br><span class="line">dictionary = corpora.Dictionary(texts)</span><br><span class="line">corpus = [dictionary.doc2bow(text) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出划分结果</span></span><br><span class="line">corpus</span><br><span class="line">&gt;&gt;[[(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">1</span>)], [(<span class="number">2</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">1</span>)]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出标签</span></span><br><span class="line">dictionary.token2id</span><br><span class="line"></span><br><span class="line">&gt;&gt;&#123;<span class="string">'也'</span>: <span class="number">3</span>, <span class="string">'你'</span>: <span class="number">0</span>, <span class="string">'我'</span>: <span class="number">4</span>, <span class="string">'我~爱'</span>: <span class="number">1</span>, <span class="string">'爱~你'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>
<p>处理过程<br>①对语料库corpusList（<strong>document</strong>）内每个句子（<strong>text</strong>）以大小为2的窗口进行滑动，</p>
<p>句子1：我爱 爱你 你爱 爱你 你我 我爱</p>
<p>句子2：我也 也爱 爱你</p>
<p>统计两两出现次数（整个语料库），得到<br>我爱：2   爱你：3   你爱：1   你我：1   我也：1   也爱：1</p>
<p>（<strong>delimiter</strong>=b’~’参数规定了2-gram的命名方式，两个词以<code>~</code>为连接符号。这里后面就省略连接符号方便看）</p>
<p>②<strong>min_count</strong>:Ignore <strong>all words and bigrams</strong> with <strong>total collected</strong> count <strong>lower than</strong> this value.</p>
<p>表示一个2-gram<strong>最少必须大于</strong>的频数，频数<strong>小于等于2 的</strong>2-gram就不以此为划分，于是变成<br>我爱：2   爱你：3 </p>
<p><strong>threshold</strong>:  Represent a <strong>score threshold</strong> for forming the phrases (higher means fewer phrases)</p>
<p>score评分方法见 附录[1]，这里设置threshold为0.01非常小的目的就是不考虑阈值参数</p>
<p>于是“我~爱”将被ignore，“爱~我”能够保留</p>
<p>③开始划分<br>句子1：<u>我爱</u> 你 <u>爱你</u> <u>我爱</u><br>句子2：我 也 <u>爱你</u></p>
<p><strong>未被划分到2-gram的，就是单字频数</strong></p>
<p>于是用<strong>doc2bow</strong>转换为词袋后，词频列表为</p>
<p>句子1：  我爱：2，爱你：1，你：1</p>
<p>句子2：  爱你：1，我：1，也：1</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ol>
<li>这个score有两种计算公式，默认使用Efficient Estimaton of Word Representations in Vector Space算法</li>
</ol>
<script type="math/tex; mode=display">
\frac{(count(worda\quad followed\quad by\quad wordb) - mincount) * N }{(count(worda) * count(wordb))} > threshold, where\quad N\quad is\quad the\quad total\quad vocabulary\quad size.</script><p>，另一种是npmi(ormalized pointwise mutual information, from “Normalized (Pointwise) Mutual)</p>
<p>可以通过设置score参数选择评价方式</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>机器学习与统计学习</category>
      </categories>
      <tags>
        <tag>n-gram</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>训练集增补后两个非常诡异的错误</title>
    <url>/2019/05/08/Pytorch-cuda-runtime-error-59-device-side-assert-triggered-at-pytorch-aten-src-THC-generic-THCT-%E9%94%99%E8%AF%AF/</url>
    <content><![CDATA[<p>这错误原因说了等于没说，完全不知道问题出在哪，只能靠 <code>想 象 力</code></p>
<ol>
<li><p><code>Pytorch: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCT........</code>错误</p>
<p>在新增了一个类别进入训练集之后（5+1）出现了非常奇怪的CUDA报错</p>
<p>这个错误原因一般是①网络输出的label和标签（训练集种类）数量应该相同</p>
<p>②是否存在-1标签</p>
<p>我的最后一层应该<code>nn.Linear(32768, 5)</code>改为<code>nn.Linear(32768, 6)</code></p>
<p>很容易忽略很隐蔽的错误！而且这个错误定位并不准。找了我半天时间，</p>
</li>
<li><p><code>DataLoader worker (pid 20991) is killed by signal: Killed.</code></p>
<p>检查内存，CPU %MEM一直在增加，跑了很多次都同样出错</p>
<p><code>Out of memory</code>了</p>
<p>查阅了很多资料，github上有些人也遇到这个问题，有说是因为后台线程设置太多，也以后说enumerate写法问题，还有个说是pytorch的问题</p>
<blockquote>
<p> a lot of those issues are because of third party libraries not being fork safe. One alternative resolution might be to use the spawn start method.</p>
</blockquote>
<p>最后将子线程设置为1解决</p>
<p><code>N M B</code></p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>fixed不生效</title>
    <url>/2019/04/19/fixed%E4%B8%8D%E7%94%9F%E6%95%88/</url>
    <content><![CDATA[<p>&#160; &#160; &#160; &#160;在ios上，<code>background-attachment: fixed;</code>标签失效，导致背景图无法自适应屏幕。</p>
<p>&#160; &#160; &#160; &#160;解决办法：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">body</span><span class="selector-pseudo">:before</span> &#123;</span><br><span class="line">  <span class="attribute">content</span>: <span class="string">' '</span>;</span><br><span class="line">  <span class="attribute">position</span>: fixed;</span><br><span class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</span><br><span class="line">  <span class="attribute">top</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">right</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">bottom</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">left</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">url</span>(/images/4.jpg) center <span class="number">0</span> no-repeat;</span><br><span class="line">  <span class="attribute">background-size</span>: cover;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>HTML/JS/CSS</category>
      </categories>
      <tags>
        <tag>Apple</tag>
      </tags>
  </entry>
  <entry>
    <title>塞尔达传说：荒野之息</title>
    <url>/2019/04/15/%E5%A1%9E%E5%B0%94%E8%BE%BE%E4%BC%A0%E8%AF%B4%EF%BC%9A%E8%8D%92%E9%87%8E%E4%B9%8B%E6%81%AF/</url>
    <content><![CDATA[<h2 id="传奇的诞生"><a href="#传奇的诞生" class="headerlink" title="传奇的诞生"></a>传奇的诞生</h2><p>&#160; &#160; &#160; &#160;这款游戏一出生，就轰动了整个游戏界。</p>
<p>&#160; &#160; &#160; &#160;几乎所有重要游戏媒体都给出满分评价，在mc（metacritic，综合所有游戏媒体给出综合平均分）上获取高达98分的平均分。在人类游戏史上排名第二（并列），毫无悬念地摘获了2017年由TGA（The Game Awards，游戏界的奥斯卡）颁发的GOTY（Game Of The Year，年度最佳游戏），同时斩获TGA最佳游戏设计、TGA最佳动作冒险游戏，GS年度最佳游戏，EDGE年度最佳游戏，GDC最佳游戏音效奖、最佳游戏设计奖和年度游戏奖，SXSW最佳游戏性奖、最佳游戏设计奖和年度最佳游戏。</p>
<p><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/1.jpg" alt="1"><br> <a id="more"></a></p>
<p>&#160; &#160; &#160; &#160;媒体，玩家，游戏界一致好评的背后，是游戏设计理念的极致，是任天堂的无数心血——<br><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/2.jpg" alt="2"></p>
<h2 id="重新定义开放世界"><a href="#重新定义开放世界" class="headerlink" title="重新定义开放世界"></a>重新定义开放世界</h2><p>&#160; &#160; &#160; &#160;公式化开放世界大行其道，玩家已经开始产生厌倦。公式化开放世界的意思就是流水化式地生产出一个开放世界，整个游戏的设计就有寥寥几种模式，而且这几种模式的变化很小，内容填充少。比如说任务类型就是跟踪、刺杀、跑路这几种，演出就几段毫无营养的对话和随便的故事背景。游戏内容看似很多，但其实大多都是重复（说你呢某球），玩家就在不断重复地玩游戏，像做作业一样慢慢清点清任务。<br>&#160; &#160; &#160; &#160;然而荒野之息打破了这种模式。那么任天堂是如何让玩家在开放世界中不感到乏味呢？</p>
<h2 id="互动与自由"><a href="#互动与自由" class="headerlink" title="互动与自由"></a>互动与自由</h2><p><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/3.jpg" alt="3"><br>&#160; &#160; &#160; &#160;无数的互动，你想得到的想不到的，让塞尔达这个世界仿佛和真实世界一般自然。<br>&#160; &#160; &#160; &#160;塞尔达中的互动实在过多，这里以火举例子。<br>&#160; &#160; &#160; &#160;在塞尔达中，你可以用木棒去引火而得到火，也可以靠铁质的武器打打火石来生火。火可以用来作为照明，也可以用来点燃一切草木以及木材做的东西（也很容易不小心就点燃到易燃物）。在寒冷之地，火可以暖身子，融化冰块，可以生火来烤熟食物。<br>&#160; &#160; &#160; &#160;你以为这样就足够了吗？<br>&#160; &#160; &#160; &#160;在荒野之息中，火还具有传递性。如果你在森林里放一把火，那可能你扭头一会儿，会发现一片森林被你烧了。如果此时有大风吹过的话，火势会蔓延地更快（放火烧山牢底坐穿（））<br>&#160; &#160; &#160; &#160;不仅如此，如果你点燃了大片的草地，可以制造出上升气流，这时候你打开降落伞，就可以随着上升气流飞到空中（然后就可以空中偷袭敌人，或者说到高处去（当然塞尔达的任何悬崖峭壁都可以直接攀爬））<br>&#160; &#160; &#160; &#160;不仅如此，你甚至可以通过火苗的摆动来判别风向和风力，借此来判断如何射弓箭能够射的更准（塞尔达的弓箭会根据被风向和风力干扰，会受重力干扰）。如果你的弓箭上带火焰，那被弓箭射杀的动物将会直接被烤熟（然后就可以直接吃，塞尔达的很多肉类不能生吃需要料理加工）<br>&#160; &#160; &#160; &#160;不仅如此，如果敌人举着木盾，你拿个火把（火在武器上燃火或火剑）甚至能够将敌人的木盾给烧起来，当然如果你自己的武器烧起来也会被烧成灰烬。<br>&#160; &#160; &#160; &#160;以上仅仅为举了一个小小的火为例，荒野之息的世界中还有无数多的细节和互动，比如天气系统，冷热饥饿，敌人的AI，料理系统等等…<br>&#160; &#160; &#160; &#160;仿佛这个世界中的所有的东西之间都能互相影响，各种物理化学原理，在荒野之息显得格外真实。</p>
<h2 id="探索"><a href="#探索" class="headerlink" title="探索"></a>探索</h2><p>&#160; &#160; &#160; &#160;对于开放世界游戏来说，如何让玩家获得探索的乐趣极为重要。荒野之息在这点上可谓做到了精妙的地步。<br>&#160; &#160; &#160; &#160;对于荒野之息探索与内容设计这块，可以从非常专业的角度来分析，比如参考这篇文章<a href="https://www.iyingdi.com/web/article/search/62084?remark=user" target="_blank" rel="noopener">https://www.iyingdi.com/web/article/search/62084?remark=user</a> 这里我仅仅以玩家的角度来举例说明————<br>&#160; &#160; &#160; &#160;荒野之息给人的感觉就是一个时时刻刻都充满未知的游戏。<br>&#160; &#160; &#160; &#160;你可能在路上看到个蔬菜，然后自己放点配菜瞎炒一下，竟然做出了一道新菜。<br>&#160; &#160; &#160; &#160;你可能走着走着天空中突然出现了一条雷龙，俯冲过你头上，你甚至还能攻击它。<br>&#160; &#160; &#160; &#160;你可能发现一座无名小山，你好奇的上去看，竟然在上面发现了一个花园。<br>&#160; &#160; &#160; &#160;你可能不小心把农民的地烧了，回头发现农民伤心的哭了..<br>&#160; &#160; &#160; &#160;你可能发现你把骷髅兵的头打掉了之后，骷髅兵到处乱跑 找到了自己的头又装上了<br>&#160; &#160; &#160; &#160;……<br>&#160; &#160; &#160; &#160;如此种种，实在是太多了无法列举。而这些，没有人强迫你去看，他们都等着你自己主动（一不小心）就发现。所以在塞尔达的世界里，想做什么就去做吧，不要被传统游戏的经验所束缚，你会收获很多惊喜。<br>&#160; &#160; &#160; &#160;在旅途中，你将会一直接触新鲜事物，即使你游山玩水慢慢打，100多个小时你也会全程保持新鲜和探索欲望。<br><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/4.jpg" alt="4"></p>
<h2 id="关卡设计"><a href="#关卡设计" class="headerlink" title="关卡设计"></a>关卡设计</h2><p>任天堂的脑洞无可质疑，过于牛逼，无法表达。</p>
<h2 id="美术风格"><a href="#美术风格" class="headerlink" title="美术风格"></a>美术风格</h2><p><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/5.jpg" alt="5"><br><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/6.jpg" alt="6"><br><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/7.jpg" alt="7"><br><img src="//aisakaki.com/2019/04/15/塞尔达传说：荒野之息/8.jpg" alt="8"><br>&#160; &#160; &#160; &#160;看图，过于美丽，无法表达。</p>
<h2 id="解密"><a href="#解密" class="headerlink" title="解密"></a>解密</h2><p>&#160; &#160; &#160; &#160;如果你是塞尔达系列的老玩家，你期盼一个比前作更强的解密的话，你可能会略微失望，荒野之息的大迷宫只有4个，小迷宫倒是有120个遍布世界各地，谜题的难度和设计上确实是没有前作塞尔达厉害，毕竟荒野之息相比前作塞尔达是完全的革新。但是与解密游戏来说，已经属于极为优秀了。</p>
<h2 id="音乐"><a href="#音乐" class="headerlink" title="音乐"></a>音乐</h2><p>&#160; &#160; &#160; &#160;大部分曲目采用了钢琴为主，但所有曲目的共同点就是，如同荒野之息这个游戏一样，给人以如同看到辽阔无人的荒野一般的感受。与环境配合恰到好处。对于音乐部分也可以开一个专栏赏析，这里就不作过多分析。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&#160; &#160; &#160; &#160;<strong>8说了，说就是天下第一。</strong></p>
<p><em>此文同步发布在社团公众号</em></p>
]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>天下第一</tag>
      </tags>
  </entry>
  <entry>
    <title>古都</title>
    <url>/2019/04/12/%E5%8F%A4%E9%83%BD/</url>
    <content><![CDATA[<blockquote>
<p>“对美的幻影，总没有厌倦的时候吧。幻影是不能践踏的，践踏了只能自食其果。”</p>
<p align="right">——《古都》 川端康成 </p>

</blockquote>
<p>&#160; &#160; &#160; &#160;川端先生不愧是日本传统美学的集大成者。</p>
<p>&#160; &#160; &#160; &#160;18~20岁是第一次真正的感受到半遮面的现实的年纪。这些脆弱而细腻，复杂但单纯的情感，在川端先生的笔下如泉水一般汨汨而出，交织出一首含蓄的诗。好像每一句话都氤氲着回音。</p>
<p>&#160; &#160; &#160; &#160;自在飞花轻似梦，无边细雨思如愁。</p>
<p>&#160; &#160; &#160; &#160;在最美好的年纪，一切都终将会消散。纵使韶华易逝，但美犹存。是幻影，是执念。</p>
<p>&#160; &#160; &#160; &#160;无论是千重子，还是雪，亦或是古都——</p>
]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>川端康成</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2Vec</title>
    <url>/2019/04/10/%E8%AF%8D%E5%90%91%E9%87%8F/</url>
    <content><![CDATA[<h2 id="什么是词向量"><a href="#什么是词向量" class="headerlink" title="什么是词向量"></a>什么是词向量</h2><p>一种词的表示方法。通常NLP中使用one-hot或词向量表示</p>
<p>one-hot即为一维一词，占用空间大，且无法体现词之间的关联性</p>
<p>于是使用词向量可以包含更多信息，且使得向量在空间的表示更稠密。</p>
<h2 id="如何生成词向量"><a href="#如何生成词向量" class="headerlink" title="如何生成词向量"></a>如何生成词向量</h2><h3 id="基于统计"><a href="#基于统计" class="headerlink" title="基于统计"></a>基于统计</h3><p>略。</p>
<h3 id="语言模型生成词向量"><a href="#语言模型生成词向量" class="headerlink" title="语言模型生成词向量"></a>语言模型生成词向量</h3><ul>
<li><p>NNLM（neural network language model）</p>
<p><img src="//aisakaki.com/2019/04/10/词向量/0.png" alt="0"></p>
<p>目标函数</p>
<p>$L=\frac{1}{T}\Sigma^T_{t=1}f(w_t,w_{t-1},…,w{t-n+1})$</p>
<p>这个函数即目标将每一步的文档集的平均输出概率最大化。</p>
<p>但是这个模型在softmax层占用过多资源（与词表长度成正比）。</p>
<p>这个模型虽然现在不使用了，但是却提出了一个很有意义的思路，embedding层，中间层，softmax层（除外）。后面word2vec会详细展开。</p>
</li>
<li><p>C&amp;W</p>
<p>基于NNLM改变了目标函数，$L=max(0,1-(g_{\theta,E}(s)-g_{\theta,E}(z)))$</p>
<p>原NNLM 模型的目标是构建一个语言概率模型 ，而 C&amp;W 则是以生成词向量为目标的模型。 在 NNLM 模型的求解中，最费时的部分当属隐藏层到输出层的权重计算(softmax层)。由于C&amp;W 模型没有采用语言模型的方式去求解词语上下文的条件概率，而是直接对 n 元短语打分，这是一种更为快速获取词向量的方式。C&amp;W 模型的核心机理是 : 如果 n 元短语在语料库中出现过，那么模型会给该短语打高分;如果是未出现在语料库中的短语则会得到较低的评分，将这两者的输出值差异最大化。</p>
</li>
<li><p><strong>Word2Vec</strong></p>
<p>其中包含①CBoW ②skip gram</p>
<ul>
<li><p>CBoW</p>
<p>输入为<strong>上下文（窗口）的one-hot</strong><br>比如我们要预测 <em>i love eating coffee</em>里的<em>eating</em><br>则one hot编码： <em>i</em> :0001  <em>love</em>:0010  <em>eating</em>:0100  <em>coffee</em>:1000<br>我们要输入eating的前后文（选择窗口），那么输入网络的就是 0001,0010,1000（对应三个块）</p>
<p><img src="//aisakaki.com/2019/04/10/词向量/word2vec.png" alt="word2vec"></p>
<p>中间那块儿projection layer得到的向量就是词向量，训练矩阵，我们保存了$W$矩阵（训练得到的权重）之后，以后再输入上下文one-hot就能得到对应的词向量了。</p>
<p><strong>由输入层到映射层，降低维度的过程($k \to N$)，就是将one-hot转变为词向量的过程，也就是 word embedding（词嵌入）</strong><br><strong>之所以词向量能够保留上下文信息，就是因为在训练输入的时候，输入了其上下文信息</strong></p>
</li>
<li><p>skip gram</p>
<p>  输入为一个词的one-hot，输出为多个词的one-hot（上下文），然后BP训练即可，相当于CBoW结构反过来，学的依然是$W$权重</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>词向量</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>百年孤独</title>
    <url>/2019/04/09/%E7%99%BE%E5%B9%B4%E5%AD%A4%E7%8B%AC/</url>
    <content><![CDATA[<blockquote>
<p>“多年以后，面对行刑队，奥雷里亚诺·布恩迪亚上校将会回想起父亲带他去见识冰块的那个遥远的下午。”</p>
<p>“同一血脉的两个孤独者之间的接近与友谊无涉，却有助于他们承受将两人分离又联合的神秘孤独。”</p>
<p>“奥雷里亚诺平生从未像此刻一般清醒，他忘却了家中的死者，忘却了死者的痛苦，用费尔南达留下的十字木条再次钉死门窗，远离世间一切干扰，因为他知道梅尔基亚德斯的羊皮卷上记载着自己的命运。他发现史前的植物、湿气蒸腾的水洼、发光的昆虫已将房间内一切人类踪迹消除净尽，但羊皮卷仍安然无恙。他顾不得拿到光亮处，就站在原地，仿佛那是用卡斯蒂利亚语写就，仿佛他正站在正午明亮的光线下阅读，开始毫不费力地大声破译。那是他家族的历史，连最琐碎的细节也无一遗漏，百年前由梅尔基亚德斯预先写出。他以自己的母语梵文书写，偶数行套用奥古斯都大帝的私人密码，奇数行择取斯巴达的军用密码。而最后一道防线，奥雷里亚诺在迷上阿玛兰妲·乌尔苏拉时就已隐隐猜到，那便是梅尔基亚德斯并未按照世人的惯常时间来叙述，而是将一个世纪的日常琐碎集中在一起，令所有事件在同一瞬间发生。奥雷里亚诺为这一发现激动不已，逐字逐句高声朗读教皇谕令般的诗行，当年阿尔卡蒂奥曾从梅尔基亚德斯口中听闻，却不知道那是关于自己死亡的预告。他读到羊皮卷中预言世上最美的女人的诞生，她的灵魂与肉身正一起向天飞升；他读到那对遗腹孪生子的来历，他们放弃破译羊皮卷不仅因为缺乏才能和毅力，更是因为时机尚未成熟。读到这里，奥雷里亚诺急于知道自己的身世，跳过几页。此时微风初起，风中充盈着过往的群声嘁喳，旧日天竺葵的昵喃窸窣，无法排遣的怀念来临之前的失望叹息。他对此毫无察觉，因为他发现了关于自己身世的初步线索。他读到一位好色的祖父一时迷了心窍穿越幻象丛生的荒野，寻找一个不会令他幸福的美女。奥雷里亚诺认出了他，沿着亲缘的隐秘小径追寻下去，找到了自己被赋予生命的一刻，那是在一间昏暗的浴室里，蝎子和黄蝴蝶的环绕间，一个工匠在一个因反叛家庭而委身于他的少女身上满足了欲望。他读得如此入神，仍未发觉风势又起，飓风刮落了门窗，掀掉了东面长廊的屋顶，拔出了房屋的地基。到这时，他才发现阿玛兰妲·乌尔苏拉不是他的姐妹，而是他的姨妈，而当年弗朗西斯·德雷克袭击里奥阿查不过是为了促成他们俩在繁复错综的血脉迷宫中彼此寻找，直到孕育出那个注定要终结整个家族的神话般的生物。当马孔多在《圣经》所载那种龙卷风的怒号中化作可怕的瓦砾与尘埃旋涡时，奥雷里亚诺为避免在熟知的事情上浪费时间又跳过十一页，开始破译他正度过的这一刻，译出的内容恰是他当下的经历，预言他正在破解羊皮卷的最后一页，宛如他正在会言语的镜中照影。他再次跳读去寻索自己死亡的日期和情形，但没等看到最后一行便已明白自己不会再走出这房间，因为可以预料这座镜子之城——或蜃景之城——将在奥雷里亚诺·巴比伦全部译出羊皮卷之时被飓风抹去，从世人记忆中根除，羊皮卷上所载一切自永远至永远不会再重复，因为注定经受百年孤独的家族不会有第二次机会在大地上出现。”</p>
</blockquote>
<p>​     “生命从来不曾离开过孤独而独立存在。无论是我们出生、我们成长、我们相爱、还是我们成功失败，直到最后的最后，孤独犹如影子一样存在于生命一隅。”<br>​    百年孤独。</p>
]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch中自定义Dataset和Dataloader总结</title>
    <url>/2019/04/05/Pytorch%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89Dataset%E5%92%8CDataloader/</url>
    <content><![CDATA[<h2 id="Dataset自定义数据集与DataLoader-数据集加载器"><a href="#Dataset自定义数据集与DataLoader-数据集加载器" class="headerlink" title="Dataset自定义数据集与DataLoader 数据集加载器"></a>Dataset自定义数据集与DataLoader 数据集加载器</h2><p><strong>Dataset定义了数据集的存在，其中包含了每个数据的路径和标签</strong>,<em>dataset类型不是拿来保存的！它只是相当于一个数据集索引器！并没有加载数据或封装数据！</em></p>
<p><strong>Dataloader则定义了指定数据集的加载方法</strong></p>
<p><strong>在对Dataloader进行迭代的时候，Dataset指向的数据集才正式被加载</strong></p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>Dataset 定义并加载自己的数据集<br><strong>继承自Dataset</strong>并<strong>必须重写三个函数</strong>：<br><code>__init__(self, )</code>：将任何需要的初始化,比如读取全部数据路径等，不限制参数个数<br><code>__getitem__(self,index)</code>：根据每次调用时的index返回对应元素和标签<br><code>__len__(self)</code>：负责返回数据集中的元素个数</p>
<p>以加载图片数据集为例<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="comment">#结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="comment">#self, 代表可以补充其他自定参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,  )</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#返回最大长度</span></span><br><span class="line">        <span class="keyword">return</span> len</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,index)</span>:</span></span><br><span class="line">        <span class="comment">#返回每次应读取的单个数据</span></span><br><span class="line">        <span class="keyword">return</span> data,label</span><br><span class="line"></span><br><span class="line"><span class="comment">#例子</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,root,transform=None)</span>:</span></span><br><span class="line">        <span class="comment"># 所有图片的绝对路径</span></span><br><span class="line">        imgs=os.listdir(root)</span><br><span class="line">        <span class="comment">#这句话可以使用glob快速加载 见66.</span></span><br><span class="line">        self.imgs=[os.path.join(root,k) <span class="keyword">for</span> k <span class="keyword">in</span> imgs]</span><br><span class="line">        self.transforms=transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img_path = self.imgs[index]</span><br><span class="line">        pil_img = Image.open(img_path)</span><br><span class="line">        pil_img = pil_img.convert(<span class="string">"RGB"</span>)</span><br><span class="line">    <span class="keyword">if</span> self.transforms:</span><br><span class="line">            data = self.transforms(pil_img)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        pil_img = np.asarray(pil_img)</span><br><span class="line">            data = torch.from_numpy(pil_img)</span><br><span class="line">            label = xxxxx(这里省略，总之是得到这个图的标签）</span><br><span class="line">        <span class="keyword">return</span> data,label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建数据集实例并初始化</span></span><br><span class="line">dataSet=FlameSet(<span class="string">'./test'</span>,transform = transform)</span><br><span class="line"><span class="comment">#依然用Dataloader加载数据集</span></span><br><span class="line">data = torch.utils.data.DataLoader(myDataset,batch_size=BATCH_SIZE,shuffle=<span class="keyword">True</span>,num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>DataLoader将数据集对象和不同的取样器联合，如<code>SequentitalSampler</code>和<code>RandomSampler</code>，并使用单进程或多线程的迭代器，为我们提供批量数据。取样器是为算法提供数据的不同策略。<br><code>dataloader = torch.utils.data.DataLoader(trainSet,batch_size=BATCH_SIZE,shuffle=True，num_workers=0,collate_fn=fn)</code><br>Dataloader的储存数据形式是一个batch一个batch存，取也是一个batch一个batch取，每组数据的内容分别为<strong>一组</strong>batch的input和该组batch每个数据对应的label<strong>组</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> phase:</span><br><span class="line">    inputs,label = data</span><br></pre></td></tr></table></figure>
<p> 注意: <code>datalodaer.__len__()</code>得到的是<strong>batch（分组）总数</strong>而不是数据总数<br><code>dataset.__len__</code>得到的是数据总数</p>
<p><strong>自定义DataLoader：</strong> </p>
<p><strong>collate_fn</strong>是非常重要的，如nlp中经常在里面做padding<br>collate_fn是<strong>自定义函数来设计数据收集的方式</strong>，意思是已经通过上面的Dataset类中的<strong>getitem</strong>函数采样了batch_size数据，以一个包的形式传递给collate_fn所指定的函数<br>collate_fn的输入是一个list，list的长度是一个batch_size，list中的每个u岸数都是<strong>getitem</strong>得到的结果</p>
<p><strong>以我一次项目中加载NLP数据集写的代码为例：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成datasets</span></span><br><span class="line"><span class="comment"># 不进行转置，直接在网络中输入的时候调用RNN的函数进行转置</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textDataset</span><span class="params">(dataset.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data,label)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">        self.label = label</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    <span class="comment">#在取数据的时候调用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,index)</span>:</span></span><br><span class="line">        data = torch.Tensor(self.data[index])</span><br><span class="line">        <span class="comment"># 这里是在getitem的时候才会转化为long</span></span><br><span class="line">        <span class="comment">#注意这里在转tensor的时候必须得是list格式，然后转成了之后再取出第0项即为单tensor元素</span></span><br><span class="line">        label = torch.LongTensor([self.label[index]])[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> data,label</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(batch_data)</span>:</span></span><br><span class="line">    max_seq_size = <span class="number">30</span> <span class="comment">#最大句长 （看清楚这里处理的句子是，分割or。分割）</span></span><br><span class="line">    vec_size = <span class="number">200</span></span><br><span class="line">    <span class="comment">#经过dataloader调用__getitem__方法从dataset取得的数据是这样的形式(dataset[0],dataset[1],xxxx)一个batch长度，</span></span><br><span class="line">    <span class="comment">#也就是相当于((data0,label0),(data1,label1),xxx)，所以用zip分别提取出来形成(data,label)</span></span><br><span class="line">    <span class="comment">#zip(*zipped) 是逆zip    </span></span><br><span class="line">    datas,labels = zip(*batch_data)</span><br><span class="line">    </span><br><span class="line">    corpus_vec_resize = list()</span><br><span class="line">    <span class="comment">#遍历每一行（每一个seq）</span></span><br><span class="line">    <span class="keyword">for</span> corpus_vec <span class="keyword">in</span> datas:</span><br><span class="line">        <span class="comment">#这里对每个data中的句子做padding   #【也可以使用掩码的方式做padding，生成掩码矩阵 应该有更简单的方法或工具】</span></span><br><span class="line">        <span class="comment">#将处理后的全部转为Tensor，如果转成narray再弄会出错</span></span><br><span class="line">        <span class="comment">#若长度过小,则从句尾开始填充</span></span><br><span class="line">        <span class="keyword">if</span> len(corpus_vec) &lt; max_seq_size:</span><br><span class="line">            pad_num = max_seq_size-len(corpus_vec) </span><br><span class="line">            padding = list([[<span class="number">0</span>]*vec_size]*pad_num)</span><br><span class="line">            seq_padded = np.append(corpus_vec,padding,axis=<span class="number">0</span>)</span><br><span class="line">            seq_padded = torch.Tensor(seq_padded)</span><br><span class="line">            corpus_vec_resize.append(seq_padded)</span><br><span class="line">            <span class="comment">#若长度过长,则依次从句头和句末开始修剪</span></span><br><span class="line">        <span class="keyword">if</span> len(corpus_vec) &gt; max_seq_size:</span><br><span class="line">            cut_num = len(corpus_vec)-max_seq_size</span><br><span class="line">            <span class="comment">#句头剪cut_num//2 句尾剪cut-num-cut_num//2</span></span><br><span class="line">            seq_cut = corpus_vec[cut_num//<span class="number">2</span>:len(corpus_vec)-(cut_num-cut_num//<span class="number">2</span>)] </span><br><span class="line">            seq_cut = torch.Tensor(seq_cut)</span><br><span class="line">            corpus_vec_resize.append(seq_cut)</span><br><span class="line">        <span class="keyword">if</span> len(corpus_vec) == max_seq_size:</span><br><span class="line">            corpus_vec = torch.Tensor(corpus_vec)</span><br><span class="line">            corpus_vec_resize.append(corpus_vec)   </span><br><span class="line">    <span class="comment">#将datas转变为tensor   </span></span><br><span class="line">    datas = torch.stack(corpus_vec_resize) </span><br><span class="line"></span><br><span class="line">    <span class="comment">#合并处理labels</span></span><br><span class="line">    label_res = list()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> labels:</span><br><span class="line">        <span class="comment">#处理打错的标签(非0和1)</span></span><br><span class="line">        <span class="keyword">if</span> i.item()!=<span class="number">0</span> <span class="keyword">and</span> i.item()!=<span class="number">1</span>:</span><br><span class="line">            label_res.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label_res.append(i.item())</span><br><span class="line">    labels = torch.LongTensor(label_res)</span><br><span class="line"><span class="comment">#     print(labels)</span></span><br><span class="line"><span class="keyword">return</span> datas,labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#封装与加载</span></span><br><span class="line">corpus_dataset = textDataset(data=corpus_vec,label=labels)</span><br><span class="line"><span class="comment">#这里定义了加载的具体方法：preprocess</span></span><br><span class="line">textDataloader = dataloader.DataLoader(corpus_dataset,batch_size=batch_size,shuffle=<span class="keyword">False</span>,num_workers=<span class="number">1</span>,collate_fn=preprocess)</span><br></pre></td></tr></table></figure>
<p>dataloader参数的更多详细资料还可以参考下面资料：</p>
<p><a href="https://www.jianshu.com/p/8ea7fba72673" target="_blank" rel="noopener">https://www.jianshu.com/p/8ea7fba72673</a><br><a href="https://www.jianshu.com/p/bb90bff9f6e5" target="_blank" rel="noopener">https://www.jianshu.com/p/bb90bff9f6e5</a></p>
<p><a href="https://blog.csdn.net/weixin_30241919/article/details/95184794" target="_blank" rel="noopener">https://blog.csdn.net/weixin_30241919/article/details/95184794</a></p>
<p><strong>dataloader的num_work子线程尽量数量开多点！不要让大量时间用去读数据.一般子线程设和核的个数相同</strong></p>
<h2 id="getitem"><a href="#getitem" class="headerlink" title="getitem()"></a><strong>getitem</strong>()</h2><p>如果类中定义了<strong>getitem</strong>()方法,那么它的实例对象（假设为P）就可以P[Key]这样取值。当实例对象做P[Key]运算的时候就会调用<strong>getitem</strong>()方法，返回的就是这个方法的return</p>
<p>dataset里有这个方法，当dataloader加载dataset的时候就会自动调用</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>数据</tag>
      </tags>
  </entry>
  <entry>
    <title>重新打理BLOG</title>
    <url>/2019/03/27/%E9%87%8D%E6%96%B0%E6%89%93%E7%90%86BLOG/</url>
    <content><![CDATA[<p>前段时间因为繁忙，没时间打理个人网站</p>
<p>现在打算开始整理一下，逐步也把评论系统，目录之类的插件给搞定</p>
<p>把绝大部分日记都迁出去了，以后这里主要是..啥都有</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>HEXO遇到的坑</title>
    <url>/2019/03/27/HEXO%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<ol>
<li><p>source文件夹下面新建一个叫CNAME的文件填上网址</p>
<p>才能由自定义域名访问自己的github.io</p>
</li>
<li><p>安装了图片插件但是图片依然无法显示问题</p>
<p>①可能是图片路径出错导致</p>
<p>要在_config.yml中修改url为自己的域名！</p>
<p>②后缀名要注意大小写！ 路径和名称大小写要统一！</p>
</li>
<li><p>hexo改标题出现乱码</p>
<p><code>language: zh-Hans</code></p>
</li>
<li><p>配置文件要保存为UTF-8格式，不然会乱码</p>
</li>
<li><p>语法上的坑</p>
<ul>
<li>HEXO 表格与正文之间需要两个空行</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
      </tags>
  </entry>
  <entry>
    <title>python相关笔记</title>
    <url>/2019/03/27/python%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><em>迄今为止在python上遇到的坑，超长预警</em></p>
<a id="more"></a>
<ol>
<li><p><code>with open</code>的使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'/path/to/file'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">print(f.read())</span><br></pre></td></tr></table></figure>
<p>等效于</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = open(<span class="string">'/path/to/file'</span>, <span class="string">'r'</span>)</span><br><span class="line">    print(f.read())</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="keyword">if</span> f:</span><br><span class="line">        f.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>os.listdir(path)</code> 返回path路径下的所有文件和文件夹列表</p>
</li>
<li><code>os.path.join(path1,str)</code> 将两个字符串合并生成一个新路径</li>
<li><p>列表解析功能<br> 比如<code>b =[i+2 for i in a]</code><br> <code>apk_dirs = [os.path.join(BASE_PATH, name) for name in names]</code></p>
</li>
<li><p><code>list</code>用于将元组转换为列表<br> 元祖不可更改，列表可以更改。列表对应一大批操作函数。</p>
</li>
<li><p>通过这种方式可以遍历打开Path路径下的所有文件<br> 注意冒号</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(Path):</span><br><span class="line"><span class="keyword">with</span> open(os.path.join(Path, file), <span class="string">'r'</span>) <span class="keyword">as</span> f</span><br><span class="line"><span class="string">'''熟悉用python操作文件路径 熟悉使用2 3 for in等'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Python JSON<br><a href="http://www.runoob.com/python/python-json.html" target="_blank" rel="noopener">http://www.runoob.com/python/python-json.html</a><br><code>json.dumps</code> 将 Python 对象编码成 JSON 字符串<br><code>json.loads</code> 将已编码的 JSON 字符串解码为 Python 对象</p>
</li>
<li><p>字典<code>{</code><br>元组<code>(</code><br>列表<code>[</code></p>
</li>
<li>经常用<code>a.append(1)</code>来代表一位记录特征</li>
<li><code>shutil.copy(source, destination)</code><br>将source路径的文件复制为destination路径的文件（相当于cp）</li>
<li><code>matplotlib.pyplot</code> 用于画画的库</li>
<li><code>dict.value()</code> 方法<br>返回所有的字典值组成的列表</li>
<li><code>index()</code> 函数用于从列表中找出某个值第一个匹配项的索引位置。</li>
<li>jupyter可以方便选择哪几个指定模块来执行 而不用执行整个文件</li>
<li><p>python目录与文件操作（部分）</p>
<ul>
<li><code>os.path.exists(path)</code> 判断一个目录是否存在</li>
<li><code>os.makedirs(path)</code> 多层创建目录</li>
<li><code>os.mkdir(path)</code> 创建目录</li>
<li><code>shutil.copy(fileA, fileB)</code> 将路径文件A复制到路径文件B。注意如果fileB路径的中间路径并不存在，需要建立中间的目录！（手动or用os.makedirs）</li>
</ul>
</li>
<li><p>混淆矩阵<br><code>sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)</code><br>y_true: 是样本真实分类结果，y_pred: 是样本预测分类结果<br>labels：是所给出的类别，通过这个可对类别进行选择<br>sample_weight:样本权重<br>后两者可以没有<br>详细用法见 <a href="https://blog.csdn.net/m0_38061927/article/details/77198990" target="_blank" rel="noopener">https://blog.csdn.net/m0_38061927/article/details/77198990</a></p>
</li>
<li><p><a href="https://blog.csdn.net/m0_37518259/article/details/80658131" target="_blank" rel="noopener">https://blog.csdn.net/m0_37518259/article/details/80658131</a></p>
</li>
<li><p>读取文件的编码问题，字符串编码问题 超级大坑<br>超级大坑！！！！！<br><a href="https://stackoverflow.com/questions/956867/how-to-get-string-objects-instead-of-unicode-from-json" target="_blank" rel="noopener">https://stackoverflow.com/questions/956867/how-to-get-string-objects-instead-of-unicode-from-json</a></p>
</li>
<li><p>python格式化输出<br><code>print (&quot;Name:%10s Age:%8d Height:%8.2f&quot;%(&quot;Aviad&quot;,25,1.83))</code></p>
</li>
<li><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#in   not in </span></span><br><span class="line"><span class="comment">#正确写法</span></span><br><span class="line">key = <span class="string">"aaa"</span></span><br><span class="line">lt = [<span class="string">"aaa"</span>,<span class="string">"bbb"</span>,<span class="string">"vvv"</span>]</span><br><span class="line"><span class="keyword">if</span> key <span class="keyword">in</span> lt:</span><br><span class="line">    print(<span class="string">"list is true"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"list is false"</span>)</span><br><span class="line">&gt;&gt;&gt;list <span class="keyword">is</span> true</span><br><span class="line"></span><br><span class="line"><span class="comment">#错误写法1：key写成一个列表。 in语法是要一个元素in一个列表</span></span><br><span class="line">key = [<span class="string">"aaa"</span>]</span><br><span class="line">lt = [<span class="string">"aaa"</span>,<span class="string">"bbb"</span>,<span class="string">"vvv"</span>]</span><br><span class="line"><span class="keyword">if</span> key <span class="keyword">in</span> lt:</span><br><span class="line">    print(<span class="string">"list is true"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"list is false"</span>)</span><br><span class="line">&gt;&gt;&gt;list <span class="keyword">is</span> false</span><br><span class="line"></span><br><span class="line"><span class="comment">#错误写法2：if a in b is True。 不能用is True。原因不明（？）</span></span><br><span class="line">key = <span class="string">"aaa"</span></span><br><span class="line">lt = [<span class="string">"aaa"</span>,<span class="string">"bbb"</span>,<span class="string">"vvv"</span>]</span><br><span class="line"><span class="keyword">if</span> key <span class="keyword">in</span> lt <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">    print(<span class="string">"list is true"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"list is false"</span>)</span><br><span class="line">&gt;&gt;&gt;list <span class="keyword">is</span> false</span><br></pre></td></tr></table></figure>
</li>
<li><p>有时候python无法判断该数据为数值or字符串。<br>所以在输出的时候，有些无法分清的情况需要<code>print(str(key))</code></p>
</li>
<li><p>python的函数参数传递（实参和形参）<br>在函数体内修改参数内容会影响到函数外的对象吗？<br>（1）如果数字、字符串或元组，本身就是不可变的，自然也不会影响到函数体外的对象<br>（2）如果是列表或字典，那么函数内修改参数内容，就会影响到函数体外的对象。<br>备注：这里的修改参数是修改参数对象内部的值，不是赋值哦。即var[1]=’hello’和var=[‘hello’,’world’]区别。即使是列表，在函数体内对参数重新赋值了，不会影响到函数体外的对象哦。注意和c++的比较。<br>相当于python函数参数传递是一个指针</p>
</li>
<li><p>字典添加新pair</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict =&#123;&#125;</span><br><span class="line">dict[<span class="string">'aaa'</span>]= <span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li>python中没有do while循环。可以由如下代码代替<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">	xxxx</span><br><span class="line">	<span class="keyword">if</span> condtion满足:</span><br><span class="line">		<span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>求列表元素个数<code>len(list)</code></p>
</li>
<li><p><code>for i in range(a,b)</code> 包含a不包含b</p>
</li>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for line in file:</span><br><span class="line">	line = line.spilt(&apos;,‘)</span><br><span class="line">这样读出来的列表元素全是字符串形式！</span><br><span class="line">而且 末尾的/n也会包含在最后一个字符里</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><code>str.strip（）</code>去掉字符串末尾字符（常用来去掉’/n’等</p>
</li>
<li><p><code>list</code>有序而<code>dict</code>无序</p>
</li>
<li><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total_list = sorted(total_dict.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> (key, value) <span class="keyword">in</span> total_list:</span><br></pre></td></tr></table></figure>
<p>python中字典是无序的，list是有序的<br>想要对字典或列表等排序需要借助<code>sort</code>函数。然而由于字典是无序的，经<code>sort</code>函数操作之后会变成一个列表。如果直接使用<code>sort(dict.values)</code>的话，将生成一个只由字典的value构成的排序后的列表，如果<code>sort(dict.keys)</code>的话同理为只由键组成的排序后的列表。如果要输出键值对的话，就按照上面的代码，转化为了有序元组列表的形式。<br><code>lambda</code>表示语法形式<br><code>lambda x: x[1]</code>中<code>x[0]</code>表示对键进行排序，<code>x[1]</code>表示对值进行排序<br><code>sort</code>函数默认是从小到大排序，如果要从大到小则添加参数<code>reverse=True</code><br>【以上内容可继续探索】</p>
</li>
<li><p>python3 中 <code>\</code>即为直接除法</p>
</li>
<li><p>判断类型是 <code>type(a) is dict</code><br>不是 <code>a is dict</code>！</p>
</li>
<li><p>字典无法在遍历中动态改变，而类别可以</p>
</li>
<li><p>Debug:Error loading notebook An unknown error occurred while loading this notebook.<br>这个错误的弥补方法是修改~/.local/share/jupyter文件夹的权限：<br><code>sudo chmod -R 777 ~/.local/share/jupyter</code></p>
</li>
<li><p>if os.path.splitext(resPath + file)[1] == ‘.jpg’ </p>
</li>
<li><p><code>ls -l filename|grep total</code><br>统计文件夹下文件个数</p>
</li>
<li><p>递归删除文件夹<br><code>shutil.rmtree(path=folderPath)</code><br>在shutil库下</p>
</li>
<li><p>python多进程默认不能共享全局变量<br><a href="https://blog.csdn.net/houyanhua1/article/details/78236514" target="_blank" rel="noopener">https://blog.csdn.net/houyanhua1/article/details/78236514</a><br>python多进程相当于将环境所有变量什么的给每一个进程复制一份，所以你在子进程修改了变量自然不会修改到全局</p>
</li>
<li><p>python子进程不报错 需要用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">print(ex)</span><br></pre></td></tr></table></figure>
<p>来捕获</p>
</li>
<li><p>重置索引用reset_index然后drop掉index列即可<br>reindex就是一坨屎</p>
</li>
<li><p>截取字符串操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">str = ‘0123456789’</span><br><span class="line">print str[0:3] #截取第一位到第三位的字符</span><br><span class="line">print str[:] #截取字符串的全部字符</span><br><span class="line">print str[6:] #截取第七个字符到结尾</span><br><span class="line">print str[:-3] #截取从头开始到倒数第三个字符之前</span><br><span class="line">print str[2] #截取第三个字符</span><br><span class="line">print str[-1] #截取倒数第一个字符</span><br><span class="line">print str[::-1] #创造一个与原字符串顺序相反的字符串</span><br><span class="line">print str[-3:-1] #截取倒数第三位与倒数第一位之前的字符</span><br><span class="line">print str[-3:] #截取倒数第三位到结尾</span><br><span class="line">print str[:-5:-3] #逆序截取，具体啥意思没搞明白？</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li>位运算<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.bitwise_and([<span class="number">7</span>],[<span class="number">3</span>])=<span class="number">3</span></span><br><span class="line">np.bitwise_or([<span class="number">7</span>],[<span class="number">3</span>])=<span class="number">7</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li>路径字符串分割<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">path.split(&apos;/&apos;)[-1]</span><br><span class="line">以/为分隔符，保留倒数第一段</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><code>dataFrame.to_csv(savePath,mode=&#39;a&#39;,header=0,index=False)</code></p>
</li>
<li><p>python list/字符串替换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Python replace() 方法把字符串中的 old（旧字符串/list） 替换成 new(新字符串/list)，如果指定第三个参数max，则替换不超过 max 次。</span></span><br><span class="line">str = str.replace(old, new[, max])</span><br><span class="line"><span class="comment"># 注意不会改变原始内容，要赋值回去！</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><strong>python多线程</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 由于每个python模块（python文件）都包含内置的变量__name__，当运行模块被执行的时候，__name__等于文件名（包含了后缀.py）。如果import到其他模块中，则__name__等于模块名称（不包含后缀.py）。而“__main__”等于当前执行文件的名称（包含了后缀.py）。所以当模块被直接执行时，__name__ == '__main__'结果为真；而当模块被import到其他模块中时，__name__ == '__main__'结果为假，就是不调用对应的方法。</span></span><br><span class="line"><span class="comment"># __name__ 是当前模块名，当模块被直接运行时模块名为 __main__ 。当模块被直接运行时，代码将被运行，当模块是被导入时，代码不被运行</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建线程对象</span></span><br><span class="line">pool = Pool(processes=k)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始执行线程内容 func即为我们每个线程要执行的东西，args为传入func函数参数</span></span><br><span class="line">pool.apply_async(func,args=(agr1,arg2,arg3))</span><br><span class="line"></span><br><span class="line"><span class="comment">#等待所有进程结束</span></span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li>常见迭代方法：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(list)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><code>os.path.getsize</code>函数的单位为字节B</p>
</li>
<li><p>正则表达式<br><a href="https://blog.csdn.net/u014535666/article/details/82972089#" target="_blank" rel="noopener">https://blog.csdn.net/u014535666/article/details/82972089#</a><br>注意python3中 .? .+等要用括号包起来！ <code>(.?)</code> <code>(.+)</code><br>且注意 findall返回的list元组！<br>python正则表达式常用函数的理解:<br><a href="https://www.cnblogs.com/papapython/p/7482349.html" target="_blank" rel="noopener">https://www.cnblogs.com/papapython/p/7482349.html</a></p>
</li>
<li><p>df.drop(x)  如果df.drop([等式]) 会出事！直接全没了！ 验证一下</p>
</li>
<li><p>linux语句返回输出<br><code>os.system()可以执行linux语句但无法获取输出</code><br><code>os.popen()</code>就可以获取命令行输出 ，返回的是一个文件<br>还要</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = os.popen(commend)</span><br><span class="line">result = r.read()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>python赋值、copy与deepcopy<br>list中套list得用deepcopy<br><a href="https://blog.csdn.net/u010712012/article/details/79754132" target="_blank" rel="noopener">https://blog.csdn.net/u010712012/article/details/79754132</a><br>赋值与深浅拷贝<br><a href="https://www.cnblogs.com/Eva-J/p/5534037.html" target="_blank" rel="noopener">https://www.cnblogs.com/Eva-J/p/5534037.html</a><br>python一切都是对象，赋值是传递引用</p>
</li>
<li><p>eval函数在python中做数据类型的转换还是很有用的。它的作用就是把数据还原成它本身或者是能够转化成的数据类型.<br>注意 为了安全性 还有个ast.literal_eval<br><a href="http://www.php.cn/python-tutorials-376459.html" target="_blank" rel="noopener">http://www.php.cn/python-tutorials-376459.html</a></p>
</li>
<li><p>xgboost版本为0.6a1的时候 对应scikit-learn的版本为0.19.0<br>版本要匹配！</p>
</li>
<li><p>弱智错误：<code>os.system(str)</code>里面只能是一个字符串，如果是要多段拼接是用加号不是逗号！<br><code>os.system(&quot;ps&quot;,&quot;-ef&quot;) 错误！</code><br><code>os.system(&quot;ps&quot;+&quot;-ef&quot;)正确！</code></p>
</li>
<li><p>python继承</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义类A</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#B继承A</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span><span class="params">(A)</span>:</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><code>if __name__ == &#39;__main__&#39;:</code></p>
</li>
<li><p><code>os.path.join</code><br>用这个组合路径更好！</p>
</li>
<li><p>format函数 (python格式化输出）<br><a href="https://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener">https://www.runoob.com/python/att-string-format.html</a><br>format函数输出可以不限制括号内的参数类型，转化为字符串输出<br>format 函数可以接受不限个参数，位置可以不按顺序。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"&#123;&#125; &#123;&#125;"</span>.format(<span class="string">"hello"</span>, <span class="string">"world"</span>)  <span class="comment"># 不设置指定位置，按默认顺序</span></span><br><span class="line">&gt;&gt;<span class="string">'hello world'</span></span><br><span class="line"><span class="string">"&#123;0&#125; &#123;1&#125;"</span>.format(<span class="string">"hello"</span>, <span class="string">"world"</span>)  <span class="comment"># 设置指定位置</span></span><br><span class="line">&gt;&gt;<span class="string">'hello world'</span></span><br><span class="line"><span class="string">"&#123;1&#125; &#123;0&#125; &#123;1&#125;"</span>.format(<span class="string">"hello"</span>, <span class="string">"world"</span>)  <span class="comment"># 设置指定位置</span></span><br><span class="line">&gt;&gt;<span class="string">'world hello world'</span></span><br></pre></td></tr></table></figure>
<p>也可以设置参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    print(<span class="string">"网站名：&#123;name&#125;, 地址 &#123;url&#125;"</span>.format(name=<span class="string">"菜鸟教程"</span>, url=<span class="string">"www.runoob.com"</span>))</span><br><span class="line">    <span class="comment"># 通过字典设置参数  </span></span><br><span class="line">    site = &#123;<span class="string">"name"</span>: <span class="string">"菜鸟教程"</span>, <span class="string">"url"</span>: <span class="string">"www.runoob.com"</span>&#125; </span><br><span class="line">    print(<span class="string">"网站名：&#123;name&#125;, 地址 &#123;url&#125;"</span>.format(**site))</span><br><span class="line">    <span class="comment"># 通过列表索引设置参数  </span></span><br><span class="line">    my_list = [<span class="string">'菜鸟教程'</span>, <span class="string">'www.runoob.com'</span>]  </span><br><span class="line">print(<span class="string">"网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;"</span>.format(my_list))  <span class="comment"># "0" 是必须的</span></span><br><span class="line">    </span><br><span class="line">    &gt;&gt;网站名：菜鸟教程,  地址 www.runoob.com </span><br><span class="line">    网站名：菜鸟教程,  地址 www.runoob.com </span><br><span class="line">    网站名：菜鸟教程,  地址 www.runoob.com</span><br></pre></td></tr></table></figure>
<p>也可以向 str.format() 传入对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="class"><span class="keyword">class</span>  <span class="title">AssignValue</span><span class="params">(object)</span>:</span> </span><br><span class="line">        <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span><span class="params">(self, value)</span>:</span> </span><br><span class="line">            self.value = value  </span><br><span class="line">    my_value = AssignValue(<span class="number">6</span>)  </span><br><span class="line">print(<span class="string">'value 为: &#123;0.value&#125;'</span>.format(my_value))  <span class="comment"># "0" 是可选的</span></span><br><span class="line">    </span><br><span class="line">    &gt;&gt;value 为:  <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>数字格式化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"&#123;:.2f&#125;"</span>.format(<span class="number">3.1415926</span>));  </span><br><span class="line">&gt;&gt;<span class="number">3.14</span></span><br></pre></td></tr></table></figure>
<p>更多用法列表见网页</p>
</li>
<li><p>可以这样用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'-'</span>*<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>除法<br><code>/</code> 一般除法<br><code>//</code>整除<br><code>%</code>求余</p>
</li>
<li><p><strong>plt画图</strong><br>导库<code>import matplotlib.pyplot as plt</code><br><a href="https://blog.csdn.net/majinlei121/article/details/83994935" target="_blank" rel="noopener">https://blog.csdn.net/majinlei121/article/details/83994935</a><br>常用函数(用法参数见help）<br><code>plt.plot()</code>画图<br><code>plt.show()</code>显示图像<br><code>plt.legend()</code>显示图像和图例</p>
<p><code>plt.xticks()``plt.yticks()</code>x,y坐标轴的值设置(可以设置旋转坐标值等）<br><code>plt.xlabel()``plt.ylabel()</code>x,y轴标签设置<br><code>plt.title()</code>设置图像标题</p>
<p><code>w1 = plt.figure()</code> 新建一个画图窗口 w1指向<br><code>ax1 = fig.add_subplot(2,1,1)</code> 向窗口中加入子图，参数为排列位置， ax1指向， 画2行1列个图形的第1个<br><code>ax1.plot()</code> 向ax1中画图</p>
<p>注意：无论是在子图中画图还是直接画图，都要用<code>plt.show()</code>来显示图像<br>在子图中画图，要用 <code>子图窗口.plot()</code>来画图，不用子图则<code>plt.plot()来画图</code></p>
<p><code>plt.scatter()</code>画散点图<br><code>plt.bar()</code>画柱状图</p>
</li>
<li><p>返回对象的属性值<br><code>vars()</code><br>用法<code>vars(object)</code><br>可用于很多封装对象</p>
</li>
<li><p>zip函数 打包<br>网上摘抄理解<br><a href="https://www.cnblogs.com/waltsmith/p/8029539.html" target="_blank" rel="noopener">https://www.cnblogs.com/waltsmith/p/8029539.html</a></p>
<p>zip函数非常好用！常用</p>
</li>
<li><p><code>class()(x)</code> 这种用法<br><code>class()</code>即为生成class对象，后面跟个<code>(x)</code>就是调用该生成对象的方法！</p>
</li>
<li><p><strong>getitem</strong>()<br>如果类中定义了<strong>getitem</strong>()方法,那么它的实例对象（假设为P）就可以P[Key]这样取值。当实例对象做P[Key]运算的时候就会调用<strong>getitem</strong>()方法，返回的就是这个方法的return</p>
</li>
<li><p>pip install xxx -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>中科大源</p>
</li>
</ol>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>PYTHON</category>
      </categories>
  </entry>
  <entry>
    <title>python数据分析相关笔记</title>
    <url>/2019/03/27/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><em>迄今为止在python数据分析相关库上遇到的坑，超长预警</em></p>
<a id="more"></a>
<ol>
<li><code>source .bashrc</code></li>
<li><code>source activate base</code>先激活base环境<br>再<code>jupyter notebook</code>启动jupyter notebook！先前anaconda安装的包都是在base环境下的！<br>在jupyter中<code>sys.path</code>就可以看出环境对没对！如果不是指定anaconda3中（我安装的）环境，那就没对！<br>查看anconda3有哪些环境<code>conda info --envs</code></li>
<li>.ipynb文件转.py文件<br><code>jupyter nbconvert --to script demo.ipynb</code></li>
<li><code>for i in (a,b)</code> <code>for i in range(a,b)</code> 别搞混了！<br>且range左闭右开</li>
<li><p><code>black10w.drop(black10w[black10w[&#39;Filename&#39;]==filename].index,axis=0,inplace=True)</code><br>去掉dataframe中文件名为filename的那一行</p>
</li>
<li><p><code>virtualenv --python=/usr/bin/python3.5 --no-site-packages envNAME</code><br>用此命令建立新的python环境，并使用系统的python3.5<br><a href="http://www.cnblogs.com/freely/p/8022923.html" target="_blank" rel="noopener">http://www.cnblogs.com/freely/p/8022923.html</a></p>
</li>
<li><p>+见20181123的周记</p>
</li>
<li><p>drop函数是个返回值！ <strong>pandas</strong>里好多函数都是返回值形式！一定勿忘赋值回去！不然不会改变<br><code>df = df.drop(list)</code> drop掉list中索引的内容，默认drop行（axis=0）<br>如<code>df=df.drop([0])</code>就是把第0行去掉<br>如果设置axis=1就是去掉列 索引可以不一定是序号 列用str索引那么列表里的元素就是list即可！<br><code>drop</code>掉指定行之后，不会自己重新索引！ 要手动重新索引!<br><code>df = df.reset_index(drop = True)</code><br>也可以直接修改列索引<br><code>df = pd.DataFrame(df,columns = [&#39;One&#39;,&#39;Two&#39;,&#39;Three&#39;])</code><br>（如果不需要用索引刻意不重建）</p>
</li>
<li><p>pandas中的<code>append()</code>函数也是个返回值！<br><code>df=df.append(x)</code><br>在<code>append</code>两个dataframe之后，索引可能就乱了！要在append里加参数<code>ignore_index=True</code>即可自动重建索引<br><code>df=df.append(x,ignore_index=True)</code></p>
<p><strong>但是</strong>，非pandas库中的不一定是！比如对于list，不在pandas中，则必须直接<code>list.append()</code></p>
</li>
<li><p><strong>iloc函数是按行数（编号）的！而不是index索引</strong>，即使drop后 index没有被重新排列，iloc也是从1开始数来定位</p>
<pre><code>drop函数是drop的对应index，而不是编号！
比如
      a b c
  0    1 6 4
  1    2 7 4
  2    3 8 5

drop掉1后就变成
      a b c
  0    1 6 4
  2    3 8 5
而这时候iloc[2]就out of bonud了，因为编号2已经不在了，在的是index2
</code></pre><p>但是 <strong>loc是按索引取号！</strong></p>
<p>即使这行删了，他也不会out of bonud 因为想要的索引那行并没有被删！</p>
<p><strong>所以在循环删除满足条件的行的时候，要用loc来删除！因为删除行后每行的索引是不变的！但行号是会变的！iloc是按行号来算的，所以就会出错！</strong></p>
<p><code>drop(x)</code> <strong>这里x是索引而不是行号</strong></p>
</li>
<li><p>item = ‘t2_asd’<br>item[0:3]<br>输出：t2_<br>截取和range一样，也是左闭右开</p>
</li>
<li><p><code>feaMatrix[:,&#39;a&#39;]</code>等效于<code>feaMatrix.a</code></p>
</li>
<li><p>DataFrame.values</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">values是DataFrame的一个特殊属性，得到一个array包含每一行的值(一个list)</span><br><span class="line"></span><br><span class="line">columns和rows也是DF的特殊属性，分布得到Index类型的列索引和行索引</span><br><span class="line"></span><br><span class="line">如果feaMatrix.columns.values则得到array类型的列索引列表和行索引列表</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;feaMatrix.columns.values</span><br><span class="line">array([<span class="string">'a'</span>, <span class="string">'b'</span>], dtype=object)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>切片和range都是左闭右开</strong>！！！！！</p>
</li>
<li><p>切片的时候 行用：而列不能用：</p>
</li>
<li><p>矩阵合并<br>可以用merge和append，都属于pandas库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pd.merge(left, right, how=&apos;inner&apos;, on=None, left_on=None, right_on=None,</span><br><span class="line">left_index=False, right_index=False, sort=True,</span><br><span class="line">suffixes=(&apos;_x&apos;, &apos;_y&apos;), copy=True, indicator=False)</span><br></pre></td></tr></table></figure>
<p>left︰ 对象<br>right︰ 另一个对象<br>on︰ 要加入的列 （名称）。必须在左、 右综合对象中找到。如果不能通过 left_index 和 right_index 是假，将推断 DataFrames 中的列的交叉点为连接键<br>left_on︰ 从左边的综合使用作为键列。可以是列名或数组的长度等于长度综合<br>right_on︰ 从正确的综合，以用作键列。可以是列名或数组的长度等于长度综合<br>left_index︰ 如果为 True，则使用索引 （行标签） 从左综合作为其联接键。在与多重 （层次） 的综合，级别数必须匹配联接键从右综合的数目<br>right_index︰ 相同用法作为正确综合 left_index<br>how︰ 之一 ‘左’，’右’，’外在’、 ‘内部’。默认为内部。每个方法的更详细说明请参阅︰<br>sort︰ 综合通过联接键按字典顺序对结果进行排序。默认值为 True，设置为 False将提高性能极大地在许多情况下<br>suffixes︰ 字符串后缀并不适用于重叠列的元组。默认值为 (‘_x’，’_y’)。<br>copy︰ 即使重新索引是不必要总是从传递的综合对象，复制的数据 （默认值True）。在许多情况下不能避免，但可能会提高性能 / 内存使用情况。可以避免复制上述案件有些病理但尽管如此提供此选项。<br>indicator︰ 将列添加到输出综合呼吁 _merge 与信息源的每一行。_merge 是绝对类型，并对观测其合并键只出现在 ‘左’ 的综合，观测其合并键只会出现在 ‘正确’ 的综合，和两个如果观察合并关键发现在两个 right_only left_only 的值。<br>eg：<code>result = pd.merge(left, right, how=&#39;outer&#39;, on=[&#39;key1&#39;, &#39;key2&#39;])</code></p>
<p><strong>遇见大坑:</strong><br>合并中发生排列组合问题！<br>当两个要合并的dataframe里合并关键字有<strong>重名项</strong>的时候，将会发生排列组合！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a  b</span><br><span class="line"><span class="number">1</span>  <span class="number">0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">a  c  d</span><br><span class="line"><span class="number">1</span>  <span class="number">1</span>  <span class="number">5</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">6</span></span><br><span class="line">    </span><br><span class="line">按a合并，结果是</span><br><span class="line">a	b	c	d</span><br><span class="line"><span class="number">1</span>	<span class="number">0</span>	<span class="number">1</span>	<span class="number">5</span></span><br><span class="line"><span class="number">1</span>	<span class="number">0</span>	<span class="number">2</span>	<span class="number">6</span></span><br><span class="line"><span class="number">1</span>	<span class="number">2</span>	<span class="number">1</span>	<span class="number">5</span></span><br><span class="line"><span class="number">1</span>	<span class="number">2</span>	<span class="number">2</span>	<span class="number">6</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>矩阵检索</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对于两个矩阵，找同一项的另一项，直接找速度非常慢。可以采用矩阵合并之后，再搜索</span></span><br><span class="line">df.loc[df.Filename==filename, <span class="string">'label'</span>] = <span class="number">1</span></span><br><span class="line">tencent[tencent.Filename == <span class="string">'5F94E5453CF23ACBF0256575D629A442'</span>]</span><br><span class="line"><span class="comment">#以上两种都可以找</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>dataframe删除行或列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">用法：DataFrame.drop(labels=<span class="keyword">None</span>,axis=<span class="number">0</span>, index=<span class="keyword">None</span>, columns=<span class="keyword">None</span>, inplace=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">在这里默认：axis=<span class="number">0</span>，指删除index，因此删除columns时要指定axis=<span class="number">1</span>；</span><br><span class="line"></span><br><span class="line">inplace=<span class="keyword">False</span>，默认该删除操作不改变原数据，而是返回一个执行删除操作后的新dataframe；</span><br><span class="line"></span><br><span class="line">inplace=<span class="keyword">True</span>，则会直接在原数据上进行删除操作，删除后就回不来了。</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">tencent = tencent.drop([tencent[tencent.Filename == <span class="string">'6A6709AA7F9DF362CF90D2DE063D1FFF'</span>].index.values[<span class="number">0</span>]],inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这里tencent[tencent.Filename == '6A6709AA7F9DF362CF90D2DE063D1FFF'] 找到了该行，.index.values[0]]取出了索引号，然后由drop函数去drop，inplace=True重建索引，然后赋值回给tencent</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##################################</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;df = pd.DataFrame(np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>), columns=[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</span><br><span class="line">&gt;&gt;&gt;df</span><br><span class="line"></span><br><span class="line">   A   B   C   D</span><br><span class="line"><span class="number">0</span>  <span class="number">0</span>   <span class="number">1</span>   <span class="number">2</span>   <span class="number">3</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">5</span>   <span class="number">6</span>   <span class="number">7</span></span><br><span class="line"><span class="number">2</span>  <span class="number">8</span>   <span class="number">9</span>  <span class="number">10</span>  <span class="number">11</span></span><br><span class="line"><span class="comment">#Drop columns,下面两种方法等价</span></span><br><span class="line">&gt;&gt;&gt;df = df.drop([<span class="string">'B'</span>, <span class="string">'C'</span>], axis=<span class="number">1</span>)</span><br><span class="line">   A   D</span><br><span class="line"><span class="number">0</span>  <span class="number">0</span>   <span class="number">3</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">7</span></span><br><span class="line"><span class="number">2</span>  <span class="number">8</span>  <span class="number">11</span></span><br><span class="line">&gt;&gt;&gt;df = df.drop(columns=[<span class="string">'B'</span>, <span class="string">'C'</span>])</span><br><span class="line">   A   D</span><br><span class="line"><span class="number">0</span>  <span class="number">0</span>   <span class="number">3</span></span><br><span class="line"><span class="number">1</span>  <span class="number">4</span>   <span class="number">7</span></span><br><span class="line"><span class="number">2</span>  <span class="number">8</span>  <span class="number">11</span></span><br><span class="line"><span class="comment">#Drop rows by index</span></span><br><span class="line">&gt;&gt;&gt;df = df.drop([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">   A  B   C   D</span><br><span class="line"><span class="number">2</span>  <span class="number">8</span>  <span class="number">9</span>  <span class="number">10</span>  <span class="number">11</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>对于<code>index</code>类型，可以用<code>index.values</code>转化为<code>array类型</code></p>
</li>
<li><p><code>.values</code>是什么属性</p>
</li>
<li><p>JSON和字典<br>[JSON]和[字典]的标准格式是用<code>{}</code>包起来的内容！必须要有外面的{}<br><code>dict = eval(str)</code>可以将字符串转化为字典</p>
</li>
<li><p>inplace</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inplace参数的理解：</span><br><span class="line">修改一个对象时：</span><br><span class="line">inplace=<span class="keyword">True</span>：不创建新的对象，直接对原始对象进行修改；</span><br><span class="line">inplace=<span class="keyword">False</span>：对数据进行修改，创建并返回新的对象承载其修改结果。</span><br></pre></td></tr></table></figure>
</li>
<li><p>python中list转字符串<br>命令：’’.join(list)<br>其中，引号中是字符之间的分割符，如“,”，“;”，“\t”等等<br>如：<br>list = [1, 2, 3, 4, 5]<br>‘’.join(list) 结果即为：12345<br>‘,’.join(list) 结果即为：1,2,3,4,5</p>
</li>
<li><p>json函数库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">dict = json.loads(string)</span><br><span class="line"><span class="comment">#即可将string转换为json，其中里面的一切格式问题都不用管，json.loads全部解决</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>python分割<br>除了用<code>split</code>方法以外，还可以用正则表达式实现多规则分割</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">a = <span class="string">"Hello world!How are you?My friend.Tom"</span></span><br><span class="line">re.split(<span class="string">" !|\?|\."</span>, a)</span><br><span class="line">[<span class="string">'Hello'</span>, <span class="string">'world'</span>, <span class="string">'How'</span>, <span class="string">'are'</span>, <span class="string">'you'</span>, <span class="string">'My'</span>, <span class="string">'friend'</span>, <span class="string">'Tom'</span>]</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">但要注意，对? ( ) . 等字符分割的时候要转义，即要写 \. \( \) \?</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>append的时候勿忘<code>ignore_index = True</code> 否则索引就凉了！</p>
</li>
<li><p>python广播<br>numpy广播使得不同数组间可以进行运算<br>如一个向量+一个常数的时候，常数会被自动扩展成一个向量<br>见松鼠书P435</p>
</li>
<li><p><strong>numpy向量化计算</strong><br>尽量少使用显式循环<br>用向量化（并行运算） 即将元素作为向量进行计算<br>numpy.dot(x,y)<br>numpy.exp(v)<br>等等 numpy有很多向量化函数<br>即为点乘<br>平时计算的时候也可以直接用向量化计算，速度快很多</p>
</li>
<li><p>numpy中常用reshape来确保数组的形式</p>
</li>
<li><p><code>a.T</code>转置</p>
</li>
<li><p><code>numpy.random.randn(5)</code>随机生成矩阵 1<em>5    <strong>注意生成的只有一个[]</strong><br><code>numpy.random.randn(1,5)</code>随机生成矩阵 <strong>注意生成的是[[]]</strong>  <strong>==用这个</strong><br>（numpy array格式）<br><em>*不要用上面那种写法，容易出错！不直观，它既不是行向量也不是列向量</em></em></p>
</li>
<li><p>不要吝惜使用<code>reshape</code></p>
</li>
<li><p><code>assert(表达式)</code>语句用于检查表达式是否正确，如果不正确程序在这里引发错误<br>方便检查</p>
</li>
<li><p><code>list.append()</code><br>不需要a = a.append()</p>
</li>
<li><p>改变DATAFRAME的顺序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">order = [<span class="string">'date'</span>, <span class="string">'time'</span>, <span class="string">'open'</span>, <span class="string">'high'</span>, <span class="string">'low'</span>, <span class="string">'close'</span>, <span class="string">'volumefrom'</span>, <span class="string">'volumeto'</span>]</span><br><span class="line">df = df[order]</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dataframe遍历<br>不能用<code>for Line in DataFrame</code>来遍历！<br>要么用iloc index方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(df)):</span><br><span class="line">    <span class="keyword">print</span> df.iloc[i][<span class="string">'c1'</span>], df.iloc[i][<span class="string">'c2'</span>]</span><br></pre></td></tr></table></figure>
<p>要么用iterrows（<a href="https://blog.csdn.net/ls13552912394/article/details/79349809）" target="_blank" rel="noopener">https://blog.csdn.net/ls13552912394/article/details/79349809）</a></p>
</li>
<li><p>排序不成功问题<br>问题：遇到排序问题，使用下面的语句对pandas的某列进行排序时,发现根本没排序成功。<br>bada_air.sort_value(by=’time’,ascending=False)<br>解决方案：这里牵扯到很重要的参数inplace,默认的inplace设置是False,并没有对本体进行覆盖，所以解决方法有两个：<br>1.设置本体覆盖，令inplace=True<br>bada_air.sort_value(by=’time’,ascending=False,inplace=True)<br>2.设置传值覆盖<br>bada_air=bada_air.sort_value(by=’time’,ascending=False)</p>
</li>
<li><p>pd.read_csv出现Unnamed:0这一列<br>解决办法：pd.read_csv(path,index_col=0)</p>
</li>
<li><p>记清楚<br>loc[行限制，列限制]<br>比如行限制就可以A=A.loc[A.Filename = ‘aaa’,:]<br>或者其他选法等等</p>
</li>
<li><p>合并 merge join 等的用法<br><a href="https://www.jianshu.com/p/5ecea164cec6" target="_blank" rel="noopener">https://www.jianshu.com/p/5ecea164cec6</a></p>
</li>
<li><p>错点！ series别忘了转成list 再用in！ a in list O     a in  Series X</p>
</li>
<li><p>df1=df2 传递的不是指针，而是内容复制了一份</p>
</li>
<li><p>关于drop</p>
<ul>
<li>drop是drop的索引</li>
<li>注意drop属于panda 要用df = df.drop(x)</li>
<li>inplace=True参数设置重建索引，df = df.drop(x,inplace=True)</li>
</ul>
</li>
<li><p><strong>重建索引</strong><br>df = df.reset_index(drop = True)</p>
</li>
<li><p><strong>修改DF某元素</strong>（最终选择方法二）<br>一。<br>一个大坑<br>在<strong>修改df元素</strong>的时候，iloc和loc有重大区别！<br>loc和iloc都可以进行切片，但是用<strong>iloc不可以修改切片中的元素！</strong><br><strong>loc可以修改切片中的元素！</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test =  pd.DataFrame(&#123;<span class="string">'a'</span>:[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],<span class="string">'b'</span>:[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]&#125;)</span><br><span class="line">test[<span class="string">'c'</span>]=<span class="number">0</span></span><br><span class="line">test.iloc[<span class="number">2</span>].c+=<span class="number">2</span>  <span class="comment">#用iloc</span></span><br><span class="line">test</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">	a	b	c</span><br><span class="line"><span class="number">0</span>	<span class="number">3</span>	<span class="number">6</span>	<span class="number">0</span></span><br><span class="line"><span class="number">1</span>	<span class="number">4</span>	<span class="number">7</span>	<span class="number">0</span></span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>	<span class="number">8</span>	<span class="number">0</span></span><br><span class="line"><span class="number">3</span>	<span class="number">6</span>	<span class="number">9</span>	<span class="number">0</span></span><br><span class="line"></span><br><span class="line">test =  pd.DataFrame(&#123;<span class="string">'a'</span>:[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],<span class="string">'b'</span>:[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]&#125;)</span><br><span class="line">test[<span class="string">'c'</span>]=<span class="number">0</span></span><br><span class="line">test.loc[<span class="number">2</span>].c+=<span class="number">2</span>  <span class="comment">#用loc</span></span><br><span class="line">test</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">	a	b	c</span><br><span class="line"><span class="number">0</span>	<span class="number">3</span>	<span class="number">6</span>	<span class="number">0</span></span><br><span class="line"><span class="number">1</span>	<span class="number">4</span>	<span class="number">7</span>	<span class="number">0</span></span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>	<span class="number">8</span>	<span class="number">2</span></span><br><span class="line"><span class="number">3</span>	<span class="number">6</span>	<span class="number">9</span>	<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><strong>二。但是！！</strong> 在很多情况下 这样还是无法修改！！！！<br><strong>所以用这个方法！！</strong><br><strong>不用loc iloc 先选列索引 再选行索引！</strong><br><strong><code>test[&#39;c&#39;][2]+=2</code></strong>  <strong>统一使用此方法！</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test =  pd.DataFrame(&#123;<span class="string">'a'</span>:[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],<span class="string">'b'</span>:[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]&#125;)</span><br><span class="line">test[<span class="string">'c'</span>]=<span class="number">0</span></span><br><span class="line">test[<span class="string">'c'</span>][<span class="number">2</span>]+=<span class="number">2</span>  <span class="comment">#看这里！！！！！！！！！！！！！！！！</span></span><br><span class="line">test</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">	a	b	c</span><br><span class="line"><span class="number">0</span>	<span class="number">3</span>	<span class="number">6</span>	<span class="number">0</span></span><br><span class="line"><span class="number">1</span>	<span class="number">4</span>	<span class="number">7</span>	<span class="number">0</span></span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>	<span class="number">8</span>	<span class="number">2</span></span><br><span class="line"><span class="number">3</span>	<span class="number">6</span>	<span class="number">9</span>	<span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在处理大批量数据时,如果需要先向df对象插入空列,在后期再填充空列中对应位置的元素,应优先考虑使用loc而不是iloc,因为前者的访问速度比后者快几十倍</p>
</li>
<li><p>dict排序<br><a href="https://www.cnblogs.com/dylan-wu/p/6041465.html" target="_blank" rel="noopener">https://www.cnblogs.com/dylan-wu/p/6041465.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict = sorted(dict.items(),key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>],reverse=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># items()将dict转化为迭代器，lambda正咋表达式，item[1]即为迭代器的第二项</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>判断nan</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">math.isnan(x) <span class="comment">#x得是float类型</span></span><br><span class="line"></span><br><span class="line">type(x) != float      <span class="comment">#用类型判断，因为nan属于float类型</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>python取数据！ python搜索数据 重要总结！</strong><br><a href="https://www.jianshu.com/p/805f20ac6e06" target="_blank" rel="noopener">https://www.jianshu.com/p/805f20ac6e06</a></p>
</li>
<li><p>python搜索数据的时候 这种情况要用isin</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">allDatasetApiCorpus[allDatasetApiCorpus[<span class="string">'Filename'</span>].isin(list)]</span><br></pre></td></tr></table></figure>
</li>
<li><p>Series可以直接与int比大小，等于对每一项比大小！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eg:</span><br><span class="line">lis = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]</span><br><span class="line">lis = pd.Series(lis)</span><br><span class="line">lis[lis&gt;<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line"><span class="number">0</span>    <span class="number">1</span></span><br><span class="line"><span class="number">1</span>    <span class="number">2</span></span><br><span class="line"><span class="number">2</span>    <span class="number">3</span></span><br><span class="line">dtype: int64</span><br><span class="line"></span><br><span class="line">lis&gt;<span class="number">0</span></span><br><span class="line">&gt;&gt;</span><br><span class="line"><span class="number">0</span>     <span class="keyword">True</span></span><br><span class="line"><span class="number">1</span>     <span class="keyword">True</span></span><br><span class="line"><span class="number">2</span>     <span class="keyword">True</span></span><br><span class="line"><span class="number">3</span>    <span class="keyword">False</span></span><br><span class="line"><span class="number">4</span>    <span class="keyword">False</span></span><br><span class="line"><span class="number">5</span>    <span class="keyword">False</span></span><br><span class="line">dtype: bool</span><br></pre></td></tr></table></figure>
</li>
<li><p>枚举 enumerate的用法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lis = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]</span><br><span class="line"><span class="keyword">for</span> index, var <span class="keyword">in</span> enumerate(lis):</span><br><span class="line">    print(index,<span class="string">' '</span>,var)</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line"><span class="number">0</span>   <span class="number">1</span></span><br><span class="line"><span class="number">1</span>   <span class="number">2</span></span><br><span class="line"><span class="number">2</span>   <span class="number">3</span></span><br><span class="line"><span class="number">3</span>   <span class="number">-1</span></span><br><span class="line"><span class="number">4</span>   <span class="number">-2</span></span><br><span class="line"><span class="number">5</span>   <span class="number">-3</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>元组可以这样用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = (<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">s[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">&gt;&gt;<span class="number">2</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>转置<br><code>narray.T, dataframe.T</code></p>
</li>
<li><p>去重 <code>drop_duplicates()</code><br><code>drop_duplicates()</code>是去除重复行，要想去除重复列要先转置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DataFrame = DataFrame.drop_duplicates(subset=<span class="keyword">None</span>, keep=<span class="string">'first'</span>, inplace=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">subset : column label or sequence of labels, optional</span></span><br><span class="line"><span class="string">用来指定特定的列，默认所有列（指的是以哪些列为参考删除行，删的还是行）</span></span><br><span class="line"><span class="string">keep : &#123;‘first’, ‘last’, False&#125;, default ‘first’</span></span><br><span class="line"><span class="string">删除重复项并保留第一次出现的项</span></span><br><span class="line"><span class="string">inplace : boolean, default False</span></span><br><span class="line"><span class="string">是直接在原来数据上修改还是保留一个副本</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化列表 list</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一维列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化递增的list，与L = [i for i in range(10)] 效果相同</span></span><br><span class="line">L = range(<span class="number">10</span>)  </span><br><span class="line"><span class="comment"># print(L)</span></span><br><span class="line"><span class="comment"># [0,1,2,3,4,5,6,7,8,9]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化每项为0的一维列表</span></span><br><span class="line">L = [<span class="number">0</span>] * <span class="number">5</span></span><br><span class="line"><span class="comment"># print(L)</span></span><br><span class="line"><span class="comment">#[0,0,0,0,0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维列表</span></span><br><span class="line"></span><br><span class="line">L = [[<span class="number">0</span>] * <span class="number">5</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line"><span class="comment">#print(L)</span></span><br><span class="line"><span class="comment">#[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里需要注意，虽然L = [[0] * 5] * 5，也输出同样的效果，但是万万不能这样做！</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#因为[0] * 5是一个一维列表的对象，再* 5的话只是把对象的引用复制了3次。什么意思呢，就是如果我们将L[0][0] = 1，#再输出L如下：</span></span><br><span class="line">L = [[<span class="number">0</span>] * <span class="number">5</span>] * <span class="number">5</span></span><br><span class="line">L[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment">#print(L)</span></span><br><span class="line"><span class="comment">#[[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#我们只是想改变L[0][0]，结果L[[n][0]全部改变了！这回知道为啥不能这么做了吧！</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#所以对于一位列表初始化，也建议大家用L = [[0] for i in range(5)]来代替L = [[0] * 5]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Numpy扁平化函数 ravel和flatten</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">   <span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">   a = arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">   print(a)</span><br><span class="line">   <span class="comment"># [[ 0  1  2  3]</span></span><br><span class="line">   <span class="comment">#  [ 4  5  6  7]</span></span><br><span class="line">   <span class="comment">#  [ 8  9 10 11]]</span></span><br><span class="line">   print(a.ravel())</span><br><span class="line">   <span class="comment"># [ 0  1  2  3  4  5  6  7  8  9 10 11]</span></span><br><span class="line">   print(a.flatten())</span><br><span class="line">   <span class="comment"># [ 0  1  2  3  4  5  6  7  8  9 10 11]</span></span><br><span class="line"></span><br><span class="line">可以看到这两个函数实现的功能一样,但我们在平时使用的时候flatten()更为合适.在使用过程中flatten()分配了新的内存,但ravel()返回的是一个数组的视图.视图是数组的引用(说引用不太恰当,因为原数组和ravel()返回后的数组的地址并不一样),在使用过程中应该注意避免在修改视图时影响原本的数组.</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>[confusion_matrix - too many values to unpack]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">you can only assign multiple variables dynamically <span class="keyword">if</span> the number of outputs <span class="keyword">is</span> certain. If you 		assign the result of `confusion_matrix` to a single variable, you can then check its contents 		<span class="keyword">in</span> a loop <span class="keyword">and</span> assign the contents conditionally:</span><br><span class="line">returned = confusion_matrix(y_true, y_predict).ravel()</span><br><span class="line">   <span class="keyword">for</span> var <span class="keyword">in</span> returned:</span><br><span class="line">   <span class="comment">#... do stuff with each item in the returned collection</span></span><br><span class="line">You could also just check its length <span class="keyword">and</span> <span class="keyword">if</span> it <span class="keyword">is</span> <span class="number">4</span>, you can proceed <span class="keyword">as</span> usual:</span><br><span class="line"><span class="keyword">if</span> len(returned) == <span class="number">4</span>:</span><br><span class="line">       tn, fp, fn, tp = returned</span><br></pre></td></tr></table></figure>
</li>
<li><p>遍历dataframe<br><code>for col in df</code>遍历的是列标签<br>要遍历行有几种方案<a href="https://blog.csdn.net/ls13552912394/article/details/79349809" target="_blank" rel="noopener">https://blog.csdn.net/ls13552912394/article/details/79349809</a><br>遍历这种东西在df选取等操作中不是一个好的解决方案，可以通过df的搜索loc iloc等很多方式来实现，比遍历好得多方便的多</p>
</li>
<li><p>python list去重<br>运用set的特性<br><code>opList = list(set(opList))</code></p>
</li>
<li><p>切片的时候若不存在<br>msgTrojanDynamicDF.loc[:,[‘android.content.Intent_setAction’,’sss’] ]<br>其中如果sss不存在，则sss列全为NaN<br>但如果两列都不存在，就会报错</p>
</li>
<li><p>Series.sum(axis-1)可以统计竖向的和</p>
</li>
<li><p>拼接dataframe</p>
<p><code>df = pd.concat([A,B,C])</code><br>numpy.transpose 用于翻转<br><a href="https://blog.csdn.net/u012762410/article/details/78912667" target="_blank" rel="noopener">https://blog.csdn.net/u012762410/article/details/78912667</a></p>
</li>
<li><p><code>torch.cat 拼接Tensor</code><br>torch.cat((a,b),1)`</p>
</li>
<li><p>转置<br><code>np.transpose()</code><br><code>torch.transpose()</code></p>
</li>
<li><p><strong>左闭右开</strong><br>注意python各个库的左闭右开原则 [:]切片的时候注意！ range,list,numpy,pandas,torch都这样</p>
</li>
<li><p>将数据结构转化为集合可以自动去重！（#去重）<br><code>a = set(a)</code><br><code>set(1,2,3) = set(1,1,1,1,1,1,2,3)</code></p>
</li>
<li><p>tensor,numpy array的创建与扩展</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建</span></span><br><span class="line">np.zeros([<span class="number">3</span>,<span class="number">10</span>]) <span class="comment">#创建一个3x10的array</span></span><br><span class="line">torch.zeros(<span class="number">3</span>,<span class="number">10</span>) <span class="comment">#创建一个3x10的tensor</span></span><br><span class="line">list([<span class="number">0</span>])*<span class="number">10</span>  <span class="comment">#[10,10,10,10,10,10,10,10,10,10]</span></span><br><span class="line">list([[<span class="number">0</span>]*<span class="number">10</span>]*<span class="number">3</span>) <span class="comment">#创建一个3x10的list</span></span><br><span class="line"></span><br><span class="line">np.random.rand(<span class="number">3</span>,<span class="number">10</span>) <span class="comment">#随机创建一个3x10的array</span></span><br><span class="line">torch.rand(<span class="number">3</span>,<span class="number">10</span>) <span class="comment">#随机创建一个3x10的tensor</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">#合并/拓展</span></span><br><span class="line">a.extend([<span class="number">4</span>,<span class="number">5</span>]) <span class="comment">#[1,2,3,4,5]</span></span><br><span class="line"><span class="comment">#注意用法</span></span><br><span class="line">extend后保存的值就为a，和append用法类似</span><br><span class="line"></span><br><span class="line">removes = np.append(array1,array2,axis=<span class="number">0</span>) <span class="comment">#axis为合并的维度</span></span><br><span class="line">tensor 使用torch.stack进行合并</span><br></pre></td></tr></table></figure>
</li>
<li><p>list 删除元素</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list.remove(x)</span><br><span class="line">print(list)</span><br></pre></td></tr></table></figure>
<p>该方法没有返回值<br>只会删掉匹配到的第一个<br>还有2个方法懒得记了</p>
<p>python remove里有个巨坑！<br>如果用for循环来remove的话，for循环的时候，每删除一个元素，长度就会改变，索引就会变化！所以remove就会出问题！<br>比如 要依次删除1，2位置的元素。<br>第一次删除了1位置，那么列表索引就变了，2元素成为了1元素，3元素成为了2元素！2元素就这样被跳过了！<br>解决办法见下</p>
</li>
<li><p>for循环 <strong>动态修改列表</strong>的时候的注意事项<br>如何解决：deepcopy一份，一个列表用于循环，一个列表用于移除值<br>（<code>from copy import deepcopy</code>)</p>
</li>
<li><p>调换维度。<br>  使用numpyarrray.swapaxes(ax0,ax1)</p>
</li>
<li><p>numpy.size()函数<br>  a = np.array(…)<br>  np.size(a) 计算数组和矩阵所有数据的个数<br>  np.size(a,1)计算该array第1维（第一个维度为0）的元素个数</p>
</li>
<li><p>tensor/array值归一化<br>  <code>tensor.div_()</code><br>  <code>narray.div_()</code></p>
</li>
<li><p>windows路径\\变成/即可</p>
</li>
</ol>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>PYTHON</category>
      </categories>
  </entry>
  <entry>
    <title>WE NEED R!G!B!</title>
    <url>/2019/02/27/R-G-B/</url>
    <content><![CDATA[<p>北极狼和北极熊水冷性能虽然很强，但是他们有个缺点就是没有RGB————</p>
<p>这怎么行呢。</p>
<p>RGB化改造迫在眉睫</p>
<p>于是我买了HALOS圣环，自己定做了显卡侧灯板</p>
 <a id="more"></a>
<p>改装之前——————</p>
<p><img src="//aisakaki.com/2019/02/27/R-G-B/IMG_2469.jpg" alt="改装之前"></p>
<p>改装之后——————</p>
<p><img src="//aisakaki.com/2019/02/27/R-G-B/IMG_3998.jpg" alt="改装后"></p>
<p>动图：</p>
<p><img src="//aisakaki.com/2019/02/27/R-G-B/final.gif" alt="改装后动态图"></p>
<p>愉快打电动~</p>
<p><img src="//aisakaki.com/2019/02/27/R-G-B/IMG_4064.JPG" alt="电动"></p>
<p>这中间遇到个小插曲</p>
<p>那块显卡侧灯右边漏光的一条是因为…</p>
<p>我定做的时候 尺寸量错了，多度了半厘米。</p>
<p>于是只能用刀片来削….</p>
<p>然后就成这样了</p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>神说，要有光</tag>
      </tags>
  </entry>
  <entry>
    <title>图像预处理：transforms和PIL小结</title>
    <url>/2019/01/28/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9Atransforms%E5%92%8CPIL%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<p><em>色图真好玩.jpg</em></p>
<h2 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h2><p>Ⅰ. <code>transforms.ToTensor()</code></p>
<p>①将(H,W,C)输入的numpy array或img转换成(C,H,W)格式</p>
<p>通道的具体顺序与cv2读的还是PIL.Image读的图片有关系<br>cv2:<code>(B,G,R)</code><br>PIL.Image:<code>(R, G, B)</code></p>
<p>②<strong>归一化</strong>，归一到$(0,1)$区间</p>
<p>Ⅱ. <code>transforms.Normalize()</code></p>
<p>标准化。</p>
<p>即使输入的每个值分布到$(-1，1)$中</p>
<p>公式为$Normalize = \frac{(x-mean)}{std}$(std为标准差，mean为均值)</p>
<p>标准差公式$std = \sigma = \sqrt{\frac{1}{N} \Sigma^N_{i=1}(x_i-μ)^2 }$</p>
<p><strong>注意实际是img里的每个像素（每个值）都要经过此公式运算</strong></p>
<p>通过标准化后可以让数据<strong>服从标准正态分布</strong></p>
<p>一般来说参数为 <code>transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))</code></p>
<p>这里的三个维度是因为输入的$x$为三维的，如果$x$是灰度图，那就是一维</p>
<p>$(0.5,0.5,0.5)$即为<strong>三个维度（分别对应RGB三个通道）</strong>的均值和标准差。这里默认为该值，如果你能够计算出你的数据集的均值和标准差，那么用计算出来的值更好。（如imagenet数据集提供方就给出了该数据集的均值和标准差）</p>
<p>以上两个函数通常是连着用的</p>
<p>Ⅲ.<code>transforms.Lambda</code></p>
<p>用法:<code>transforms.Lambda(lambda img:func(img,))</code></p>
<p>这里的<code>func</code><strong>可以是自己定义的函数，也可以直接调用表达式</strong>比如<code>transforms.Lambda(lambda img:img)</code>(什么都没处理，返回了自己本身)</p>
<p>在transforms库中存在很多官方给的预处理工具，但是如果需要自己定义，比如截取图像指定区域，则需要自己用Lambda函数封装一个处理函数</p>
<p>eg:自己定义一个截取图片的crop函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__crop</span><span class="params">(img, pos, size)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param img: 输入的图像</span></span><br><span class="line"><span class="string">    :param pos: 图像截取的位置,类型为元组，包含(x, y)</span></span><br><span class="line"><span class="string">    :param size: 图像截取的大小</span></span><br><span class="line"><span class="string">    :return: 返回截取后的图像</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ow, oh = img.size</span><br><span class="line">    x1, y1 = pos</span><br><span class="line">    tw = th = size</span><br><span class="line">    <span class="comment"># 有足够的大小截取</span></span><br><span class="line">    <span class="comment"># img.crop坐标表示 (left, upper, right, lower)</span></span><br><span class="line">    <span class="keyword">if</span> (ow &gt; tw <span class="keyword">or</span> oh &gt; th):</span><br><span class="line">        <span class="keyword">return</span> img.crop((x1, y1, x1+tw, y1+th))</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后使用transforms.Lambda封装其为transforms策略</span></span><br><span class="line"><span class="comment"># 然后定义新的transforms为</span></span><br><span class="line">normalize = transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">data_transforms = transforms.Compose([</span><br><span class="line">    <span class="comment">#注意这里的用法</span></span><br><span class="line">    transforms.Lambda(<span class="keyword">lambda</span> img: __crop(img, (<span class="number">5</span>,<span class="number">5</span>), <span class="number">224</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转给定的PIL.Image,翻转概率为0.5</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转成Tensor格式，大小范围为[0,1]</span></span><br><span class="line">    normalize</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>Ⅳ. <strong>逆过程</strong></p>
<p><code>transforms.Normalize()</code>的逆过程即<strong>原公式求反函数</strong></p>
<p>原公式$Normalize = \frac{(x-(-mean))}{std}$的反函数即为</p>
<p>$x = Normalize*std+mean$</p>
<p>注意是$x$中的<strong>每个元素</strong>都要经过此逆运算，可以使用numpy array的矩阵批量乘法</p>
<p>如果$std$和$mean$三个维度（RGB情况）是不一样的，那就要分别在三个维度上单独做逆运算</p>
<p><code>transforms.ToTensor()</code>的逆过程为<code>transforms.ToPILImage()</code>(这个类里面就包含各种处理了包括逆归一化)</p>
<p>这里也可以直接用<code>plt.imshow(tensor)</code>来显示图像</p>
<p>这里tensor要*255，因为.ToTensor也有归一化的功能</p>
<p>注：另外<strong>transforms组件可以单独拿出来用</strong>，不一定要compose组合</p>
<p><strong>但有个问题需要注意！用法是</strong></p>
<p><code>transforms.ToPILImage()(tensor)</code></p>
<p><strong>因为transforms里的组件都是对象！不是方法，得建立对象之后再调用对象的方法!</strong></p>
<h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><p>Ⅰ. 打开图片</p>
<ul>
<li><p>使用PIL.Image.open只会调用本地图片浏览器打开图片<br>所以要用<code>matplotlib.pyplot.imshow()</code>来打开PIL导入的图片</p>
<p>(一般<code>import matplotlib.pyplot as plt</code>)</p>
<p><strong>注意先要将PIL.Image导入的图片转化为narray格式 (np.array)（导入进来的时候是PIL.Image.Image对象）</strong></p>
</li>
<li><p>注意<code>open</code>: 打开并识别所提供的图像文件。不过，使用这函数的时候，<strong>真正的图像数据在你进行数据处理之前并没有被读取出来</strong>。可使用 <code>load</code>函数进行强制加载。 mode 参数可以省略，但它只能是 “r” 值。</p>
</li>
<li><p>如果想要迭代打开多张图片每张都显示，则必须</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(img)</span><br><span class="line"><span class="comment">#多一句show</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>也可以使用<code>PIL.Image.open</code>加载图片</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.open(iMagePath)</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意读取单个图片进入神经网络训练的时候</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imga = torch.unsqueeze(imga, <span class="number">0</span>)</span><br><span class="line">imga = imga.cuda()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Ⅱ. plt.imshow()不能同时显示多张图片<br>如果想要显示多张，<strong>可以在每个plt.imshow()后面加一个plt.show()</strong></p>
<p>Ⅲ. 图片处理部分</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform=transforms.Compose(</span><br><span class="line">    [transforms.Resize(<span class="number">256</span>),   <span class="comment">#resize并不会改变长宽比例！ 重要问题1</span></span><br><span class="line">     transforms.CenterCrop(<span class="number">256</span>),   <span class="comment">#resize后要crop！</span></span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)), </span><br><span class="line">     ])</span><br><span class="line"><span class="comment">#然后可以用这个来处理单个PIL图片</span></span><br><span class="line">img = transform(img)</span><br><span class="line"><span class="comment">#这样即应用了变换</span></span><br><span class="line"><span class="comment">#也可以用以下这个来处理数据集</span></span><br><span class="line">trainset = torchvision.datasets.ImageFolder(train_dir, transform=transform)</span><br></pre></td></tr></table></figure>
<p>   Ⅳ. 应用图片数据预处理 transforms模块</p>
<p>   生成的transforms直接作用在PIL上</p>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 比如这段定义了变换之后</span></span><br><span class="line">  transform=transforms.Compose(</span><br><span class="line">    [transforms.Resize(<span class="number">256</span>),   </span><br><span class="line">     transforms.CenterCrop(<span class="number">256</span>), </span><br><span class="line">     transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)), </span><br><span class="line">     ])</span><br><span class="line"><span class="comment">#如果是用loader来加载的话就是</span></span><br><span class="line">trainset = torchvision.datasets.ImageFolder(train_dir, transform=transform)</span><br><span class="line"><span class="comment">#如果是直接应用到一个图片上的话就是</span></span><br><span class="line">img = Image.open(picPath)</span><br><span class="line">imga = transform(img)</span><br></pre></td></tr></table></figure>
<p>   Ⅴ. resize与toTensor的先后顺序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform=transforms.Compose(</span><br><span class="line">    [transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">     ])</span><br><span class="line"><span class="comment"># 这里面的变换是有顺序的！</span></span><br><span class="line"><span class="comment"># transforms.Resize()是图片变换，是作用在PIL.Images上的！所以一定要在ToTensor之前！</span></span><br></pre></td></tr></table></figure>
<p>   <strong>图片变换》ToTensor》归一化</strong><br>   <code>transforms.Resize()</code> works on <code>PIL.Images</code>, so just swap the order of your transformations and your code should run.<br>   否则会报错TypeError: img should be PIL Image. Got <class 'torch.tensor'=""></class></p>
<p>Ⅵ. resize和crop的区别<br>resize并不会改变长宽比例！是等比缩放的<br>crop是在图中挖出一部分（裁剪） 这个才会改变比例！<br><strong>resize后要crop！</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transforms.Resize(<span class="number">256</span>)</span><br><span class="line">transforms.CenterCrop(<span class="number">256</span>)</span><br></pre></td></tr></table></figure>
<p>crop 有很多种方式 待学习<br>crop和resize的作用范围都是PIL而不是tensor</p>
<h2 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h2><p>图片数据集定义<br>   <code>trainset = torchvision.datasets.ImageFolder(root=train_dir)</code><br>   train_dir中要有子文件夹！ 按照子文件夹读取标签！</p>
<p>   <strong>imageFolder打标签的问题：</strong>和文件夹的顺序有关<br>   【按照各子文件夹的顺序打上0.1.2….  且是刷新后的顺序！ 所以最好是直接0,1,2,3给子文件夹命名，自己清楚里面代表的是啥就是了！】<br>   所以如果说两个文件夹内的子文件夹都是一样的话，相同分类打的标签就是一样的</p>
<p>   <strong>ImageFolder两个重要属性</strong>： ①class_to_idx ②classes</p>
<h2 id="关于输入网络的图片尺寸"><a href="#关于输入网络的图片尺寸" class="headerlink" title="关于输入网络的图片尺寸"></a>关于输入网络的图片尺寸</h2><p>输入图片尺寸必须一样是因为在出特征提取层，进入全连接层的时候，全连接层是固定的，所以如果只是要提取出特征的话，并不要求输入图片尺寸一样！</p>
<p>有个很好玩的FCN（全卷积网络）就是这样</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>炼丹小结</title>
    <url>/2019/01/20/%E7%82%BC%E4%B8%B9%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<ol>
<li><p>40层网络的参数比4层网络多10倍 因为每层都有参数<br>所以batch size也要相应的减小！不然太大了</p>
</li>
<li><p>train loss 一直处于一个摇摆（震荡）状态可能是学习率设置过大导致的</p>
</li>
<li><p>如何选择神经网络的自适应优化算法：</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SGD：随机梯度下降</span><br><span class="line">SGD+Momentum: 基于动量的SGD（在SGD基础上做过优化）</span><br><span class="line">SGD+Nesterov+Momentum：基于动量，两步更新的SGD（在SGD+Momentum基础上做过优化）</span><br><span class="line">Adagrad：自适应地为各个参数分配不同学习速率</span><br><span class="line">Adadelta： 针对Adagrad问题，优化过的算法（在Adagrad基础上做过优化）</span><br><span class="line">RMSprop：对于循环神经网络（RNNs）是最好的优化器（在Adadelta基础上做过优化）</span><br><span class="line">Adam：对每个权值都计算自适应的学习速率（在RMSprop基础上做过优化）</span><br></pre></td></tr></table></figure>
<p> 如果数据输入量很小，那就选一种自适应学习速率的方法（如Adam）。这样你就不用对学习速率进行调优，因为你的数据本来就小，NN学习耗时也小。这种情况你更应该关心网络分类的准确率；<br> 对于稀疏数据集，应该使用某种自适应学习率的方法（如Adam），且另一好处为不需要人为调整学习率，使用默认参数就可能获得最优值；<br> <strong>如果想使训练深层网络模型快速收敛或所构建的神经网络较为复杂，则应该使用自适应学习速率的方法</strong>（如Adam），通常它们收敛起来比较快，实际效果更优；</p>
<p> Adam在不同超参数下的鲁棒性较好，<strong>不过有时可能需要调整下η值。Adam算法中的超参数β1和β2以及learning-rate也会显著影响模型，有时需要调试</strong>；</p>
<p> RMSprop, Adadelta, 和 Adam 非常相似，在相同的情况下表现都很好，但偏置校验让Adam的效果稍微比RMSprop好一点；</p>
<p> <strong>如果不知道为你的神经网络选择哪种优化算法，就直接选Adam</strong></p>
<p> [Insofar, Adam might be the best overall choice.]</p>
<p> 上面的结论都未必正确，重在自己的实践</p>
<p> （本段总结自 <a href="https://www.jianshu.com/p/e8c2d0371c8a" target="_blank" rel="noopener">神经网络优化算法的选择</a>）</p>
</li>
<li><p>超参数搜索</p>
<p>python有超参数自动搜索模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.model_selection</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/nwpuxuezha/p/6618205.html" target="_blank" rel="noopener">Python超参数自动搜索模块GridSearchCV上手</a><br><a href="https://www.jianshu.com/p/5378ef009cae" target="_blank" rel="noopener">ML模型超参数调节：网格搜索、随机搜索与贝叶斯优化</a></p>
</li>
<li><p>基于要解决的问题类别,神经网络的最后一层通常是确定的.</p>
<ul>
<li><strong>回归</strong>问题: 最后使用一个有输出的线性层,输出的值是连续的.</li>
<li><strong>二分类</strong>问题:最后使用sigmoid激活函数, 因为它的输出值不是接近1就是接近0</li>
<li>对于<strong>多分类</strong>问题,网络最后使用softmax层. 它从前一先行层获取输入,并输出给定数量样例上的概率. (所有概率相加的总和必然为1)<br>(当然如果二分类问题想要输出概率也可以用softmax)</li>
</ul>
</li>
<li><p><strong>BN（Batch Normalization）是个好文明</strong></p>
</li>
<li><p>损失函数选择<br>对于回归问题,通常使用MSE(均方误差)<br>对于分类问题,通常使用交叉熵<br>(MSE对于每一个输出的结果都非常看重，而交叉熵只对正确分类的结果看重。)<br>其他的</p>
<p>| 损失函数           | 通常用法 |<br>| —- | —- |<br>| L1 loss            | 通常作为正则化器使用 |<br>| MSE loss           | 均方误差损失,用于回归问题的损失函数 |<br>| Cross-entropy loss | 交叉熵损失,用于二分类和多分类问题  |<br>| NLL Loss           | 用于分类问题,允许用户使用特定的权重处理不平衡数据集 |<br>| NLL Loss2d         | 用于像素级分类,通常和图像分割问题有关|<br><strong>注意</strong>：<br>①<strong>若使用了nn.CrossEntropyLoss() 函数，则最后一层就不应该使用softmax函数！</strong><br>这个函数包含了<strong>log_softmax</strong>，其操作为“使用对数似然损失函数和log_softmax激活函数进行DNN分类输出”  CrossEntropyLoss()=log_softmax() + NLLLoss()<br><strong>但是，使用log_softmax运算是在output输出进入损失函数的时候。如果要获取输出概率，那就得在得到outputs后，在网络外面单独求一个<code>F.softmax(outputs.data,dim=1)</code></strong>(但若使用log_softmax() + NLLLoss()， log_softmax函数是在网络里的（获得网络输出之前），所以不需要在网络外面单独求softmax)<br>②损失函数 CrossEntropyLoss() 与 NLLLoss() 类似<br>③<strong>损失函数NLLLoss()</strong> 的 输入 是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率，<strong>适合最后一层是log_softmax()的网络.</strong>  </p>
</li>
<li><p>log_softmax和softmax的选择<br><strong>损失函数NLLLoss()</strong> 的 输入 是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率，<strong>适合最后一层是log_softmax()的网络.</strong><br>用了log_softmax得到的结果是对softmax求次对数（并非采用直接求 ，而是一种较快的方式）</p>
</li>
<li><p>一般用loader加载的时候，num_workers使用少于机器内核数量的woker是一个通用的实践</p>
</li>
<li><p>权值增速比 0.01比较好</p>
</li>
<li><p><strong>网络结构中的参数计算</strong><br> O=输出图像的尺寸 output<br> I=输入图像的尺寸 input<br> K=卷积层的核尺寸 kernal<br> N=核数量 number of kernal<br> S=移动步长 strike<br> P =填充数 padding<br> ①想要con2d卷积出来的图片尺寸没有变化, </p>
<script type="math/tex; mode=display">P=\frac{K-1}{2}</script><p> 每层连接的参数计算：<a href="https://blog.csdn.net/guo1988kui/article/details/78480282" target="_blank" rel="noopener">详细解释CNN卷积神经网络各层的参数和链接个数的计算</a><br> ②输出图像的大小计算<br> 输出图像尺寸的计算公式如下：</p>
<script type="math/tex; mode=display">O=\frac{I-K+2P}{S}+1</script></li>
<li><p>Softmax 是神经网络中另一种输出层函数，计算输出层的值。主要用于神经网络最后一层，作为输出层进行多分类，是Logistic二分类的推广。也可以单独使用。</p>
</li>
<li><p>batch size的选择<br> <a href="https://www.jiqizhixin.com/articles/2018-07-12-4" target="_blank" rel="noopener">训练神经网络时如何确定batch size</a></p>
</li>
<li><p>学习率变更策略<br> <a href="https://www.jianshu.com/p/1f943de39582" target="_blank" rel="noopener">如何选择最适合你的学习率变更策略</a></p>
</li>
<li><p>神经网络权值初始化</p>
<p> 神经网络初始权值不能全部设置一样，否则学不到<br> <a href="https://blog.csdn.net/u012328159/article/details/80025785" target="_blank" rel="noopener">深度学习中神经网络的几种权重初始化方法</a></p>
</li>
<li><p>常见学习率选择策略</p>
<p>| 策略              | 解释 |<br>| —- | —-|<br>| StepLR            | 步长规则，学习率倍数变化(步长内变gamma倍)  |<br>| MultiStepLR       | 步长不规则，学习率倍数变化 |<br>| ExponentialLR     | 步长为1，学习率乘数变化 |<br>| ReduceLROnPlateau | 常用。当特定的度量指标，如训练损失、验证损失或准确率不再变化时，学习率就会改变。通用实践是降为原来的1/2~1/10 |</p>
</li>
<li><p>池化/卷积 与 ReLU的顺序<br> 在最大池化之后或者在应用卷积<strong>之后使用非线性层</strong>是通用的最佳实践</p>
</li>
<li><p>FC, dropput都是可以有多层，不是只能用一层</p>
</li>
<li><p>深度学习的11个常见陷阱应对方法总结</p>
<p> <a href="https://36kr.com/p/5091825" target="_blank" rel="noopener">神经网络 11 大常见陷阱及应对方法</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>炼丹</tag>
      </tags>
  </entry>
  <entry>
    <title>imageTest</title>
    <url>/2019/01/16/imageTest/</url>
    <content><![CDATA[<p>图片测试</p>
<p> <a id="more"></a> <img src="//aisakaki.com/2019/01/16/imageTest/5474960.png" alt="5474960"></p>
]]></content>
  </entry>
  <entry>
    <title>2080Ti上一体式水冷</title>
    <url>/2019/01/16/2080Ti%E4%B8%8A%E4%B8%80%E4%BD%93%E5%BC%8F%E6%B0%B4%E5%86%B7/</url>
    <content><![CDATA[<p>前段时间买了非公的2080Ti，感觉自带的散热不太行，玩游戏温度飙升到84度直接撞温度墙。于是我搞了个 ocool的北极狼一体式水冷（花了我1300大洋，这水冷都够我买一张1060了orz）</p>
<p>自己动手，丰衣足食~</p>
 <a id="more"></a>
<p>这是原装风冷的样子<img src="//aisakaki.com/2019/01/16/2080Ti上一体式水冷/1969.jpg" alt="正面"></p>
<p>首先拆去原装散热器，脱去外套</p>
<p><img src="//aisakaki.com/2019/01/16/2080Ti上一体式水冷/1973.jpg" alt="背部"></p>
<p>可以看到一件暴露的核心和硅脂</p>
<p>然后卸下底板</p>
<p><img src="//aisakaki.com/2019/01/16/2080Ti上一体式水冷/1984.jpg" alt="背部"></p>
<p>这就是PCB背部了</p>
<p>然后就可以把PCB上的均热板给去掉，完整的PCB板子 就暴露在眼前</p>
<p>可以看到11个gddr6显存，11G，下面那个地方缺了一块强迫症就很难受</p>
<p>16相供电，嗯果然是公版PCB</p>
<p>拆解结束~</p>
<p><img src="//aisakaki.com/2019/01/16/2080Ti上一体式水冷/1989.jpg" alt="背部"></p>
<p>擦掉硅脂，裁剪了一堆导热胶，将显存和供电给覆盖住。可以看到核心上的标签写着TU102核心</p>
<p><img src="//aisakaki.com/2019/01/16/2080Ti上一体式水冷/1992.jpg" alt="背部"></p>
<p>涂上祖传的德国暴力熊硅脂</p>
<p><img src="//aisakaki.com/2019/01/16/2080Ti上一体式水冷/1993.jpg" alt="背部"></p>
<p>然后安装好水冷的冷头和底板，这里要注意安装对齐！第一次就没安装好导致水冷头没和核心密切接触使得温度超高。</p>
<p><img src="//aisakaki.com/2019/01/16/2080Ti上一体式水冷/1995.jpg" alt="背部"></p>
<p>然后就结束啦！2080Ti水冷显卡诞生！</p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>显卡</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习实践结构性经验总结</title>
    <url>/2019/01/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E7%BB%93%E6%9E%84%E6%80%A7%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h2><ul>
<li>定义网络结构</li>
<li>定义数据变换</li>
<li>定义超参数，损失函数，优化方法，自适应学习率等…</li>
<li><p>读取训练集与测试集（自定义数据集，多种读入方式等..）<br><strong>开始训练</strong>：</p>
<ul>
<li><p><strong>训练一个Epoch</strong><br>自适应学习率更新<br><strong>循环：训练一个batch——从dataloader读取一个mini batch</strong></p>
<ul>
<li>转换为cuda变量</li>
<li>梯度清0<br><strong>FORWARD</strong></li>
<li>将数据输入模型得到结果</li>
<li>根据结果得到预测标签</li>
<li>计算train loss<br><strong>BACKWARD</strong></li>
<li>反向计算梯度</li>
<li>优化器更新<br><strong>统计本batch</strong></li>
</ul>
</li>
<li><strong>统计本epoch</strong></li>
</ul>
</li>
</ul>
<p>【注】<br>①如果是<strong>测试</strong>，则取消[自适应学习率更新]和[BACKWARD]的步骤即可<br><strong>训练和测试可以结合起来</strong> train=datalodaer(trainset…),test=dataloader(testset…)<br>则在开始训练一个epoch的时候，加一个循环 <code>for phase in [train,test]</code>即可，后面的在测试不需要的步骤作判断<code>if phase is train: do</code>就行<br>②可以设置一个全局的最优准确率，然后记录下来最佳准确率，并储存当时的<strong>最优模型</strong>权重<br>③注意<strong>设置模型模式</strong>，<code>model.train(True)</code>设置为训练模式<code>model.eval(True)</code>设置为测试模式</p>
<h2 id="定义网络结构"><a href="#定义网络结构" class="headerlink" title="定义网络结构"></a>定义网络结构</h2><p>在定义网络结构的时候,如下形式<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义的网络结构类必须继承自nn.Module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 注意传入参数self到结构函数</span></span><br><span class="line">    <span class="comment"># init中的参数为初始化网络的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__</span><br><span class="line">        <span class="comment"># 定义网络</span></span><br><span class="line">        <span class="comment"># 单个定义</span></span><br><span class="line">        self.s1 = nn.Conv2d(......)</span><br><span class="line">        <span class="comment"># 或集合定义</span></span><br><span class="line">        self.s1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(......),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 全连接层(对于卷积神经网络用于分类）</span></span><br><span class="line">        self.fc = nn.Linear()</span><br><span class="line">    <span class="comment"># 注意传入参数self和x（输入矩阵）！</span></span><br><span class="line">    <span class="comment"># forward中的参数为训练的时候要传入的参数 比如求output = model(x)时</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># 经过特征提取层后，要扁平化才能进入fc层！</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) </span><br><span class="line">        <span class="keyword">return</span> F.softmax(x)</span><br></pre></td></tr></table></figure></p>
<p>注意，一个误区：定义网络是定义网络的结构！输入一般输入一个batch，对该batch的处理为网络的内容</p>
<p>nn包含了所有的层结构定义组件, F包含了所有的函数, nn中的每个组件都需要添加到nn.Module容器中才能使用. def参数中的self即指向nn.Module(指向本类,而本类继承自nn.Module), 通过定义self.diyiceng = nn.Conv2d(xxx) 即将组件添加到了nn.Module容器中 </p>
<h2 id="nn-Sequential-组合一系列组件定义"><a href="#nn-Sequential-组合一系列组件定义" class="headerlink" title="nn.Sequential 组合一系列组件定义"></a><strong>nn.Sequential</strong> 组合一系列组件定义</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#eg</span></span><br><span class="line">self.conv2 = nn.Sequential( </span><br><span class="line">	nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>), </span><br><span class="line">	nn.ReLU(),  </span><br><span class="line">	nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">)  </span><br><span class="line"><span class="comment">#这里就将三个组件组合成一个组件了</span></span><br><span class="line"><span class="comment">#注意里面直接写组件组成即可 不是变量定义</span></span><br></pre></td></tr></table></figure>
<h2 id="建立深度学习模型一般实践"><a href="#建立深度学习模型一般实践" class="headerlink" title="建立深度学习模型一般实践"></a>建立深度学习模型一般实践</h2><p><strong>数据预处理与特征工程</strong></p>
<ul>
<li>向量化</li>
<li>值归一化</li>
<li>处理缺省值</li>
<li>特征工程<br><strong>过拟合</strong></li>
<li>获取更多数据、数据增强等</li>
<li>缩小网络规模</li>
<li>应用权重正则化</li>
<li>应用dropout<br><strong>欠拟合</strong></li>
<li>获取更多数据</li>
<li>增加权重</li>
</ul>
<p><strong>一般流程</strong></p>
<ul>
<li>问题定义与数据集创建</li>
<li>选择模型评估标准</li>
<li>评估协议（测试集 验证集）</li>
<li>准备数据</li>
<li>模型基线（创建一个非常简单的模型来打破基线分数 如二分类为0.5)</li>
<li>训练大达到过拟合</li>
<li>应用正则化</li>
<li>学习率选择策略</li>
</ul>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>向目标函数加入正则的具体实现</title>
    <url>/2018/12/01/%E5%90%91%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%8A%A0%E5%85%A5%E6%AD%A3%E5%88%99%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<ol>
<li><p>在数据经过模型之后</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l1, l2 = torch.tensor(<span class="number">0</span>), torch.tensor(<span class="number">0</span>)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">output = model(input)</span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> model.parameters():</span><br><span class="line">    l1 += torch.norm(param,<span class="number">1</span>)  <span class="comment">#torch.norm(param,1)为求param的l1范数</span></span><br><span class="line">    l2 += torch.norm(param,<span class="number">2</span>)  <span class="comment">#torch.norm(param,2)为求param的l2范数</span></span><br><span class="line">loss_origin = loss(output,label)</span><br><span class="line">loss = loss_origin + l1 + l2</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>ATT：<strong>不能</strong>使用在模型中输出！</p>
<p>使用以下这种方法是输出的中间层的输出，而不是参数！</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#错误示范</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">32</span>,<span class="number">16</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></span><br><span class="line">        linear1_out = self.linear1(input)</span><br><span class="line">        linear2_out = self.linear2(linear1_out)</span><br><span class="line">        <span class="keyword">return</span> linear1_out,linear2_out</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用pytorch自带的权重衰减</p>
<p><code>torch.optim</code>自带的参数<code>weight_decay</code>可以设置权值衰减率，即可以实现L2正则化</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>维度及Pytorch中常用的维度变换与torch.max()</title>
    <url>/2018/11/12/Pytorch%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2%E4%B8%8Etorch-max/</url>
    <content><![CDATA[<h2 id="Tensor-view-扁平化"><a href="#Tensor-view-扁平化" class="headerlink" title="Tensor.view() 扁平化"></a>Tensor.view() 扁平化</h2><p>简单说就是一个把tensor 进行reshape的操作。<br>常常配合 <code>a.size(0)</code>来取得某一位的维度数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">7</span>)</span><br><span class="line">b = a.view(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">print(b.size())</span><br><span class="line">&gt;&gt;torch.Size([<span class="number">1</span>, <span class="number">420</span>])</span><br></pre></td></tr></table></figure>
<p><strong>其中参数-1表示剩下的值的个数一起构成一个维度</strong>。如上例中，第一个参数1将第一个维度的大小设定成1，后一个-1就是说第二个维度的大小=元素总数目/第一个维度的大小，此例中为<code>3*4*5*7/1=420</code>. （由于第一个维度被展为1了，那后面所有元素全部摊开展平了）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第1,2个维度不变（保持原来的维度），第三个位置写-1表示剩下的维度全部扁平化为一个维度</span></span><br><span class="line">d = a.view(a.size(<span class="number">0</span>),a.size(<span class="number">1</span>),<span class="number">-1</span>)</span><br><span class="line">print(d.size())</span><br><span class="line">&gt;&gt;torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">35</span>])</span><br><span class="line"></span><br><span class="line"> e=a.view(<span class="number">4</span>,<span class="number">-1</span>,<span class="number">5</span>)</span><br><span class="line"> print(e.size())</span><br><span class="line"> &gt;&gt;torch.Size([<span class="number">4</span>, <span class="number">21</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p> 在<strong>扁平化批处理的图片</strong>的时候（CNN中会出现）<br> 为了扁平化每一张图片的数据，又不想把不同图片的数据混在一起扁平，那么就应该</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意：遇到错误</strong><code>RuntimeError: invalid argument 2: view size is not compatible with input tensor&#39;s size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view().</code><br>需要让Tensor连续化，即加一步<code>x = x.contiguous()</code><br>原因解释:<a href="https://blog.csdn.net/appleml/article/details/80143212" target="_blank" rel="noopener">https://blog.csdn.net/appleml/article/details/80143212</a>使用torch.view()的条件是Tensor是连续的,在对Tensor进行了transpose等操作之后就会变得不连续，就需要这个函数来连续化</p>
<h2 id="unsqueeze降维-和squeeze升维"><a href="#unsqueeze降维-和squeeze升维" class="headerlink" title="unsqueeze降维 和squeeze升维"></a>unsqueeze降维 和squeeze升维</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">对于torch:使用unsqueeze降维 和squeeze升维</span><br><span class="line"><span class="comment">#升维</span></span><br><span class="line">unsqueeze(torch1,<span class="number">0</span>)  <span class="comment">#在第0个位置增加一个值为1的维度</span></span><br><span class="line"><span class="comment">#(2, 2)-&gt;(1,2,2)</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#降维</span></span><br><span class="line">squeeze(torch1)  <span class="comment">#squeeze给torch删去值为1的维度</span></span><br><span class="line"><span class="comment">#(2, 2, 1)-&gt;(2, 2)</span></span><br><span class="line">    </span><br><span class="line">比如灰度图像只有<span class="number">2</span>维,这个时候就需要升维</span><br><span class="line">    </span><br><span class="line">对于numpy array,函数不一样:expand_dims 升维 squeeze 降维</span><br><span class="line"><span class="comment">#升维</span></span><br><span class="line">expand_dims(narray1,<span class="number">0</span>) <span class="comment">#在第0个位置增加一个值为1的维度</span></span><br><span class="line">与torch用法一致</span><br></pre></td></tr></table></figure>
<h2 id="torch-max函数"><a href="#torch-max函数" class="headerlink" title="torch.max函数"></a>torch.max函数</h2><p>①torch.max()简单来说是返回一个tensor中的最大值。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">si=torch.randn(<span class="number">4</span>,<span class="number">5</span>)  </span><br><span class="line">print(si)  </span><br><span class="line">&gt;&gt;tensor([[ <span class="number">1.1659</span>, <span class="number">-1.5195</span>, <span class="number">0.0455</span>, <span class="number">1.7610</span>, <span class="number">-0.2064</span>],  </span><br><span class="line">[<span class="number">-0.3443</span>, <span class="number">2.0483</span>, <span class="number">0.6303</span>, <span class="number">0.9475</span>, <span class="number">0.4364</span>],  </span><br><span class="line">[<span class="number">-1.5268</span>, <span class="number">-1.0833</span>, <span class="number">1.6847</span>, <span class="number">0.0145</span>, <span class="number">-0.2088</span>],  </span><br><span class="line">[<span class="number">-0.8681</span>, <span class="number">0.1516</span>, <span class="number">-0.7764</span>, <span class="number">0.8244</span>, <span class="number">-1.2194</span>]])</span><br><span class="line"></span><br><span class="line">print(torch.max(si))  </span><br><span class="line">&gt;&gt;tensor(<span class="number">2.0483</span>) <span class="comment">#输出了最大值</span></span><br></pre></td></tr></table></figure>
<p>②这个函数的参数中还有一个dim参数，使用方法为re = torch.max(Tensor,dim),<strong>返回的res为一个</strong>二维向量<strong>，</strong>其中res[0]为最大值的Tensor，res[1]为Tensor中最大值对应的index**。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(torch.max(si,<span class="number">0</span>))</span><br><span class="line">(tensor([<span class="number">1.1659</span>, <span class="number">2.0483</span>, <span class="number">1.6847</span>, <span class="number">1.7610</span>, <span class="number">0.4364</span>]), tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(torch.max(si,<span class="number">0</span>)[<span class="number">0</span>])  </span><br><span class="line">tensor([<span class="number">1.1659</span>, <span class="number">2.0483</span>, <span class="number">1.6847</span>, <span class="number">1.7610</span>, <span class="number">0.4364</span>])</span><br></pre></td></tr></table></figure>
<p>注意，Tensor的维度从0开始算起。在torch.max()中指定了dim之后，比如对于一个3x4x5的    Tensor，指定dim为0后，得到的结果是维度为0的“每一行”对应位置求最大的那个值，此时输出的Tensor的维度是4x5. （<strong>在这个二维tensor里，dim=0就是求每行最大值</strong>）</p>
<p>对于简单的二维Tensor，如上面例子的这个4x5的Tensor。指定dim为0，则给出的结果是4行做比较之后的最大值；如果指定dim为1，则给出的结果是5列做比较之后的最大值，且此处做比较时是按照位置分别做比较，得到一个新的Tensor。（<strong>在这个二维tensor里，dim=1就是求每列最大值</strong>）</p>
<p>eg<br><code>_, preds = torch.max(outputs.data, 1)</code></p>
<p>输出的outputs也是torch.autograd.Variable格式，得到输出后（网络的全连接层的输出）还希望能到到模型预测该样本属于哪个类别的信息，这里采用torch.max。torch.max()的第一个输入是tensor格式，所以用outputs.data而不是outputs作为输入；第二个参数1是代表dim的意思，也就是取每一行的最大值，<strong>其实就是我们常见的取概率最大的那个index</strong>；第三个参数loss也是</p>
<p>这个函数可以直接处理组</p>
<h2 id="对于python中的维度的理解-tensor-array等"><a href="#对于python中的维度的理解-tensor-array等" class="headerlink" title="对于python中的维度的理解(tensor array等)"></a>对于python中的<strong>维度</strong>的理解(tensor array等)</h2><p>[   [    [    ]   ]   ]<br>最外层是第0维，中间层是第1维，最里层是第二维<br>操作哪一维，就相当于该维以外的维度都<strong>固定</strong>住，操作只在该维下进行，对于更高维度（外维），<strong>都执行相同的操作</strong></p>
<p>对于网络中fc层之后还要返回计算softmax的值的时候，应该选择<code>return F.los_softmax(x,dim=1)</code><br>因为经过fc层之后的矩阵结构为[[a1,a2],[b1,b2]]  (假设二分类，batch=2）<br>则要对[a1,a2]求softmax，再对[b1,b2]求softmax，所以是固定第0维，操作第1维，对第一维的每组求softmax</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>对BP算法的理解</title>
    <url>/2018/11/08/%E5%AF%B9%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>损失函数$J(W_{11},W_{12},\cdots,W_{ij},\cdots,W{mn})$</p>
<p>要计算梯度$\nabla J=\frac{\partial J}{\partial W_{11}}e_{11}+\cdots+\frac{\partial J}{\partial W_{mn}}e_{mn} $，$e_{ij}$为单位正交向量（$J$对每个参数求偏导）</p>
<p>那么使用链式法则，我们可以画出计算图</p>
<p>①先标记出所有中间结点</p>
<p>②对每一个结点，写出<strong>上一层直连结点</strong>（最顶层就是$J$）对该节点求得的偏导式（偏的是本结点中间变量），其他变量都视为常数</p>
<p>③根据计算图，使用链式法则（两个结点之间，偏导式同路径相乘，所有路径相加）</p>
<p>主要理解清，某一个中间结点对另一个邻层中间结点求导，<strong>是对中间变量求导</strong>，只看局部，<strong>其它的变量都视为常数</strong>，都先只计算<strong>邻接层求导</strong>，再分别计算完每个偏导式之后，再运用链式法则根据全路径计算<strong>跨层求导</strong></p>
<p>这样一来会有很多的重复计算（重复路径）。</p>
<p>为了解决这个问题，BP的做法是<strong>自顶向下，逐层反向求导，累计计算</strong></p>
<p>也就是说从<strong>最高层$J$开始</strong>，<strong>按层计算</strong>出该层的偏导式值，然后再把这个值（<strong>是个含变量的表达式，但是对于这个中间层来说是个常数</strong>）发给这个结点下属路径下的每个结点（让他们乘上这个上层来的偏导式），然后再计算下一层的偏导式，如此<strong>迭代到输入层</strong></p>
<p>注意都是先推导式子，最后再代入值计算，比如输入以及真实label都是最后推导完式子后计算的时候的一个常数值</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title>notes_1108</title>
    <url>/2018/11/08/notes-1108/</url>
    <content><![CDATA[<ol>
<li><p>NN的输出是一个向量，包含对各个种类的输出值，最后一个层有多少个神经元，输出向量就有多少维，所以在求的时候是矩阵求梯度下降，Loss Function作用于一个矩阵</p>
</li>
<li><p>NN为什么用非线性激活函数</p>
<p>线性函数公式写出来之后 会发现 无论多少层，他都只能生成线性分类器（成为了感知机）<br>无法去拟合绝大多数的真实情况<br>而非线性的组合 就可以去逼近任何函数</p>
</li>
<li><p>关于梯度下降的算法<br>一个训练样本的损失函数就是一个向量，有多少特征就有多少维<br>ML算法中的代价函数通常可以分解成每个样本的代价函数的总和<br>具体求和之后如何求导见1</p>
<p>于是不使用batch的整体梯度下降需要计算($m$为训练集样本个数)。所有样本计算出最终结果来计算梯度。这样对显存的要求就极高（在大样本集上是不可能）</p>
<script type="math/tex; mode=display">
     \nabla_{\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^m\nabla_\theta L(x^{(i)},y^{(i)},\theta)</script><p>如果使用minibatch，则变成（一个batch有$m’$个训练样本） 一个batch计算一个$g$来更新梯度</p>
<script type="math/tex; mode=display">
 g=\frac{1}{m'}\sum_{i=1}^{m'}\nabla_\theta L(x^{(i)},y^{(i)},\theta)</script><p>然后使用如下的梯度下降算法估计梯度</p>
<script type="math/tex; mode=display">
    \theta \leftarrow \theta - \epsilon g</script><p>于是在一轮训练集中，要更新很多次梯度，目标去逼近整体更新梯度的方向<br>在一轮中的几个批次，可能是局部梯度减小，整体损失变大，但是在一轮下来之后，会变成朝着整体减小的方向震荡前进</p>
</li>
<li><p>CNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数，所谓权值共享就是同一个Feature Map中神经元权值共享，该Feature Map中的所有神经元使用同一个权值。因此参数个数与神经元的个数无关，只与卷积核的大小及Feature Map的个数相关。但是共有多少个连接个数就与神经元的个数相关了，神经元的个数也就是特征图的大小。</p>
</li>
<li><p>为什么用交叉熵作损失函数？每层怎么做计算<br><a href="https://blog.csdn.net/huwenxing0801/article/details/82791879" target="_blank" rel="noopener">https://blog.csdn.net/huwenxing0801/article/details/82791879</a><br>对于分类问题，用交叉熵损失函数更好，因为交叉熵函数公式可知道，<strong>交叉熵只关心对正确类别的预测概率</strong>，因为只要其值<strong>足够大</strong>，就可以保证分类结果正确。<br>对于回归问题，用平方差损失函数更好，它希望<strong>预测概率尽量完全等于标签概率</strong>。（比交叉熵损失函数严格很多）</p>
</li>
<li><p>前馈神经网络，如BP，RFN</p>
<p>循环神经网络，RNN</p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
  </entry>
  <entry>
    <title>使用markdown编辑数学公式</title>
    <url>/2018/11/07/%E4%BD%BF%E7%94%A8markdown%E7%BC%96%E8%BE%91%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    <content><![CDATA[<p>Typora是我认为最好用的markdown编辑器，书写直观，数学公式编辑器非常方便，总结一下在上面编辑数学公式的语法。</p>
<p>注意，首先要在文件—偏好设置—Markdown关联语法 里勾选内联公式</p>
<h3 id="创建公式编辑栏的方式"><a href="#创建公式编辑栏的方式" class="headerlink" title="创建公式编辑栏的方式"></a>创建公式编辑栏的方式</h3><ul>
<li>内联公式：两个$之间​</li>
<li>公式块：两个$$之间</li>
<li>ctrl + shift +m</li>
</ul>
<h3 id="常用公式的代码"><a href="#常用公式的代码" class="headerlink" title="常用公式的代码"></a>常用公式的代码</h3><p>   注意，一个块要用{}包裹起来，^_上下标叠着写就行</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">输入</th>
<th style="text-align:center">公式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">x^2</td>
<td style="text-align:center">$x^2$</td>
</tr>
<tr>
<td style="text-align:center">x_1</td>
<td style="text-align:center">$x_1$</td>
</tr>
<tr>
<td style="text-align:center">\frac{x}{y}</td>
<td style="text-align:center">$\frac{x}{y}$</td>
</tr>
<tr>
<td style="text-align:center">\sqrt[x]{y}</td>
<td style="text-align:center">$\sqrt[x]{y}$</td>
</tr>
<tr>
<td style="text-align:center">\vec{x}</td>
<td style="text-align:center">$\vec{x}$</td>
</tr>
<tr>
<td style="text-align:center">\int_{a}^{b}{x}dx</td>
<td style="text-align:center">$\int_{a}^{b}{x}dx$</td>
</tr>
<tr>
<td style="text-align:center">\cdots</td>
<td style="text-align:center">$\cdots$</td>
</tr>
<tr>
<td style="text-align:center">\sum_{n=1}^{100}{a_n}</td>
<td style="text-align:center">$\sum_{n=1}^{100}{a_n}$</td>
</tr>
<tr>
<td style="text-align:center">\lim_{n\to +\infty}</td>
<td style="text-align:center">$\lim_{n\to +\infty}$</td>
</tr>
<tr>
<td style="text-align:center">\prod_{n=1}^{99}{x_n}</td>
<td style="text-align:center">$\prod_{n=1}^{99}{x_n}$</td>
</tr>
<tr>
<td style="text-align:center">a \quad b</td>
<td style="text-align:center">$a \quad b$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="特殊字符"><a href="#特殊字符" class="headerlink" title="特殊字符"></a>特殊字符</h3>   <a id="more"></a>
<p>   希腊字母、三角函数、对数函数直接写转义字符+对应的内容即可，比如</p>
<p>   对于如希腊字母，第一个字母大写就对应其大写字母，小写即对应其小写字母</p>
<p>   \sinx = $\sin x$    \log_2 = $\log_2$</p>
<p>   \lambda=$\lambda$     \theta=$\theta$   \Omega=$\Omega$</p>
<p>   \Delta=$\Delta$</p>
<p>   <strong>梯度\nabla</strong>=$\nabla$</p>
<h3 id="其它特殊符号"><a href="#其它特殊符号" class="headerlink" title="其它特殊符号"></a>其它特殊符号</h3><p>   前三行关系运算符</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">输入</th>
<th style="text-align:center">公式</th>
<th style="text-align:center">输入</th>
<th style="text-align:center">公式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\pm</td>
<td style="text-align:center">$\pm$</td>
<td style="text-align:center">\cdot</td>
<td style="text-align:center">$\cdot$</td>
</tr>
<tr>
<td style="text-align:center">\div</td>
<td style="text-align:center">$\div$</td>
<td style="text-align:center">\leq</td>
<td style="text-align:center">$\leq$</td>
</tr>
<tr>
<td style="text-align:center">\geq</td>
<td style="text-align:center">$\geq$</td>
<td style="text-align:center">\partial</td>
<td style="text-align:center">$\partial$</td>
</tr>
<tr>
<td style="text-align:center">\in</td>
<td style="text-align:center">$\in$</td>
<td style="text-align:center">\notin</td>
<td style="text-align:center">$\notin$</td>
</tr>
<tr>
<td style="text-align:center">\cup</td>
<td style="text-align:center">$\cup$</td>
<td style="text-align:center">\cap</td>
<td style="text-align:center">$\cap$</td>
</tr>
<tr>
<td style="text-align:center">\subset</td>
<td style="text-align:center">$\subset$</td>
<td style="text-align:center">\subseteq</td>
<td style="text-align:center">$\subseteq$</td>
</tr>
<tr>
<td style="text-align:center">\supset</td>
<td style="text-align:center">$\supset$</td>
<td style="text-align:center">\supseteq</td>
<td style="text-align:center">$\supseteq$</td>
</tr>
<tr>
<td style="text-align:center">\forall</td>
<td style="text-align:center">$\forall$</td>
<td style="text-align:center">\infty</td>
<td style="text-align:center">$\infty$</td>
</tr>
<tr>
<td style="text-align:center">\varnothing</td>
<td style="text-align:center">$\varnothing$</td>
<td style="text-align:center">\exists</td>
<td style="text-align:center">$\exists$</td>
</tr>
<tr>
<td style="text-align:center">\because</td>
<td style="text-align:center">$\because$</td>
<td style="text-align:center">\therefore</td>
<td style="text-align:center">$\therefore$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="大括号"><a href="#大括号" class="headerlink" title="大括号"></a>大括号</h3>   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c(u)=</span><br><span class="line"></span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line"></span><br><span class="line"> formula1,condition1\\ </span><br><span class="line"></span><br><span class="line">formula2,condition2 </span><br><span class="line"></span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure>
<p>   $c(u)=\begin{cases} formula1,condition1\\\ formula2,condition2 \end{cases}$</p>
<p>   <code>以\begin{cases}开始，以\end{cases}结束，中间行间隔用\\，每行的条件和公式之间用逗号相隔</code></p>
<h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3>   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\</span><br><span class="line">&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\</span><br><span class="line">&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\</span><br><span class="line">&#123;a_&#123;m1&#125;&#125;&amp;&#123;a_&#123;m2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;mn&#125;&#125;\\</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure>
<p>   $\begin{bmatrix}<br>   {a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\<br>   {a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\<br>   {\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>   {a_{m1}}&amp;{a_{m2}}&amp;{\cdots}&amp;{a_{mn}}\\<br>   \end{bmatrix}$</p>
<p><em>关于HEXO使用mathjax渲染公式的问题见此篇文章<a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a></em></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>更换HEXO框架</title>
    <url>/2018/11/06/%E6%9B%B4%E6%8D%A2HEXO%E6%A1%86%E6%9E%B6/</url>
    <content><![CDATA[<p>这两天将更换了个人网站的框架</p>
<p>更改框架是个蛮麻烦的过程</p>
<p>其中以前的文章很多格式都乱掉了</p>
<p>而且图片全部没有了…….</p>
<p>只能手动修正了</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch常用模块以及nn.Module</title>
    <url>/2018/10/18/Pytorch%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%E4%BB%A5%E5%8F%8Ann-Module/</url>
    <content><![CDATA[<h2 id="pytroch常用包"><a href="#pytroch常用包" class="headerlink" title="pytroch常用包"></a>pytroch常用包</h2><p>PyTorch框架中有一个很常用的包：<code>torchvision</code><br>torchvision主要由3个子包构成：<code>torchvision.datasets</code>、<code>torchvision.models</code>、<code>torchvision.transforms</code><br><code>torchvision</code>和<code>torchtext</code>基于<code>torch.utils.data.Dataset</code>和<code>torch.utils.data.DataLoder</code>类构建<br><code>nn</code>来源于<code>torch.nn</code><br><code>optim</code>来源于<code>torch.optim</code> 超参数算法都在里面<br><code>lr_scheduler</code>来源于<code>torch.optim.lr_scheduler</code> 自适应学习率算法在里面<br><code>ImageFolder</code>在<code>torchvision.datasets</code>中<br><code>torchvision.models</code>中提供一些现有的流行算法</p>
<p><code>torch.nn.functional</code> 各中层函数的实现，与层类型对应，如：卷积函数、池化函数、归一化函数等等</p>
<h2 id="torch-nn与nn-Module"><a href="#torch-nn与nn-Module" class="headerlink" title="torch.nn与nn.Module"></a>torch.nn与nn.Module</h2><p>torch.nn是专门为神经网络设计的模块化接口。nn构建于autograd之上，可以用来定义和运行神经网络。</p>
<p>torch.nn的核心数据结构是<code>Module</code>，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。无需纠结variable和tensor了，0.4版本已经将两个类彻底合并了。</p>
<p>在实际使用中，最常见的做法是<strong>继承</strong><code>nn.Module</code>，撰写自己的网络/层。</p>
<ul>
<li><p>自定义层<code>Linear</code>必须继承<code>nn.Module</code>，并且在其构造函数中需调用<code>nn.Module</code>的构造函数，即<code>super(Linear, self).__init__()</code>  或<code>nn.Module.__init__(self)</code>，推荐使用第一种用法。</p>
</li>
<li><p>在<strong>构造函数<code>__init__</code>中必须自己定义可学习的参数，并封装成<code>Parameter</code></strong>，比如 _FasterRcnn类init中定义了 self.RCNN_loss_cls = 0 和 self.RCNN_loss_bbox = 0 还有在本例中我们把<code>w</code>和<code>b</code>封装成<code>parameter</code>。<strong><code>parameter</code>是一种特殊的<code>Variable</code>，但其默认需要求导</strong>（requires_grad = True）。</p>
</li>
<li><p><code>forward</code>函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。</p>
</li>
<li><p><strong>无需写反向传播函数</strong>，<strong>因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这点比Function简单许多。 faster rcnn等中反向传播直接写pass的原因。</strong></p>
</li>
<li><p>使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于<code>layers.__call__(input)</code>，在<code>__call__</code>函数中，主要调用的是  <code>layer.forward(x)</code>，另外还对钩子做了一些处理。所以在实际使用中<strong>应尽量使用<code>layer(x)</code>而不是使用<code>layer.forward(x)</code></strong>。</p>
</li>
<li><p><code>Module</code>中的<strong>可学习参数</strong>可以<strong>通过<code>named_parameters()</code>或者<code>parameters()</code>返回迭代器</strong>，前者会给每个parameter都附上名字，使其更具有辨识度。</p>
<p>Module能够自动检测到自己的<code>Parameter</code>，并将其作为学习参数。</p>
</li>
</ul>
<h2 id="nn-ReLU和F-ReLU有什么区别"><a href="#nn-ReLU和F-ReLU有什么区别" class="headerlink" title="nn.ReLU和F.ReLU有什么区别?"></a>nn.ReLU和F.ReLU有什么区别?</h2><p>将ReLU层添加到网络有两种不同的实现，即nn.ReLU和F.ReLU两种实现方法。<br>其中nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用，看上去作为一个函数调用更方便更简洁。具体使用哪种方式，取决于编程风格。在PyTorch中,nn.X都有对应的函数版本F.X，但是并不是所有的F.X均可以用于forward或其它代码段中，因为当网络模型训练完毕时，在存储model时，在forward中的F.X函数中的参数是无法保存的。也就是说，在forward中，使用的F.X函数一般均没有状态参数，比如F.ReLU，F.avg_pool2d等，均没有参数，它们可以用在任何代码片段中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nn.Softmax() = nn.Linear() + forward最后输出的时候调用F.softmax()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN的卷积过程理解</title>
    <url>/2018/10/15/CNN%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="CNN的卷积层"><a href="#CNN的卷积层" class="headerlink" title="CNN的卷积层"></a>CNN的卷积层</h2><p>这就涉及到CNNs要做的工作了。每一个卷积核中的数值，都是<strong>算法自己学习来的</strong>，不需要我们费心去设置。<br>【CNN的特征提取部分和FC部分一样，也是需要训练的。训练学得Fliter的具体值，然后学得图像特征<br>相当于Fliter是W，图片是X，最终提取得到的特征是Y 输入进FC里（全连接层）<br>训练过程中，W不断学习得到最佳Fliter<br>（Filter其实就是权值！每个像素的权值！二维的<br>所以在最开始的FIlter时候其实就是权值初始化的问题,后面算法自己会学到）<br>】<br>我们需要做的是设置超参数（学习超参数）</p>
<p><strong>所谓的卷积层conv就是一堆卷积核，卷积层的参数就是卷积核的参数</strong></p>
<p>CONV-&gt;ACTIVATE-&gt;POOL-&gt; XXXXXXXX -&gt;FC</p>
<h2 id="多通道卷积——理解卷积层的工作原理"><a href="#多通道卷积——理解卷积层的工作原理" class="headerlink" title="多通道卷积——理解卷积层的工作原理"></a>多通道卷积——理解卷积层的工作原理</h2><p>非常重要<br><strong>输出的层数必为卷积核的层数，以卷积核的视角来做卷积，一个卷积核生成一个卷积层！</strong></p>
<p><img src="//aisakaki.com/2018/10/15/CNN的卷积过程理解/神经网络实践_多通道卷积.png" alt="神经网络实践_多通道卷积">**</p>
<p><strong>同一个卷积核对所有通道做卷积然后进行求和得到该卷积核对应的卷积层</strong></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>中秋</title>
    <url>/2018/09/25/%E4%B8%AD%E7%A7%8B/</url>
    <content><![CDATA[<p>中秋到了，月儿圆了。</p>
 <a id="more"></a>
<p>遥想起两年前的中秋节，那时候正还看着动画，正好那天是喜欢的七海死的那集。那段时间开始入了另一个大坑，认识了很多人。</p>
<p>去年的这个时候，因为学业繁忙，甚至都没有时间去赏赏月，吃吃月饼。今年也是把去年的月饼给补上了。</p>
<p>然后这就是两年过去了。跨过了毕业的槛，现在正坐在实验室里，对着面前的数据绞尽脑汁。</p>
<p>经历了动荡的一年，现在，这几天算是终于安稳下来了。有了自由的空间，有了喜欢的研究的东西，电脑也配好了，也能和朋友一起玩了。</p>
<p>即将到来的是9天国庆，又能够好好休息一阵子。</p>
<p>远离了喧嚣（我也不知道是好是坏，对我来说，也是有两面性的。）也终于能够开始做高中毕业就要想做的那些事了。</p>
<p>さ、はじまるよ</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>notes_0915</title>
    <url>/2018/09/15/notes-0915/</url>
    <content><![CDATA[<ol>
<li><p>尽量少使用显式循环<br>用向量化（并行运算） 即将元素作为向量进行计算<br>numpy.dot(x,y)<br>numpy.exp(v)<br>等等 numpy有很多向量化函数<br>即为点乘<br>平时计算的时候也可以直接用向量化计算，速度快很多</p>
</li>
<li><p>神经网络——堆叠的线性分类器<br>一个网元就是一个线性分类器</p>
</li>
<li><p>激活函数的选择 ReLU&gt;sigmod<br>ReLU最常用，sigmod基本不用</p>
</li>
<li><p>tensor = 张量<br>在pytorch中，matrix即为张量</p>
</li>
<li><p>tensor，list，numpy array, tensor list的 <strong>转换方法</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]]</span><br><span class="line">tensor = torch.Tensor(list) <span class="comment">#List-&gt;Tensor</span></span><br><span class="line">narray = numpy.array(list)  <span class="comment">#List-&gt;narray</span></span><br><span class="line">tensor = torch.from_numpy(narray) <span class="comment">#narray-&gt;tensor</span></span><br><span class="line">narray = tensor.numpy()   <span class="comment">#Tensor-&gt;narray</span></span><br><span class="line">注意list narray tensor之间的转换方法</span><br></pre></td></tr></table></figure>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">将tensor list转变为tensor： </span><br><span class="line">用torch.stack而不是torch.Tensor</span><br><span class="line">tensor_list = [tensor0,tensor1,tensor2..]=&gt;tensor</span><br><span class="line">tensor = torch.stack(tensor_list)</span><br><span class="line">注意tensor中的格式必须相同，如果想要和list一样合并变长tensor必须用cat）</span><br></pre></td></tr></table></figure>
 <a id="more"></a>
</li>
<li><p>Tensor数学运算</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor.sub(b)  <span class="comment">#tensor-b</span></span><br><span class="line">fin = torch.add(tensor1,tensor2) <span class="comment">#fin = tensor1+tensor2</span></span><br><span class="line">(mul div同add)</span><br><span class="line"></span><br><span class="line">torch.sum()</span><br><span class="line"></span><br><span class="line"><span class="comment">#批矩阵相乘函数</span></span><br><span class="line">torch.bmm(A,B)</span><br></pre></td></tr></table></figure>
<p> 对numpy array求math function，不要用math.function，直接用numpy.function如</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.exp(nparray)</span><br></pre></td></tr></table></figure>
<p> 如果对tensor求math函数的报错<code>only one element tensors can be converted to Python scalars</code>的话，就将tensor转化为numpy array再用numpy库的数学函数求</p>
</li>
<li><p><strong>torch.Varibale和torch.Tensor用法基本一致，可以替换使用</strong><br><code>Variable</code>包装了一个<code>Tensor</code>，并且保存着梯度和创建这个<code>Variable</code>function的引用<br>本质上Variable和Tensor没有什么区别，<br>不过Variable会放在一个计算图里面，<br>可以进行前向传播和反向传播以及求导</p>
</li>
<li><p>显卡信息<br>nvidia-smi</p>
</li>
<li><p><code>sum(condition)</code><br>计算满足condition的总数<br>eg：<code>sum(predict == label)</code><br>preidict 是一个list<br>label是一个list<br>他们形状相同<br>则统计两个列表中对应元素相同的个数</p>
</li>
<li><p>batch<br>每一次从训练集/验证集/预测集 里面取数（从dataloader里取），都是以一个batch为一组来取<br>按batch进行训练的模型，在使用的时候也是按batch进行预测，输出的也是按batch的输出值</p>
</li>
<li><p>释放CUDA显存<br> <code>torch.cuda.empty_cache()</code></p>
</li>
<li><p>保存和加载模型</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#直接保存模型和参数</span></span><br><span class="line">torch.save(model_object, <span class="string">'resnet.pth'</span>)</span><br><span class="line"><span class="comment">#直接加载模型和参数</span></span><br><span class="line">model = torch.load(<span class="string">'resnet.pth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#但要注意！ 加载的时候，得先运行网络的定义！（即要先有网络结构，它才会把加载的参数填入网络）</span></span><br></pre></td></tr></table></figure>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载预训练模型</span></span><br><span class="line">https://blog.csdn.net/lscelory/article/details/<span class="number">81482586</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>测试的时候爆显存</strong>有可能是忘记设置no_grad, 示例代码如下：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> ii,(inputs,filelist) <span class="keyword">in</span> tqdm(enumerate(test_loader), desc=<span class="string">'predict'</span>):</span><br><span class="line">            <span class="keyword">if</span> opt.use_gpu:</span><br><span class="line">                inputs = inputs.cuda()</span><br><span class="line">                <span class="keyword">if</span> len(inputs.shape) &lt; <span class="number">4</span>:</span><br><span class="line">                    inputs = inputs.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> len(inputs.shape) &lt; <span class="number">4</span>:</span><br><span class="line">                    inputs = torch.transpose(inputs, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                    inputs = inputs.unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p> 先设置不使用梯度，然后将测试时候的batchsize设置成训练时候的二分之一或者三分之一就不会爆了。<br> 可能原因是测试的时候真的需要更大的显存。</p>
</li>
<li><p>指定使用哪块GPU 0123</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="number">3</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>保存和加载模型 ,获得模型权重参数</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存和加载整个模型</span></span><br><span class="line">torch.save(model, <span class="string">'model.pkl'</span>)</span><br><span class="line">model = torch.load(<span class="string">'model.pkl'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅保存和加载模型参数(推荐使用)</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">'params.pkl'</span>)</span><br><span class="line"><span class="comment"># 注意是先在里面torch.load文件 在加载到model（网络实例）的权重字典里</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">'params.pkl'</span>))</span><br></pre></td></tr></table></figure>
<p> 得到模型权重： <code>model.state_dict</code></p>
</li>
<li><p>查看trainset打的标签<br>  <code>trainset.class_to_idx</code></p>
</li>
<li><p><code>nn.Linear层</code>就相当于一个<code>y = w·x + b</code></p>
</li>
<li><p>对于带有梯度的tensor（由于variable和tensor合并了， 在cuda且求了grad），要对其操作则要<br> tensor.cpu().detach() 再进行运算操作<br> <code>detach</code>可以去掉去梯度部分</p>
</li>
<li><p><strong>Pytorch 训练和测试时记得加 model.train 和 model.eval</strong> 设置模型模式<br> 如果用到了BN和dropout，用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval，eval（）时，框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大。<br> <strong>这两个方法是针对在网络train和eval时采用不同方式的情况，比如Batch Normalization和Dropout</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Class Inpaint_Network()</span><br><span class="line">......</span><br><span class="line">Model = Inpaint_Nerwoek()</span><br><span class="line"><span class="comment">#train:</span></span><br><span class="line">Model.train(mode=<span class="keyword">True</span>)</span><br><span class="line">.....</span><br><span class="line"><span class="comment">#test:</span></span><br><span class="line">Model.eval()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>glob模块</strong> (非常有用的文件读取操作模块）<br> glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件，类似于Windows下的文件搜索，支持通配符操作，_,?,[]这三个通配符，_代表0个或多个字符，?代表一个字符，[]匹配指定范围内的字符，如[0-9]匹配数字。两个主要方法如下。<br> <strong>①glob方法</strong>：<br> glob模块的主要方法就是glob,该方法返回所有匹配的文件路径<strong>列表</strong>（list）；该方法需要一个参数用来指定匹配的路径字符串（字符串可以为绝对路径也可以为相对路径），其返回的文件名只包括当前目录里的文件名，不包括子文件夹里的文件。<br> 比如：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">glob.glob(r’c:*.txt’)</span><br><span class="line"><span class="comment">#我这里就是获得C盘下的所有txt文件</span></span><br><span class="line">glob.glob(r’E:\pic**.jpg’)</span><br><span class="line"><span class="comment">#获得指定目录下的所有jpg文件</span></span><br><span class="line"><span class="comment">#使用相对路径：</span></span><br><span class="line">glob.glob(r’../*.py’)</span><br></pre></td></tr></table></figure>
<p> <strong>②iglob方法</strong></p>
<pre><code> 获取一个**迭代器**（ iterator ）对象，使用它可以逐个获取匹配的文件路径名。与glob.glob()的区别是：glob.glob同时获取所有的匹配路径，而 glob.iglob一次只获取一个匹配路径。
</code></pre><p> 这样就可以不一次性读完所有文件，节约内存<br> 下面是一个简单的例子：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = glob.iglob(<span class="string">r'../*.py'</span>) </span><br><span class="line"><span class="keyword">print</span> f </span><br><span class="line">&gt;&gt; &lt;generator object iglob at <span class="number">0x00B9FF80</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> py <span class="keyword">in</span> f:</span><br><span class="line">    <span class="keyword">print</span> py</span><br></pre></td></tr></table></figure>
</li>
<li><p>常用<strong>help</strong>看用法..torch里的函数用法繁多</p>
</li>
<li><p>向量运算的一个注意点：要先去掉梯度！<br> 涉及梯度计算的Tensor（以前是被封装成Variable，但后来合并了），需要用<code>.data</code>来取得其tensor<br> eg:<code>loss.data</code> <code>outputs.data</code><br> 损失函数返回值，模型输出，都是含有梯度的向量，需要用<code>.data</code>再参与运算！</p>
</li>
<li><p>注意<strong>单元素tensor</strong>！<br> Use tensor.item() to convert a 0-dim tensor to a Python number<br> 比如每个batch的loss返回值、sum(tensor1==tensor2)等都是单元素tensor<br> tensor和普通数字不能随意运算，不然会出错<br> <strong>要注意，tensor运算得到的结果也是tensor，就算是sum(tensor1==tensor2)得到相等元素数量，得到的也是tensor(x) （x为相等元素数量，一个常数），依然需要sum(temsor1==temsor2).item()才能参与普通数字运算</strong><br> 否则会出莫名其妙的错误！：比如</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.Tensor([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">t2 = torch.Tensor([<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">6</span>])</span><br><span class="line">sum(t1==t2)</span><br><span class="line">&gt;&gt; tensor(<span class="number">2</span>,dtype=torch.uint8)</span><br><span class="line">sum(t1==t2)/<span class="number">10</span></span><br><span class="line">&gt;&gt; tensor(<span class="number">0</span>,dtype=torch.uint8)</span><br><span class="line">torch.sum(t1==t2)</span><br><span class="line">&gt;&gt; tensor(<span class="number">2</span>)</span><br><span class="line">torch.sum(t1==t2)/<span class="number">10</span></span><br><span class="line">&gt;&gt; tensor(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这样才正确</span></span><br><span class="line">sum(t1==t2).item()/<span class="number">10</span></span><br><span class="line">&gt;&gt; <span class="number">0.2</span></span><br><span class="line">torch.sum(t1==t2).item()/<span class="number">10</span></span><br><span class="line">&gt;&gt; <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor除(/)一个常数是整除！</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>数据集<br> 最好使用三个数据集：训练集、验证集、测试集<br>  <code>sklearn.model_selection.train_test_split</code> 函数可用于方便的划分数据集</p>
</li>
<li><p>dropout 和 dropout2d的区别<br> torch.nn.Dropout对所有元素中<strong>每个元素</strong>按照概率0.5更改为零<br> 而torch.nn.Dropout2d是对<strong>每个通道</strong>按照概率0.5置为0（即一个通道全为0）</p>
</li>
<li><p>对于网络</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(nn.Module)</span></span></span><br><span class="line"><span class="class">...</span></span><br><span class="line"><span class="class"></span></span><br><span class="line">net = model()</span><br><span class="line"></span><br><span class="line">model.features</span><br><span class="line">net</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意<code>a = a.cuda()</code> 要赋值回去</p>
</li>
<li><p><strong>torch的交叉熵的输入第一个位置的输入应该是在每个label下的概率, 而不是对应的label</strong><br>否则会报错dimension out of range (expected to be in range of [-1, 0], but got 1)</p>
</li>
<li><p>返回对象的属性值<br><code>vars()</code><br>用法<code>vars(object)</code><br>可用于很多封装对象如dataloader等等</p>
</li>
<li><p>交叉熵损失函数中的weight参数 （权重）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight = torch.FloatTensor([<span class="number">0.13859937</span>, <span class="number">0.5821059</span>, <span class="number">0.63871904</span>, <span class="number">2.30220396</span>, <span class="number">7.1588294</span>, <span class="number">0</span>]).cuda()</span><br></pre></td></tr></table></figure>
</li>
<li><p>迁移学习修改网络应该直接修改层，修改线性层的out_features会遇到问题</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#应该直接修改最后一层网络</span></span><br><span class="line">vgg.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意初始化网络结构和传入参数到网络里进行计算的区别<br> 初始化网络实体的时候是传入<strong>init</strong>()里的参数，相当于初始化网络部件<br> 传入参数是传入forward()里的传输，这个传入的就需要计算了<br> 先初始化网络再传入参数进行计算</p>
</li>
<li><p><strong>输入损失函数的真实标签项，必须为long类型</strong> 不能是tensor float！tensor int！</p>
<p> 具体如此生成：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">tensor = torch.LongTensor(list)</span><br></pre></td></tr></table></figure>
</li>
<li><p>cuda runtime error: device-side assert triggered at xxx<br> 这个问题一般来自模型输出的label数量和标签类别种类数量不同。要洗一遍数据集的标签</p>
</li>
<li><p><strong>自定义层</strong></p>
<p> 可以自己定义层，只需要继承自<code>nn.Module</code>并实现<code>forward()</code>函数</p>
<p> <strong>问题：需要实现backwards函数吗？它会自动求导吗？</strong>：</p>
<p> pytorch可以自动求导，但如果实现的层<strong>不可导</strong>，就需要<strong>自己实现梯度的反向传递</strong>（<strong>比如存在if条件，孤立点，拐点</strong>，就需要自定义求导式）也就是所谓的 “Extending torch.autograd”. 官网虽然给了例子</p>
<p> 以下举例，自己建立了一个计算Gram Matrix 格拉姆矩阵的层（由于是可导的，所以不需要自己实现）</p>
</li>
<li><p>继承的时候别忘了<code>super().__init__()</code></p>
</li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
  </entry>
  <entry>
    <title>DigitalOcean成功迁移Vultr</title>
    <url>/2018/09/13/DigitalOcean%E6%88%90%E5%8A%9F%E8%BF%81%E7%A7%BBVultr/</url>
    <content><![CDATA[<p>新日本东京服务器，平均延迟122ms，联通延迟更低，甚至可以加速器用233</p>
<p>但是pixiv默认ban Vultr IP，这就很蛋疼了…..</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>优化器optimizer和超参数更新</title>
    <url>/2018/09/10/%E4%BC%98%E5%8C%96%E5%99%A8optimizer%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0/</url>
    <content><![CDATA[<h2 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a>optimizer</h2><p><code>optimizer.param_group</code><br>记录了模型迭代的优化器参数信息<br><code>optimizer.step</code>后可从<code>optimizer.param_groups[0][‘params’]</code>查看参数变化<br><code>optimizer.param_groups[0]</code>是一个字典<br>包含<code>dict_keys([&#39;params&#39;, &#39;lr&#39;, &#39;betas&#39;, &#39;eps&#39;, &#39;weight_decay&#39;, &#39;amsgrad&#39;])</code><br>所以可以自己看想要的信息，也可以直接修改！</p>
<p>模型权重参数是存在于<code>model.parameters()</code>中的，这个也是要传入optimizer的变量</p>
<h2 id="超参数更新"><a href="#超参数更新" class="headerlink" title="超参数更新"></a>超参数更新</h2><p>（<code>optimizer</code>和<code>scheduler</code>都是自己事先定义的变量，前者为优化器，后者为自适应学习率优化器<br><code>optimzer = torch.optim.Adam(……)</code><br><code>scheduler = torch.optim.lr_scheduler.StrpLR(……)</code>)</p>
<p><code>optimizer.step()</code>：通常用在<strong>每个mini-batch之中</strong>，用了之后<strong>模型才会更新</strong><br><code>scheduler.step()</code>：通常用在<strong>epoch里面</strong>，用了之后才会<strong>对学习率lr进行调整</strong><br><code>optimizer.zero_grad()</code> 用在<strong>每个mini-batch之中</strong>，梯度参数清0，以避免参数把上一次optimizer调用时创建的梯度累加在一起（根据pytorch中的backward()函数的计算，当网络参量进行反馈时，梯度是被积累的而不是被替换掉；但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积，因此这里就需要每个batch设置一遍zero_grad 了。）</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>神经网络</category>
      </categories>
  </entry>
  <entry>
    <title>永无止境的庙会里</title>
    <url>/2018/09/09/%E6%B0%B8%E6%97%A0%E6%AD%A2%E5%A2%83%E7%9A%84%E5%BA%99%E4%BC%9A%E9%87%8C/</url>
    <content><![CDATA[<p>永无止境的庙会，我能远离它吗？</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>No title</title>
    <url>/2018/08/25/523/</url>
    <content><![CDATA[<blockquote>
<p>Your time is limited, so don’t waste it living someone else’s life. Don’t be trapped by dogma — which is living with the results of other people’s thinking. Don’t let the noise of others’ opinions drown out your own inner voice. And most important, have the courage to follow your heart and intuition. They somehow already know what you truly want to become. Everything else is secondary.</p>
<p>By Steve Jobs</p>
</blockquote>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>旅途之后</title>
    <url>/2018/08/15/%E6%97%85%E9%80%94%E4%B9%8B%E5%90%8E/</url>
    <content><![CDATA[<p>从最后一节暑期课，到现在，快一个月了。我的旅途结束了。</p>
<p>走了很多地方，看见了不同的人与景，进入了不同的生活。</p>
<p>旅行也许是改变现在的尝试。回到了家。迷茫与困惑的感觉随之而归。一段旅行的结束，是思考的最好时机。</p>
 <a id="more"></a>
<p>仿佛又如四年前那样。</p>
<p>摆在面前的，是无数的不确定。我不知道，我想要做的，是否正确 ——没有答案，亦或是只有在未来才能知晓那个答案。甚至，我不知道那是不是我内心到底想要做的。</p>
<blockquote>
<p>现在想，高考是人生中最轻松的一次考试，它有答案有老师。</p>
<p>可后来生活里的每一道题，都没有答案。</p>
</blockquote>
<p>我只能用很长的时间，付出最大的辛苦，来给出一个我的答案。</p>
<p>筚路蓝缕，以启山林。</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>致过去与未来交界的现在</title>
    <url>/2018/07/01/%E8%87%B4%E8%BF%87%E5%8E%BB%E4%B8%8E%E6%9C%AA%E6%9D%A5%E4%BA%A4%E7%95%8C%E7%9A%84%E7%8E%B0%E5%9C%A8/</url>
    <content><![CDATA[<p>蓝渐白的天幕落下</p>
 <a id="more"></a>
<p>闪烁着一颗星星<br>天空纯粹无暇<br>矮矮的楼房立于两边<br>在天幕下，只显得两道黑影</p>
<p>仰望无暇的星空<br>静静伫立<br>沐浴在着清澈湛蓝的夜空下</p>
<p>脑海里闪过了过去与现在的境像<br>那些情感——激动，难过，感动，兴奋，生气，期待…在心中闪烁，点燃了思绪</p>
<p>不一会儿，在夜空的沐浴下，思绪也归于这夜空的宁静。</p>
<p>致过去与未来交界的现在。</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>鱼玄机</title>
    <url>/2018/06/25/%E9%B1%BC%E7%8E%84%E6%9C%BA/</url>
    <content><![CDATA[<p>我憨的这首鱼玄机唱的太好听了 av8444592</p>
]]></content>
      <categories>
        <category>Music</category>
      </categories>
  </entry>
  <entry>
    <title>十字路口</title>
    <url>/2018/06/23/%E5%8D%81%E5%AD%97%E8%B7%AF%E5%8F%A3/</url>
    <content><![CDATA[<p>下一站，</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>AS安装实录</title>
    <url>/2018/06/01/AS%E5%AE%89%E8%A3%85%E5%AE%9E%E5%BD%95/</url>
    <content><![CDATA[<p>你墙国还是你墙国</p>
<p>XCode10分钟搞定的事情AS能折腾一年</p>
<p>1/安装SDK问题。<br>离线安装SDK，并修改SDK路径</p>
 <a id="more"></a>
<p>2/无限卡下载SDK<br>在/home/aisaka/android-studio/bin/idea.properties中加入一行<br>disable.android.first.run=true</p>
<p>3/安装gradel问题<br>手动离线安装gradel，讲gradel压缩包放入wrapper里的乱码文件夹里，放一个压缩包和一个文件夹<br>然后在setting里手动设置gradel路径到4.4-all</p>
<p>4/gradel无法同步问题<br><a href="https://www.e-learn.cn/content/wangluowenzhang/28422" target="_blank" rel="noopener">https://www.e-learn.cn/content/wangluowenzhang/28422</a></p>
<p>5/同步gradel速度极慢问题<br>改hosts(ipv6 hosts)<br>关掉gradel代理</p>
<p>6/运行模拟器时无法加载驱动问题<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">failed to load driver: nouveau</span><br><span class="line">下午3:55 Emulator: libGL error: unable to load driver: nouveau_dri.so</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: libGL error: driver pointer missing</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: libGL error: failed to load driver: nouveau</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: libGL error: unable to load driver: swrast_dri.so</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: libGL error: failed to load driver: swrast</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: emulator: ERROR: Missing initial data partition file: /home/aisaka/.android/avd/Nexus_5X_API_27.avd/userdata.img</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: X Error of failed request: BadValue (integer parameter out of range for operation)</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: Major opcode of failed request: 155 (GLX)</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: Minor opcode of failed request: 24 (X_GLXCreateNewContext)</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: Value in failed request: 0x0</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: Serial number of failed request: 58</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: Current serial number in output stream: 59</span><br><span class="line"></span><br><span class="line">下午3:55 Emulator: Process finished with exit code 1</span><br><span class="line"></span><br><span class="line">下午3:55 Gradle build finished in 1s 316ms</span><br></pre></td></tr></table></figure></p>
<p>问题解决：<a href="https://kotlintc.com/articles/4062" target="_blank" rel="noopener">https://kotlintc.com/articles/4062</a><br>执行如下命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ cd ~/Android/Sdk/emulator/lib64/libstdc++</span><br><span class="line"></span><br><span class="line">$ mv libstdc++.so.6 libstdc++.so.6.bak</span><br><span class="line"></span><br><span class="line">$ ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 libstdc++.so.6</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>开发</category>
        <category>andoird开发</category>
      </categories>
  </entry>
  <entry>
    <title>分治策略</title>
    <url>/2018/05/27/%E5%88%86%E6%B2%BB%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<p>对分治策略的分析见文章[递归分析]<a href="http://aisakaki.com/?p=406">http://aisakaki.com/?p=406</a></p>
<p>1、分解（分）</p>
<p>2、解决（治）</p>
<p>3、合并</p>
<p>一些使用分治策略的算法</p>
<p>①归并排序</p>
<p>T(n)=2T(n/2)+Θ(n)</p>
<p>2为子问题数目，n/2为子问题规模，Θ(n)为附加计算量（分治法运行时间）</p>
<p>通过主定理case2，可以得到其T(n)=O(nlgn)</p>
  <a id="more"></a>
<p>②二分查找</p>
<p>分：二分</p>
<p>治：比较。只在一个子数组中递归</p>
<p>T(n)=1*T(n/2)+Θ(1)</p>
<p>1个子问题</p>
<p>③乘方问题</p>
<p>求x^n</p>
<p>如果用朴素方法，那么T(n)=O(n)</p>
<p>如果采用分治策略，那么T(n)=T(n/2)+Θ(1)=O(lgn) (master method) &lt;O(n)</p>
<p>*注意奇偶n的处理，但其递归式不变</p>
<p>只有一个子问题，画递归树的话就是一条链</p>
<p>④非不拉几数列</p>
<p>Fn=Fn-1 + Fn-2 if n&gt;=2</p>
<p>Fn=0 if n=0</p>
<p>Fn=1 if n=1</p>
<p>1*若采用朴素的递归方式，不停计算Fn-1,Fn-2,触底就返回，那么</p>
<p>T(n)=Ω(φ^n)  φ=(1+√5)/2 （φ为黄金比例，神奇）</p>
<p><strong>子问题的规模仅仅缩小到了n-1，递归树非常庞大</strong></p>
<p>复杂度为指数级，代价很高，所以此种方法不好。</p>
<p><strong>最理想的算法复杂度为多项式级</strong></p>
<p>2*事实上，在建立非不拉几递归树的时候，会发现很多公共子树，这些子树重复计算，产生了大量冗余。</p>
<p>如果<strong>考虑从底向上计算</strong>（线性。依次计算F(1),F(2),F(3),….)，那么其复杂度会是O(n)</p>
<p>3*有一个更简单的办法（数学），F(n)=φ^n/√5并取整至最接近的整数，复杂度即为③中的问题，为O(lgn)</p>
<p>由于φ为浮点数，所以这样不精确且对运算要求极高，在现有机器上无法运行</p>
<p>4*第二种数学方法</p>
<p>Fn-1 Fn        =       1      1   ^n</p>
<p>Fn    Fn-2    =        1       0</p>
<p>（左右皆为矩阵，右边是矩阵的n次幂，待安装数学公式插件再修改）</p>
<p>证明：数学归纳法。比较简单，证略。</p>
<p>其复杂度为O(lgn)</p>
<p>⑤矩阵乘法</p>
<p>朴素方法：三层循环，i对应左行，j对应右列，k对应遍历每个元素。T(n)=O(n^3)</p>
<p>分治方法1：</p>
<p>利用分块矩阵，将母矩阵变成2*2的矩阵</p>
<p>A= A11 A12</p>
<p>A21 A22</p>
<p>B=B11 B12</p>
<p>B21 B22</p>
<p>C=C11 C12</p>
<p>C21 C22</p>
<p>由此可以写得四个表达式</p>
<p>c11=A11<em>B11 + A12</em>B21</p>
<p>C12=A11<em>B12+A12</em>B22</p>
<p>C21=A21<em>B11+A22</em>B22</p>
<p>C22=A21<em>B12+A22</em>B22</p>
<p>由此可得每次递归需要将有八个递归子问题（乘），并加上4次矩阵加法时间</p>
<p>（8个规模为n/2的矩阵乘法（递归）和4个规模为n/2（即合并起来为一个规模为n）的矩阵加法（非递归，常数）</p>
<p>所以其递归表达式为 T(n)=8T(n/2)+Θ(n^2)</p>
<p>由主定理得，T(n)=Θ(n^3)</p>
<p>此方法效率依然很低，和朴素方法没什么区别。</p>
<p>思考：为什么效率低？</p>
<p>因为矩阵乘法是Θ(n^3)级复杂度，而一次递归将产生8个子问题，每个子问题皆矩阵乘法</p>
<p>而矩阵加法是Θ(n^2)级复杂度。<strong>所以为了使递归树变小，我们应尽量减少矩阵乘法，多做矩阵加法。</strong></p>
<p>比如，一次递归产生8个子矩阵乘法问题，递归树太“茂盛”了，我们能否减小到7个？</p>
<p>通过这种思考下去，就想到了第二种方法</p>
<p>分治方法2：Strassen方法</p>
<p>该算法具体内容见<a href="https://blog.csdn.net/qwertyuer/article/details/44255087" target="_blank" rel="noopener">https://blog.csdn.net/qwertyuer/article/details/44255087</a></p>
<p>该算法下，T(n)=7T(n/2)+Θ(n^2)=Θ(n^lg7)&lt;Θ(n^3)</p>
<p>分：将AB 矩阵分为8个子矩阵</p>
<p>治：中间计算过程</p>
<p>合：将子问题的解合并为原问题的解</p>
<p>未完待续</p>
]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>C#</title>
    <url>/2018/05/25/c/</url>
    <content><![CDATA[<p>闲暇时间做游戏，在学习Unity</p>
<p>由于U3D需要用C#，所以迅速对比java c了解一下C#的语法特性方便记忆。</p>
<p>——————————————————————————</p>
<p>1、析构函数</p>
<p><code>~object() {}</code></p>
<p>2、重写方法</p>
<p><code>override void method() {}</code></p>
 <a id="more"></a>
<p>3、继承(类与接口）</p>
<p><code>public class son : father {}</code></p>
<p>4、无指针，用引用，同java</p>
<p>5、使用<strong>ref</strong>关键字进行引用传递（必须初始化）</p>
<p>ref关键字放在需要传递的变量面前，把一个输出参数的<strong>内存地址</strong>传递给方法（即传递实参）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">public void Grow (int _nSpan, **ref** outCurrentAge)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">this.nAge+=_nSpan;</span><br><span class="line"></span><br><span class="line">nOutCurrentAge=this.nAge;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主程序中<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">int nCurrentAge=0;</span><br><span class="line"></span><br><span class="line">s.Grow(3 ,**ref** nCurrentAge)</span><br><span class="line"></span><br><span class="line">Console.WriteLine(nCurrentAge);</span><br></pre></td></tr></table></figure></p>
<p>6、使用<strong>out</strong>关键字进行引用传递（无需初始化）</p>
<p>7、使用<strong>params</strong>关键字传递多个参数</p>
<p>params关键字指明一个输出参数被看作为一个参数数组，这种类型的输出参数只能作为方法的最后一个输入参数</p>
<p>主要用于，在调用一个方法时，预先不能确定参数的数量、数据类型等。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public void setHobby(**params** string[] _setArrHobby)</span><br><span class="line"></span><br><span class="line">&#123; &#125;</span><br><span class="line"></span><br><span class="line">主程序：s.setHobby(“reading”,”singing”,”programing”);</span><br></pre></td></tr></table></figure>
<p>8、比较字符串</p>
<p><code>int Compare(strA,strB);</code> 输出为0则相等</p>
<p>亦可用<code>lool Equals</code>，<code>int CompareTo</code></p>
<p>不过，Compare是静态方法，且可重载。</p>
<p><code>CompareTo</code>不是静态方法，且没有重载形式</p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>C#</category>
      </categories>
  </entry>
  <entry>
    <title>递归分析</title>
    <url>/2018/05/23/%E9%80%92%E5%BD%92%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>1、第一种，代入法（数学归纳）</p>
<p>这里需要用到数学归纳法</p>
<p>对于递归式</p>
<p>T(n) = 4T(n/2) + n</p>
<p>函数增长：</p>
<p>当n乘2的时候，T将乘以4</p>
<p>先考虑求其渐进上界</p>
<p>猜测，为theta(n^3)的时候，满足情况</p>
<p>对k&lt;n</p>
<p>T(k)&lt;=ck^3 (T(k) ∈ O(n^3))</p>
<p>设置一个常量g(n)来替代ck^3 （  g(n) ∈ O(n^3))  ）</p>
<p>用数学归纳法证明：</p>
 <a id="more"></a>
<p>1*  T(1)=O(1)</p>
<p>T(1)=c =O(1)</p>
<p>2* 假设T(n/2) = O((n/2)^3) +n</p>
<p>3<em> T(n) = 1/2</em>g(n) +n</p>
<p>=  g(n) + (n – 1/2g(n))</p>
<p>后部为余项</p>
<p>n -1/2g(n) = n – cn^3 <0 (n="">1)</0></p>
<p>∴ T(n) &lt;= g(n)</p>
<p>= O(n^3) （非对称相等）</p>
<p>猜测，为theta(n^2)的时候，满足情况</p>
<p>同理 设置 g(n) = O(n^2)</p>
<p>用数学归纳法证明：</p>
<p>1*  T(1)=O(1)</p>
<p>2* 假设T(n/2) = O((n/2)^2) + n</p>
<p>3<em> T(n) = 4</em>T(n/2) +n</p>
<p>=  g(n) +n</p>
<p>= g(n) – (-n)</p>
<p>(-n)为余项。此时无法证明-n为非负数</p>
<p>所以需要在原来的假设上进行改进</p>
<p>对T(k)进行更低阶的展开 以凑得一个非负的余项（想法：扩展系数到余项）</p>
<p>【注意这里使用了更强的归纳法，所以需要证明的结论更强了。</p>
<p>在归纳的时候需要归纳到的式子的主部已经变了。这里容易忽略】</p>
<p>对k&lt;n</p>
<p><strong>假设 T(k)&lt;=c1*k^2 – c2*k</strong>    (T(k) ∈ O(n^2))</p>
<p>即令g(n)=c1<em>n^2 – c2</em>n</p>
<p>T(n) = 4*T(n/2)+n</p>
<p>=4(c1<em>(n/2)^2 – c2</em>(n/2) +n)</p>
<p>=c1<em>n^2 – (2c2 -4)</em>n</p>
<p>=c1<em>n^2 – c2</em>n – (c2-1)*n n&gt;0)</p>
<p><strong>余项为(c2-1)*n</strong> </p>
<p>∴当c2-1&gt;0即c2&gt;1时</p>
<p>T(n)&lt;=g(n)</p>
<p>=O(n^2)</p>
<p>对基本情况</p>
<p>为使T(1) = c1- c2 =O(1)成立</p>
<p>即 对于∀c2&gt;1,Ec1&gt;c2</p>
<p>∴c1需要足够大</p>
<p>综上所述，当c1足够大,c2&gt;1,T(n)=O(n^2)</p>
<p>证毕</p>
<p>对于渐进下界Omega同理可证</p>
<p>所以有个很关键的地方，就是要进行合理的猜测。</p>
<p>可以通过经验猜测，也可以通过第二种方法递归树法来猜测</p>
<p>2、递归树法</p>
<p>这个方法不太严谨，但很常用（一般要通过代入法来证明）</p>
<p>ATT：</p>
<p>注意并理解递归树的画法（和式画法），理解递归式每一项的意义</p>
<p>理解<strong>非递归代价，递归代价，总代价。</strong></p>
<p>对于式T(n)=aT(n/b)+f(n)中，f(n)即为非递归代价，n为当层规模，比如第3层所有结点的非递归代价和即为a^2 * f(n/b^3) ，单个结点的非递归代价为 f(n/b^3)。如果算上递归代价，就还需要计算该结点下递归子树的总代价。</p>
<p><strong>f(n)不再递归，反应了该节点的非递归部分的代价。</strong></p>
<p><strong>aT(n/b)反应了函数增长，我们可以在其中看出横向增长(子问题数目）和递归规模（子问题规模）。</strong></p>
<p>注意层数=高度+1 完全N叉树的叶节点=N^高度</p>
<p>网站上不方便画图emmm</p>
<p>所以，图略</p>
<p>分析递归树：</p>
<p>对于T(n)=cn^2+3T(n/4)</p>
<p>纵向分析：</p>
<p>子问题的规模为上一步的1/4</p>
<p>最终问题规模会到达1 （代价:T(1)）</p>
<p>深度为i的结点对应规模为 n/(4^i) 的子问题</p>
<p>所以当触底，子问题规模为1时 ，即 n/(4^i)=1， 递归树有log4(n)+1层（深度为0,1,2…log4(n)）</p>
<p>横向分析：</p>
<p>对于第i层，结点数为3^i</p>
<p>所以对于第i层的代价为 3^i * (n/(4^i))^2</p>
<p>所以对于最后一层，即i=log4(n)，代入即可得 最后一层的代价为 n^log4(3) * T(1)</p>
<p>T(1)为常数,n为常数</p>
<p>所以最后一层的代价为theta(n^log4(3))</p>
<p>经过以上分析，我们可以知道每一层的代价为一个等比级数，用等比级数求和公式即可求出整颗树的总代价。</p>
<p>由于有时候在表示渐进上界的时候并不需要如此严格(精确度不需要这么高），所以可以放缩为比较好处理的级数</p>
<p>3、主方法</p>
<p>主定理(master method)适用于一种形式的递归式</p>
<p>T(n)=aT(n/b)+f(n)</p>
<p>（公式比较复杂，等安装了插件之后再写数学表达式，网页不方便表示）</p>
<p>定理的证明：通过三个引理来证明</p>
<p>1*使用递归树求得代价公式（为一个等比级数和与叶结点代价和之和），</p>
<p>2*考虑f(n)的三种情况，在等比级数上进行变形，推导出简化渐进上界</p>
<p>3*将变形后的等比级数和所求得的简化渐进上界与原来的叶节点代价和相加，得到最终的渐进上界</p>
<p>（具体推导过程略，网站暂时上不方便写太多公式）</p>
<p>主定理对于快速求解非常重要，需要牢记结论</p>
]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>JAVA的两个问题</title>
    <url>/2018/05/15/JAVA%E7%9A%84%E4%B8%A4%E4%B8%AA%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>今天在写工厂的时候，对于这样一个语句，遇到了两个问题</p>
<p>mom obj = new son();</p>
<p>以下为源代码</p>
<p><img src="http://aisakaki.com/wp-content/uploads/2018/05/0FEEB447832EE00327DC103C3A4A6B57-278x300.png" alt="0FEEB447832EE00327DC103C3A4A6B57"></p>
<p>输出的结果为</p>
<p>0</p>
<p>Type son</p>
<p>一、</p>
<p>这就很奇怪了。按照已知知识，mom obj = new son();这句话是在堆上新建一个son类型的对象，然后向上转型为mom类型。此时的obj，既是属于son类型，也属于其基类mom类型。这是java多态性的一个具体体现。对象实体是son类型的，那么我在调用son中的方法test的时候，和输出变量a的时候，理想输出应为</p>
 <a id="more"></a>
<p>1</p>
<p>Type son</p>
<p>然而结果却不是。方法调用的是子类的方法，然而成员变量却调用的是基类的变量值。</p>
<p>我又运行了如下代码，更奇怪的事情发生了。</p>
<p>我在son类中加入了一个新方法test2()。在主函数中调用obj中的test2方法。</p>
<p><img src="http://aisakaki.com/wp-content/uploads/2018/05/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2018-05-15-%E4%B8%8B%E5%8D%888.49.52-300x253.png" alt="屏幕快照 2018-05-15 下午8.49.52"></p>
<p>更奇怪的事情发生了。</p>
<p><img src="http://aisakaki.com/wp-content/uploads/2018/05/29ECB79D17CD24DFC8F94CD0615777E2-300x91.png" alt="29ECB79D17CD24DFC8F94CD0615777E2"></p>
<p>为什么编译器不认这个儿子了？</p>
<p>经过多次测试，和查阅资料，我终于知道了原因。</p>
<p><strong>首先要知道，java中除了static方法和final方法（private方法属于final方法）之外，其他所有的方法否是后期绑定的（动态绑定）。</strong>(final可以关闭动态绑定，但实际上，这对性能提升并不大，所以不要试图用final来提升性能，而是根据设计决定是否使用final）</p>
<p><strong>向上转型，在运行时，会遗忘子类对象中与父类对象中不同的方法。也会覆盖与父类中相同的方法–重写。</strong><br>所以obj可以调用的是mom中有的方法，而son中有，mom中却没有的方法，是不能调用的。<br>java 的这种机制遵循一个原则：当超类对象引用变量引用子类对象时,被引用对象的类型而不是引用变量的类型决定了调用谁的成员方法， 但是这个被调用的方法必须是在超类中定义过的，也就是说被子类覆盖的方法。</p>
<p>于是我们可以知道，向上转型机制，实际上遗失了子类的成员变量和基类没有的方法。</p>
<p>这就解释了为什么是这样的结果。</p>
<p>二、</p>
<p>在探究这个语句的执行过程的时候，就又引申出了另一个问题。构造器的继承问题。</p>
<p>对于mom obj = new son();</p>
<p>按照已知知识，mom obj是建立了一个mom类型的引用obj，相当于c中的一个指针。此时由于没有对象实体，其值为null（空指针）。但是当我在执行以下代码的时候，却发生了很奇怪的事情。</p>
<p>对于以下语句</p>
<p><img src="http://aisakaki.com/wp-content/uploads/2018/05/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2018-05-15-%E4%B8%8B%E5%8D%888.38.42-300x264.png" alt="屏幕快照 2018-05-15 下午8.38.42"></p>
<p>输出结果为</p>
<p>mom</p>
<p>son</p>
<p>奇怪了，难道是mom obj这个语句的时候就已经进行了对象的创建和初始化？</p>
<p>我试了一下，如果只是</p>
<p><img src="http://aisakaki.com/wp-content/uploads/2018/05/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2018-05-15-%E4%B8%8B%E5%8D%888.40.24-300x31.png" alt="屏幕快照 2018-05-15 下午8.40.24"></p>
<p>那么结果为</p>
<p><img src="http://aisakaki.com/wp-content/uploads/2018/05/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2018-05-15-%E4%B8%8B%E5%8D%888.40.33-300x59.png" alt="屏幕快照 2018-05-15 下午8.40.33"></p>
<p>说明了mom obj语句并没有创建和初始化对象，只是声明了一个mom类型的引用。</p>
<p>于是我查阅书籍，在《thinking in java》看到了如下几段</p>
<p><strong>“类的代码在初次使用的时候才加载。这通常是指加载发生于创建类的第一个对象之时，但是当访问static域或static方法的时候，也会发生加载（构造器也是static方法，尽管static关键字并没有显示地写出来。因此更准确地讲，类是在其任何static成员被访问的时候加载的。”</strong></p>
<p>所以说实际上，</p>
<p>在加载类的时候，会依次访问加载基类（或叫父类，超类），其中在遇到static对象和代码段的时候，会依程序中的顺序初始化。（而构造器也是static方法，只是没有写出来而已。</p>
<p>所以举例 对继承关系 A-&gt;B-&gt;C-&gt;D</p>
<p>在生成D类对象的时候，会执行构造器且构造器执行顺序为A-&gt;B-&gt;C-&gt;D。</p>
<p>这就解释了为什么是这个结果。</p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title>图书馆</title>
    <url>/2018/05/12/%E5%9B%BE%E4%B9%A6%E9%A6%86/</url>
    <content><![CDATA[<p>一个知道自己有多渺小的地方…</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>多级中文域名解析系统-3</title>
    <url>/2018/04/05/%E5%A4%9A%E7%BA%A7%E4%B8%AD%E6%96%87%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%B3%BB%E7%BB%9F-3%EF%BC%88c%E8%AF%AD%E8%A8%80%EF%BC%89/</url>
    <content><![CDATA[<h2 id="ROOT-DNS"><a href="#ROOT-DNS" class="headerlink" title="ROOT_DNS"></a>ROOT_DNS</h2><p><em>根服务器的逻辑代码</em></p>
 <a id="more"></a>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"defAndTools.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> RRFILE <span class="meta-string">"RRroot.txt"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">char</span> sendbuf[<span class="number">512</span>];</span><br><span class="line">	<span class="keyword">char</span> recvbuf[<span class="number">512</span>]; </span><br><span class="line">	<span class="keyword">int</span> sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">int</span> sockfd=socket(AF_INET,SOCK_DGRAM,<span class="number">0</span>);</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">	addr.sin_family =AF_INET;</span><br><span class="line">	addr.sin_port =htons(SERVER_PORT);</span><br><span class="line">	addr.sin_addr.s_addr=inet_addr(ROOT_SERVER_IP);   </span><br><span class="line">	</span><br><span class="line">	bind(sockfd,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">cli</span>;</span></span><br><span class="line">	<span class="keyword">socklen_t</span> len=<span class="keyword">sizeof</span>(cli);</span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)   </span><br><span class="line">	&#123;</span><br><span class="line">		recvfrom(sockfd,recvbuf,<span class="keyword">sizeof</span>(recvbuf),<span class="number">0</span>,(struct sockaddr*)&amp;cli,&amp;len);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n收到本地服务器请求：\n"</span>);</span><br><span class="line">		<span class="comment">//读取，构造包，得到需要找的域名和类型</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">recv_header</span>;</span></span><br><span class="line">		recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		decode_header(recv_header,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_header(recv_header);	</span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span> *<span class="title">query_section</span>;</span></span><br><span class="line">		query_section = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">		decode_query_section(query_section,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_query_section(query_section);      </span><br><span class="line">		</span><br><span class="line">		<span class="comment">//开始查找与写缓冲</span></span><br><span class="line">		<span class="comment">//调用初次搜索函数</span></span><br><span class="line">		<span class="keyword">int</span> over = firstFindRR(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//第一次搜索没有查到结果,开始查询下一个该去哪个服务器</span></span><br><span class="line">		<span class="keyword">if</span>(over==<span class="number">0</span>) </span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n本服务器没有找到\n"</span>);</span><br><span class="line">			loopFindNS(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//发送</span></span><br><span class="line">		sendto(sockfd,sendbuf,sendBufferPointer,<span class="number">0</span>,(struct sockaddr*)&amp;cli,len);  </span><br><span class="line">		<span class="comment">//缓冲区重置</span></span><br><span class="line">		sendBufferPointer=<span class="number">0</span>;    </span><br><span class="line">		recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">		<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">		<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	close(sockfd);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="DNS1"><a href="#DNS1" class="headerlink" title="DNS1"></a>DNS1</h2><p><em>第一级域名解析服务器主逻辑，以下四个中间级服务器逻辑都差不多</em></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"defAndTools.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_IP <span class="meta-string">"127.0.0.4"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> RRFILE <span class="meta-string">"RRL1.txt"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">char</span> sendbuf[<span class="number">512</span>];</span><br><span class="line">	<span class="keyword">char</span> recvbuf[<span class="number">512</span>]; </span><br><span class="line">	<span class="keyword">int</span> sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">int</span> sockfd=socket(AF_INET,SOCK_DGRAM,<span class="number">0</span>);</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">	addr.sin_family =AF_INET;</span><br><span class="line">	addr.sin_port =htons(SERVER_PORT);</span><br><span class="line">	addr.sin_addr.s_addr=inet_addr(SERVER_IP);   </span><br><span class="line">	</span><br><span class="line">	bind(sockfd,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">cli</span>;</span></span><br><span class="line">	<span class="keyword">socklen_t</span> len=<span class="keyword">sizeof</span>(cli);</span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)   </span><br><span class="line">	&#123;</span><br><span class="line">		recvfrom(sockfd,recvbuf,<span class="keyword">sizeof</span>(recvbuf),<span class="number">0</span>,(struct sockaddr*)&amp;cli,&amp;len);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n收到本地服务器请求：\n"</span>);</span><br><span class="line">		<span class="comment">//读取，构造包，得到需要找的域名和类型</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">recv_header</span>;</span></span><br><span class="line">		recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		decode_header(recv_header,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_header(recv_header);	</span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span> *<span class="title">query_section</span>;</span></span><br><span class="line">		query_section = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">		decode_query_section(query_section,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_query_section(query_section);      </span><br><span class="line">		</span><br><span class="line">		<span class="comment">//开始查找与写缓冲</span></span><br><span class="line">		<span class="comment">//调用初次搜索函数</span></span><br><span class="line">		<span class="keyword">int</span> over = firstFindRR(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//第一次搜索没有查到结果,开始查询下一个该去哪个服务器</span></span><br><span class="line">		<span class="keyword">if</span>(over==<span class="number">0</span>) </span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n本服务器没有找到\n"</span>);</span><br><span class="line">			loopFindNS(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//发送</span></span><br><span class="line">		sendto(sockfd,sendbuf,sendBufferPointer,<span class="number">0</span>,(struct sockaddr*)&amp;cli,len);  </span><br><span class="line">		<span class="comment">//缓冲区重置</span></span><br><span class="line">		sendBufferPointer=<span class="number">0</span>;    </span><br><span class="line">		recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">		<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">		<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	close(sockfd);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="DNS2"><a href="#DNS2" class="headerlink" title="DNS2"></a>DNS2</h2><p><em>第二级域名解析服务器主逻辑</em></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"defAndTools.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_IP <span class="meta-string">"127.0.0.5"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> RRFILE <span class="meta-string">"RRL2.txt"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">char</span> sendbuf[<span class="number">512</span>];</span><br><span class="line">	<span class="keyword">char</span> recvbuf[<span class="number">512</span>]; </span><br><span class="line">	<span class="keyword">int</span> sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">int</span> sockfd=socket(AF_INET,SOCK_DGRAM,<span class="number">0</span>);</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">	addr.sin_family =AF_INET;</span><br><span class="line">	addr.sin_port =htons(SERVER_PORT);</span><br><span class="line">	addr.sin_addr.s_addr=inet_addr(SERVER_IP);   </span><br><span class="line">	</span><br><span class="line">	bind(sockfd,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">cli</span>;</span></span><br><span class="line">	<span class="keyword">socklen_t</span> len=<span class="keyword">sizeof</span>(cli);</span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)   </span><br><span class="line">	&#123;</span><br><span class="line">		recvfrom(sockfd,recvbuf,<span class="keyword">sizeof</span>(recvbuf),<span class="number">0</span>,(struct sockaddr*)&amp;cli,&amp;len);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n收到本地服务器请求：\n"</span>);</span><br><span class="line">		<span class="comment">//读取，构造包，得到需要找的域名和类型</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">recv_header</span>;</span></span><br><span class="line">		recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		decode_header(recv_header,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_header(recv_header);	</span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span> *<span class="title">query_section</span>;</span></span><br><span class="line">		query_section = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">		decode_query_section(query_section,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_query_section(query_section);      </span><br><span class="line">		</span><br><span class="line">		<span class="comment">//开始查找与写缓冲</span></span><br><span class="line">		<span class="comment">//调用初次搜索函数</span></span><br><span class="line">		<span class="keyword">int</span> over = firstFindRR(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//第一次搜索没有查到结果,开始查询下一个该去哪个服务器</span></span><br><span class="line">		<span class="keyword">if</span>(over==<span class="number">0</span>) </span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n本服务器没有找到\n"</span>);</span><br><span class="line">			loopFindNS(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//发送</span></span><br><span class="line">		sendto(sockfd,sendbuf,sendBufferPointer,<span class="number">0</span>,(struct sockaddr*)&amp;cli,len);  </span><br><span class="line">		<span class="comment">//缓冲区重置</span></span><br><span class="line">		sendBufferPointer=<span class="number">0</span>;    </span><br><span class="line">		recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">		<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">		<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	close(sockfd);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="DNS3"><a href="#DNS3" class="headerlink" title="DNS3"></a>DNS3</h2><p><em>第三级域名解析服务器主逻辑</em></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"defAndTools.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_IP <span class="meta-string">"127.0.0.6"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> RRFILE <span class="meta-string">"RRL3.txt"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">char</span> sendbuf[<span class="number">512</span>];</span><br><span class="line">	<span class="keyword">char</span> recvbuf[<span class="number">512</span>]; </span><br><span class="line">	<span class="keyword">int</span> sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">int</span> sockfd=socket(AF_INET,SOCK_DGRAM,<span class="number">0</span>);</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">	addr.sin_family =AF_INET;</span><br><span class="line">	addr.sin_port =htons(SERVER_PORT);</span><br><span class="line">	addr.sin_addr.s_addr=inet_addr(SERVER_IP);   </span><br><span class="line">	</span><br><span class="line">	bind(sockfd,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">cli</span>;</span></span><br><span class="line">	<span class="keyword">socklen_t</span> len=<span class="keyword">sizeof</span>(cli);</span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)   </span><br><span class="line">	&#123;</span><br><span class="line">		recvfrom(sockfd,recvbuf,<span class="keyword">sizeof</span>(recvbuf),<span class="number">0</span>,(struct sockaddr*)&amp;cli,&amp;len);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n收到本地服务器请求：\n"</span>);</span><br><span class="line">		<span class="comment">//读取，构造包，得到需要找的域名和类型</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">recv_header</span>;</span></span><br><span class="line">		recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		decode_header(recv_header,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_header(recv_header);	</span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span> *<span class="title">query_section</span>;</span></span><br><span class="line">		query_section = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">		decode_query_section(query_section,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_query_section(query_section);      </span><br><span class="line">		</span><br><span class="line">		<span class="comment">//开始查找与写缓冲</span></span><br><span class="line">		<span class="comment">//调用初次搜索函数</span></span><br><span class="line">		<span class="keyword">int</span> over = firstFindRR(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//第一次搜索没有查到结果,开始查询下一个该去哪个服务器</span></span><br><span class="line">		<span class="keyword">if</span>(over==<span class="number">0</span>) </span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n本服务器没有找到\n"</span>);</span><br><span class="line">			loopFindNS(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//发送</span></span><br><span class="line">		sendto(sockfd,sendbuf,sendBufferPointer,<span class="number">0</span>,(struct sockaddr*)&amp;cli,len);  </span><br><span class="line">		<span class="comment">//缓冲区重置</span></span><br><span class="line">		sendBufferPointer=<span class="number">0</span>;    </span><br><span class="line">		recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">		<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">		<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	close(sockfd);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="DNS4"><a href="#DNS4" class="headerlink" title="DNS4"></a>DNS4</h2><p><em>第四级域名解析服务器主逻辑</em></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"defAndTools.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_IP <span class="meta-string">"127.0.0.7"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> RRFILE <span class="meta-string">"RRL4.txt"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">char</span> sendbuf[<span class="number">512</span>];</span><br><span class="line">	<span class="keyword">char</span> recvbuf[<span class="number">512</span>]; </span><br><span class="line">	<span class="keyword">int</span> sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">int</span> sockfd=socket(AF_INET,SOCK_DGRAM,<span class="number">0</span>);</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">	addr.sin_family =AF_INET;</span><br><span class="line">	addr.sin_port =htons(SERVER_PORT);</span><br><span class="line">	addr.sin_addr.s_addr=inet_addr(SERVER_IP);   </span><br><span class="line">	</span><br><span class="line">	bind(sockfd,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">cli</span>;</span></span><br><span class="line">	<span class="keyword">socklen_t</span> len=<span class="keyword">sizeof</span>(cli);</span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)   </span><br><span class="line">	&#123;</span><br><span class="line">		recvfrom(sockfd,recvbuf,<span class="keyword">sizeof</span>(recvbuf),<span class="number">0</span>,(struct sockaddr*)&amp;cli,&amp;len);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n收到本地服务器请求：\n"</span>);</span><br><span class="line">		<span class="comment">//读取，构造包，得到需要找的域名和类型</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">recv_header</span>;</span></span><br><span class="line">		recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		decode_header(recv_header,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_header(recv_header);	</span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span> *<span class="title">query_section</span>;</span></span><br><span class="line">		query_section = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">		decode_query_section(query_section,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		print_query_section(query_section);      </span><br><span class="line">		</span><br><span class="line">		<span class="comment">//开始查找与写缓冲</span></span><br><span class="line">		<span class="comment">//调用初次搜索函数</span></span><br><span class="line">		<span class="keyword">int</span> over = firstFindRR(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//第一次搜索没有查到结果,开始查询下一个该去哪个服务器</span></span><br><span class="line">		<span class="keyword">if</span>(over==<span class="number">0</span>) </span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n本服务器没有找到\n"</span>);</span><br><span class="line">			loopFindNS(query_section,RRFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//发送</span></span><br><span class="line">		sendto(sockfd,sendbuf,sendBufferPointer,<span class="number">0</span>,(struct sockaddr*)&amp;cli,len);  </span><br><span class="line">		<span class="comment">//缓冲区重置</span></span><br><span class="line">		sendBufferPointer=<span class="number">0</span>;    </span><br><span class="line">		recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">		<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">		<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	close(sockfd);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参数与设置"><a href="#参数与设置" class="headerlink" title="参数与设置"></a>参数与设置</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LOCAL_SERVER_IP <span class="meta-string">"127.0.0.2"</span></span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发</category>
        <category>网络</category>
      </categories>
  </entry>
  <entry>
    <title>多级中文域名解析系统-2</title>
    <url>/2018/04/05/%E5%A4%9A%E7%BA%A7%E4%B8%AD%E6%96%87%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%B3%BB%E7%BB%9F-2%EF%BC%88c%E8%AF%AD%E8%A8%80%EF%BC%89/</url>
    <content><![CDATA[<h2 id="LOCAL-DNS"><a href="#LOCAL-DNS" class="headerlink" title="LOCAL_DNS"></a>LOCAL_DNS</h2><p><em>local DNS作为本地网络中的逻辑核心非常重要</em></p>
 <a id="more"></a>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"defAndTools.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"localServer.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CACHEFILE <span class="meta-string">"localCache.txt"</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">isEnd</span><span class="params">(struct DNS_Header *header)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (header-&gt;authorNum!=<span class="number">0</span>)</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//①设置监听</span></span><br><span class="line">	<span class="keyword">int</span> serverSocket = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line">	<span class="comment">//发送缓冲区和接收缓冲区</span></span><br><span class="line">	<span class="keyword">char</span> sendbuf[<span class="number">512</span>];</span><br><span class="line">	<span class="keyword">char</span> recvbuf[<span class="number">512</span>]; </span><br><span class="line">	<span class="keyword">int</span> sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="comment">//初始化缓冲区</span></span><br><span class="line">	<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">		</span><br><span class="line">	<span class="comment">//声明两个套接字sockaddr_in结构体，分别用于客户端和服务器 </span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">server_addr</span>;</span>  </span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">clientAddr</span>;</span>  </span><br><span class="line">	<span class="keyword">int</span> addr_len = <span class="keyword">sizeof</span>(clientAddr);  </span><br><span class="line">	</span><br><span class="line">	<span class="keyword">int</span> client;  </span><br><span class="line">	<span class="comment">//初始化服务器端的套接字</span></span><br><span class="line">	bzero(&amp;server_addr, <span class="keyword">sizeof</span>(server_addr));  </span><br><span class="line">	server_addr.sin_family = AF_INET;  </span><br><span class="line">	server_addr.sin_port = htons(SERVER_PORT); </span><br><span class="line">	server_addr.sin_addr.s_addr = inet_addr(LOCAL_SERVER_IP);  </span><br><span class="line">	 </span><br><span class="line">	<span class="comment">//绑定套接字</span></span><br><span class="line">	bind(serverSocket, (struct sockaddr *)&amp;server_addr, <span class="keyword">sizeof</span>(server_addr));</span><br><span class="line">	<span class="comment">//设置监听状态</span></span><br><span class="line">	listen(serverSocket, <span class="number">5</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//②循环监听</span></span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"监听端口 ： %d\n"</span>,SERVER_PORT);</span><br><span class="line">		<span class="comment">//调用accept，进入阻塞状态，返回一个client套接字描述符</span></span><br><span class="line">		client = accept(serverSocket, (struct sockaddr*)&amp;clientAddr, (<span class="keyword">socklen_t</span>*)&amp;addr_len);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"连接成功\n"</span>);</span><br><span class="line">	</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">c</span>;</span></span><br><span class="line">		<span class="keyword">socklen_t</span> cLen = <span class="keyword">sizeof</span>(c);</span><br><span class="line">		getpeername(client, (struct sockaddr*) &amp;c, &amp;cLen); </span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"请求端信息： %s : %d\n"</span>,inet_ntoa(c.sin_addr),ntohs(c.sin_port));	</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//对于TCP，先接收一个两字节的包长度</span></span><br><span class="line">		<span class="keyword">unsigned</span> <span class="keyword">short</span> recv_length;</span><br><span class="line">		recv(client,&amp;recv_length,<span class="number">2</span>,<span class="number">0</span>);</span><br><span class="line">		recv_length = ntohs(recv_length);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//接收客户端发来的数据，recv返回值为接收字节数</span></span><br><span class="line">		<span class="keyword">int</span> dataNum = recv(client,recvbuf,recv_length,<span class="number">0</span>);</span><br><span class="line">					</span><br><span class="line">		<span class="comment">//③提取recvbuf，构建DNS包</span></span><br><span class="line">		<span class="comment">//构造DNS包头部，从缓冲区读取一个DNS头部</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">client_query_header</span>;</span></span><br><span class="line">		client_query_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		decode_header(client_query_header,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"\n请求端信息：\n"</span>);</span><br><span class="line">		print_header(client_query_header);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//构造准备发送的DNS头部</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">query_header</span>;</span></span><br><span class="line">		query_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		<span class="built_in">memcpy</span>(query_header,client_query_header,<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		query_header-&gt;queryNum = <span class="number">1</span>;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//④解析并处理请求</span></span><br><span class="line">		<span class="comment">//有多少个请求，就进行几次循环，每个循环完成一次系统运作</span></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;client_query_header-&gt;queryNum;i++)   </span><br><span class="line">		&#123;</span><br><span class="line">			<span class="comment">//读取解析一个请求部分</span></span><br><span class="line">			<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span> *<span class="title">client_query_section</span>;</span></span><br><span class="line">			client_query_section = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">			decode_query_section(client_query_section,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n正在处理第 %d 个请求\n"</span>,i+<span class="number">1</span>);</span><br><span class="line">			print_query_section(client_query_section);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//判断本地缓存中是否存在</span></span><br><span class="line">			<span class="keyword">int</span> findInCache = firstFindRR(client_query_section,CACHEFILE,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">if</span> (findInCache==<span class="number">1</span>)</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">"在本地缓存中找到记录，直接回复请求\n"</span>);</span><br><span class="line">				<span class="keyword">goto</span> findit;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="comment">//本地缓存不存在</span></span><br><span class="line">			<span class="keyword">char</span> UDPsendbuf[<span class="number">512</span>];</span><br><span class="line">			<span class="keyword">char</span> UDPrecvbuf[<span class="number">512</span>]; </span><br><span class="line">			<span class="keyword">int</span> UDPsendBufferPointer=<span class="number">0</span>;</span><br><span class="line">			<span class="keyword">int</span> UDPrecvBufferPointer=<span class="number">0</span>;</span><br><span class="line">			<span class="built_in">memset</span>(UDPsendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">			<span class="built_in">memset</span>(UDPrecvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//直接将从客户端接受收的包写入缓冲区</span></span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n发送给根服务器的请求：\n"</span>);</span><br><span class="line">			encode_header(query_header,UDPsendbuf,&amp;UDPsendBufferPointer);</span><br><span class="line">			print_header(query_header);</span><br><span class="line">			encode_query_section(client_query_section,UDPsendbuf,&amp;UDPsendBufferPointer);</span><br><span class="line">			print_query_section(client_query_section);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//定义用于接收的结构。由于根服务器必然不可能返回最终结果，所以不需要构造answer</span></span><br><span class="line">			<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">recv_header</span>;</span></span><br><span class="line">			<span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span> *<span class="title">recv_answer</span>,*<span class="title">recv_authority</span>,*<span class="title">recv_additional</span>;</span></span><br><span class="line">			recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">			recv_authority = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">			recv_additional = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//与根服务器建立UDP连接</span></span><br><span class="line">			<span class="keyword">int</span> sockfd=socket(AF_INET,SOCK_DGRAM,<span class="number">0</span>);</span><br><span class="line">			<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">			addr.sin_family =AF_INET;</span><br><span class="line">			addr.sin_port =htons(SERVER_PORT);</span><br><span class="line">			addr.sin_addr.s_addr=inet_addr(ROOT_SERVER_IP);	</span><br><span class="line">			</span><br><span class="line">			bind(sockfd,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//发送  </span></span><br><span class="line">			sendto(sockfd,UDPsendbuf,UDPsendBufferPointer,<span class="number">0</span>,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//接收回复</span></span><br><span class="line">			<span class="keyword">socklen_t</span> len=<span class="keyword">sizeof</span>(addr);</span><br><span class="line">			recvfrom(sockfd,UDPrecvbuf,<span class="keyword">sizeof</span>(UDPrecvbuf),<span class="number">0</span>,(struct sockaddr*)&amp;addr,&amp;len);   </span><br><span class="line">			</span><br><span class="line">			<span class="comment">//断开</span></span><br><span class="line">			close(sockfd); </span><br><span class="line">			</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n收到根服务器的回复：\n"</span>);</span><br><span class="line">			<span class="comment">//从接收缓冲区解析并构造包结构然后打印</span></span><br><span class="line">			decode_header(recv_header,UDPrecvbuf,&amp;UDPrecvBufferPointer);</span><br><span class="line">			print_header(recv_header);</span><br><span class="line">			 </span><br><span class="line">			decode_resource_record(recv_authority,UDPrecvbuf,&amp;UDPrecvBufferPointer);</span><br><span class="line">			print_resource_record(recv_authority);</span><br><span class="line">			decode_resource_record(recv_additional,UDPrecvbuf,&amp;UDPrecvBufferPointer);</span><br><span class="line">			print_resource_record(recv_additional);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//重置缓冲区</span></span><br><span class="line">			UDPsendBufferPointer=<span class="number">0</span>;</span><br><span class="line">			UDPrecvBufferPointer=<span class="number">0</span>;</span><br><span class="line">			<span class="built_in">memset</span>(UDPsendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">			<span class="built_in">memset</span>(UDPrecvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//用于计数本次循环是第几级服务器发来的</span></span><br><span class="line">			<span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">			<span class="keyword">while</span>(isEnd(recv_header)==<span class="number">0</span>)  </span><br><span class="line">			&#123;</span><br><span class="line">				count++;</span><br><span class="line">				<span class="comment">//向下一个服务器建立UDP连接</span></span><br><span class="line">				<span class="keyword">int</span> sockfm=socket(AF_INET,SOCK_DGRAM,<span class="number">0</span>);</span><br><span class="line">				<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">addr</span>;</span></span><br><span class="line">				addr.sin_family =AF_INET;</span><br><span class="line">				addr.sin_port =htons(SERVER_PORT);</span><br><span class="line">				addr.sin_addr.s_addr=inet_addr(recv_additional-&gt;rdata);		</span><br><span class="line">				<span class="comment">//recv_additional-&gt;rdata即保存着下一个服务器的IP</span></span><br><span class="line">				</span><br><span class="line">				bind(sockfm,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">				</span><br><span class="line">				<span class="comment">//重置缓冲区和结构</span></span><br><span class="line">				UDPsendBufferPointer=<span class="number">0</span>;</span><br><span class="line">				UDPrecvBufferPointer=<span class="number">0</span>;</span><br><span class="line">				<span class="built_in">memset</span>(UDPsendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">				<span class="built_in">memset</span>(UDPrecvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">				<span class="built_in">free</span>(recv_header);     recv_header = <span class="literal">NULL</span>;           </span><br><span class="line">				<span class="built_in">free</span>(recv_authority);  recv_authority = <span class="literal">NULL</span>;</span><br><span class="line">				<span class="built_in">free</span>(recv_additional); recv_additional = <span class="literal">NULL</span>;</span><br><span class="line">				</span><br><span class="line">				<span class="comment">//直接将从客户端接受收的请求写入发送缓冲区</span></span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">"\n发送给 %d级 服务器的请求：\n"</span>,count);</span><br><span class="line">				encode_header(query_header,UDPsendbuf,&amp;UDPsendBufferPointer);</span><br><span class="line">				print_header(query_header);</span><br><span class="line">				encode_query_section(client_query_section,UDPsendbuf,&amp;UDPsendBufferPointer);</span><br><span class="line">				print_query_section(client_query_section);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">//发送</span></span><br><span class="line">				sendto(sockfm,UDPsendbuf,UDPsendBufferPointer,<span class="number">0</span>,(struct sockaddr*)&amp;addr,<span class="keyword">sizeof</span>(addr));</span><br><span class="line">				</span><br><span class="line">				<span class="comment">//接收回复</span></span><br><span class="line">				len=<span class="keyword">sizeof</span>(addr);</span><br><span class="line">				recvfrom(sockfm,UDPrecvbuf,<span class="keyword">sizeof</span>(UDPrecvbuf),<span class="number">0</span>,(struct sockaddr*)&amp;addr,&amp;len);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">//断开连接</span></span><br><span class="line">				close(sockfm); </span><br><span class="line">				</span><br><span class="line">				<span class="comment">//开始处理</span></span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">"\n收到 %d级 服务器发来的回复：\n"</span>,count);	</span><br><span class="line">				</span><br><span class="line">				recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">				recv_answer = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">				recv_authority = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">				recv_additional = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">				</span><br><span class="line">				decode_header(recv_header,UDPrecvbuf,&amp;UDPrecvBufferPointer);</span><br><span class="line">				print_header(recv_header);</span><br><span class="line">				</span><br><span class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;recv_header-&gt;answerNum;j++)&#123;   </span><br><span class="line">					decode_resource_record(recv_answer,UDPrecvbuf,&amp;UDPrecvBufferPointer);</span><br><span class="line">					print_resource_record(recv_answer);</span><br><span class="line">				&#125;</span><br><span class="line">				</span><br><span class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;recv_header-&gt;authorNum;j++)&#123;  </span><br><span class="line">					decode_resource_record(recv_authority,UDPrecvbuf,&amp;UDPrecvBufferPointer);</span><br><span class="line">					print_resource_record(recv_authority);</span><br><span class="line">				&#125;</span><br><span class="line">				 </span><br><span class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;recv_header-&gt;addNum;j++)&#123;      </span><br><span class="line">					decode_resource_record(recv_additional,UDPrecvbuf,&amp;UDPrecvBufferPointer);</span><br><span class="line">					print_resource_record(recv_additional);</span><br><span class="line">				&#125;			</span><br><span class="line">			&#125;</span><br><span class="line">			</span><br><span class="line">			<span class="comment">//UDP请求的循环结束，此时构造得到的结构体已经得到该次请求目标结果，且已经在循环中打印  </span></span><br><span class="line">			<span class="comment">//将从最终结果服务器返回来的结构写入发送缓冲区，本次循环结束</span></span><br><span class="line">			encode_header(recv_header,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;recv_header-&gt;answerNum;j++)&#123;</span><br><span class="line">				encode_resource_record(recv_answer,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">				<span class="comment">//将结果写入cache</span></span><br><span class="line">				addRRToCache(recv_answer,<span class="string">"localCache.txt"</span>);</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;recv_header-&gt;authorNum;j++)&#123;</span><br><span class="line">				encode_resource_record(recv_authority,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">				addRRToCache(recv_authority,<span class="string">"localCache.txt"</span>);</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;recv_header-&gt;addNum;j++)&#123;</span><br><span class="line">				encode_resource_record(recv_additional,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">				addRRToCache(recv_additional,<span class="string">"localCache.txt"</span>);</span><br><span class="line">			&#125;</span><br><span class="line">			</span><br><span class="line">		findit:;</span><br><span class="line">			<span class="comment">//⑤发送缓冲</span></span><br><span class="line">			<span class="comment">//发送已准备好的在缓冲区的数据,包总长度即为当下发送缓冲区指针下标</span></span><br><span class="line">			<span class="keyword">unsigned</span> <span class="keyword">short</span> send_length = htons(sendBufferPointer);</span><br><span class="line">			send(client,&amp;send_length,<span class="number">2</span>,<span class="number">0</span>);</span><br><span class="line">			send(client, sendbuf, sendBufferPointer, <span class="number">0</span>);   </span><br><span class="line">			<span class="comment">//一个请求的解析与回答发送结束,清空发送缓冲区与指针，准备进行下一次发送</span></span><br><span class="line">			sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">			<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);	</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//对一个客户端的所有请求解析结束</span></span><br><span class="line">		close(client);</span><br><span class="line">		recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">		<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"连接关闭\n"</span>);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"===================================\n\n"</span>);</span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p><em>Client为本机主逻辑</em></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;errno.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netdb.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdint.h&gt;</span></span></span><br><span class="line"><span class="comment">//底层操作</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"defAndTools.h"</span></span></span><br><span class="line"><span class="comment">//本地服务器</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"localServer.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">	<span class="comment">//容错</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=(argc<span class="number">-1</span>)/<span class="number">2</span>;i++)</span><br><span class="line">		<span class="keyword">if</span> (strTypeToCode(argv[<span class="number">2</span>*i])==<span class="number">0</span>)</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">"类型错误！\n"</span>);</span><br><span class="line">				<span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">			&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//①连接本地服务器 初始化TCP连接</span></span><br><span class="line">	<span class="keyword">int</span> clientSocket = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line">	<span class="comment">//发送缓冲和接收缓冲</span></span><br><span class="line">	<span class="keyword">char</span> sendbuf[<span class="number">512</span>];    </span><br><span class="line">	<span class="keyword">char</span> recvbuf[<span class="number">512</span>];</span><br><span class="line">	<span class="comment">//定义缓冲区指示下标</span></span><br><span class="line">	<span class="keyword">int</span> sendBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">	<span class="comment">//清空缓冲区</span></span><br><span class="line">	<span class="built_in">memset</span>(sendbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">serverAddr</span>;</span></span><br><span class="line">	serverAddr.sin_family = AF_INET;</span><br><span class="line">	serverAddr.sin_port = htons(SERVER_PORT);</span><br><span class="line">	serverAddr.sin_addr.s_addr = inet_addr(LOCAL_SERVER_IP);</span><br><span class="line">	<span class="comment">//连接服务器</span></span><br><span class="line">	<span class="keyword">if</span>(connect(clientSocket, (struct sockaddr *)&amp;serverAddr, <span class="keyword">sizeof</span>(serverAddr))==<span class="number">-1</span>)</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"连接失败\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"发送信息：\n"</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//②根据输入内容，准备DNS包，并写入发送缓冲区</span></span><br><span class="line">	<span class="comment">//定义DNS头部</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">query_header</span>;</span></span><br><span class="line">	query_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">	<span class="comment">//调用函数，填写欲发送的DNS包的头部结构体</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> tag = create_tag(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>);    </span><br><span class="line">	<span class="comment">//argc-1除2即为域名请求的个数，因为每个域名参数后带一个类型</span></span><br><span class="line">	create_query_header(query_header,<span class="number">999</span>,tag,argc/<span class="number">2</span>,<span class="number">0x0000</span>,<span class="number">0x0000</span>,<span class="number">0x0000</span>);</span><br><span class="line">	<span class="comment">//将头部写入缓冲区</span></span><br><span class="line">	encode_header(query_header,sendbuf,&amp;sendBufferPointer);</span><br><span class="line">	<span class="comment">//打印生成的头部</span></span><br><span class="line">	print_header(query_header);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//根据运行参数生成一个或多个请求部分并写入缓冲区                         </span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=(argc<span class="number">-1</span>)/<span class="number">2</span>;i++) </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="comment">//填写DNS请求结构，argv[2*i]字符串对应的类型</span></span><br><span class="line">		<span class="keyword">unsigned</span> <span class="keyword">short</span> qtype = strTypeToCode(argv[<span class="number">2</span>*i]);    </span><br><span class="line">		<span class="keyword">unsigned</span> <span class="keyword">short</span> qclass = <span class="number">0x0001</span>;  </span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span> *<span class="title">query_section</span>;</span></span><br><span class="line">		query_section = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">		create_query_section(query_section,argv[<span class="number">2</span>*i<span class="number">-1</span>],qtype,qclass);</span><br><span class="line">		encode_query_section(query_section,sendbuf,&amp;sendBufferPointer);	</span><br><span class="line">		print_query_section(query_section);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//③向本地服务器发包</span></span><br><span class="line">	<span class="comment">//发送已准备好的在缓冲区的数据,包总长度即为当下发送缓冲区指针下标</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> length = htons(sendBufferPointer);</span><br><span class="line">	<span class="comment">//对于TCP连接，必须先发送一个DNS包总长度，否则wireshark不会识别！</span></span><br><span class="line">	send(clientSocket,&amp;length,<span class="number">2</span>,<span class="number">0</span>);</span><br><span class="line">	send(clientSocket, sendbuf, sendBufferPointer, <span class="number">0</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//④根据请求数量收包,有多少个请求就会收到多少个DNS包</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;query_header-&gt;queryNum;k++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">unsigned</span> <span class="keyword">short</span> recv_length;</span><br><span class="line">		recv(clientSocket,&amp;recv_length,<span class="number">2</span>,<span class="number">0</span>);</span><br><span class="line">		recv_length = ntohs(recv_length);</span><br><span class="line">		<span class="keyword">int</span> dataNum = recv(clientSocket, recvbuf, recv_length, <span class="number">0</span>);</span><br><span class="line">	</span><br><span class="line">		<span class="comment">//⑤处理接收到缓冲区的DNS包,从中抽取出需要返还给用户的数据</span></span><br><span class="line">		<span class="comment">//构造DNS包头部，从缓冲区读取并填充DNS头部	</span></span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">recv_header</span>;</span></span><br><span class="line">		recv_header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">		decode_header(recv_header,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"[回复： %d]\n"</span>,k+<span class="number">1</span>);</span><br><span class="line">		print_header(recv_header);	</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span> *<span class="title">recv_answer</span>,*<span class="title">recv_add</span>;</span></span><br><span class="line">		<span class="comment">//标准回复只可能在answer和addition有值，所以只需要考虑读这两个部分</span></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;recv_header-&gt;answerNum;i++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="comment">//读取解析打印一个回应部分</span></span><br><span class="line">			recv_answer = <span class="literal">NULL</span>;</span><br><span class="line">			recv_answer = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">			decode_resource_record(recv_answer,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">			print_resource_record(recv_answer);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;recv_header-&gt;addNum;i++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="comment">//读取解析打印一个addition部分</span></span><br><span class="line">			recv_add = <span class="literal">NULL</span>;</span><br><span class="line">			recv_add = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_QUERY));</span><br><span class="line">			decode_resource_record(recv_add,recvbuf,&amp;recvBufferPointer);</span><br><span class="line">			print_resource_record(recv_add);</span><br><span class="line">		&#125;</span><br><span class="line">		recvBufferPointer=<span class="number">0</span>;</span><br><span class="line">		<span class="built_in">memset</span>(recvbuf,<span class="number">0</span>,<span class="number">512</span>);	</span><br><span class="line">	&#125;</span><br><span class="line">	close(clientSocket);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title>多级中文域名解析系统-1（c语言）</title>
    <url>/2018/04/05/%E4%B8%AD%E6%96%87DNS%E5%AE%9E%E7%8E%B0%EF%BC%88c%E8%AF%AD%E8%A8%80%EF%BC%89/</url>
    <content><![CDATA[<h2 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h2><ul>
<li><p>支持中文域名解析，支持缓存与多请求</p>
</li>
<li><p>遵循RFC1180标准，可被wireshark抓包检测到</p>
</li>
</ul>
<ul>
<li><p>本系统包含一个客户端，四个中间多级DNS，一个根DNS，一个本地DNS</p>
</li>
<li><p>难点在于①不用python而使用C语言必须写非常多的底层操作，主要集中在defAndTools库中②DNS请求的逻辑</p>
</li>
</ul>
<h2 id="Project-Structure"><a href="#Project-Structure" class="headerlink" title="Project Structure"></a>Project Structure</h2><div class="table-container">
<table>
<thead>
<tr>
<th>CODES</th>
<th>EXPLAINATION</th>
</tr>
</thead>
<tbody>
<tr>
<td>defAndTools.h</td>
<td>此包为主要工具包，涉及网络中的核心底层的操作，是主要难点之一</td>
</tr>
<tr>
<td>LOCAL_DNS.c</td>
<td>本地DNS服务器核心逻辑，是本网络中的核心部件，是主要难点之一</td>
</tr>
<tr>
<td>Client.c</td>
<td>客户端主逻辑</td>
</tr>
<tr>
<td>ROOT_DNS.c</td>
<td>根域名解析服务器的逻辑代码</td>
</tr>
<tr>
<td>DNS1.c</td>
<td>第一级域名解析服务器主逻辑</td>
</tr>
<tr>
<td>DNS2.c</td>
<td>第二级域名解析服务器主逻辑</td>
</tr>
<tr>
<td>DNS3.c</td>
<td>第三级域名解析服务器主逻辑</td>
</tr>
<tr>
<td>DNS4.c</td>
<td>第四级域名解析服务器主逻辑</td>
</tr>
<tr>
<td>localServer.h</td>
<td>本地DNS服务器的参数设置</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>FILES</th>
<th>EXPLAINATION</th>
</tr>
</thead>
<tbody>
<tr>
<td>localCache.txt</td>
<td>本地DNS服务器的缓存</td>
</tr>
<tr>
<td>RRL1.txt</td>
<td>第一级域名解析服务器的缓存</td>
</tr>
<tr>
<td>RRL2.txt</td>
<td>第二级域名解析服务器的缓存</td>
</tr>
<tr>
<td>RRL3.txt</td>
<td>第三级域名解析服务器的缓存</td>
</tr>
<tr>
<td>RRL4.txt</td>
<td>第四级域名解析服务器的缓存</td>
</tr>
<tr>
<td>RRroot.txt</td>
<td>根域名解析服务器的缓存</td>
</tr>
</tbody>
</table>
</div>
<h2 id="defAndTools"><a href="#defAndTools" class="headerlink" title="defAndTools"></a>defAndTools</h2><p><em>此包为主要工具包，涉及网络中的底层的操作</em></p>
 <a id="more"></a>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_SIZE_OF_DOMAIN 100</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_PORT 53</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ROOT_SERVER_IP <span class="meta-string">"127.0.0.3"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//用于储存域名方便处理的全局变量</span></span><br><span class="line"><span class="keyword">char</span> domain_temp[MAX_SIZE_OF_DOMAIN];</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> id;       <span class="comment">//16位的消息ID标示一次正常的交互，该ID由消息请求者设置，消息响应者回复请求时带上该ID。</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> tag;      <span class="comment">//tag要拆，并单独写一个生成tag函数</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> queryNum; <span class="comment">//标示请求部分的条目数 </span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> answerNum;<span class="comment">//标示响应部分的资源记录数。如果响应消息中没有记录，则设置为0</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> authorNum;<span class="comment">//标示权威部分的域名服务器资源记录数。如果响应消息中没有权威记录，则设置为0</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> addNum;   <span class="comment">//标示额外部分的资源记录数。</span></span><br><span class="line">&#125;DNS_HEAD;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">DNS_Query</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">char</span> *name;              <span class="comment">//请求的域名。</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> qtype;    <span class="comment">//记录的类型 [A:0x0001] [NS:0x0002] [CNAME:0x0005] [MX:0x000F]</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> qclass;   <span class="comment">//请求的资源记录的类型 一般为[IN:0x0001]</span></span><br><span class="line">&#125;DNS_QUERY;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span>  </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">char</span> *name;   </span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> type;     <span class="comment">//请求的域名</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> _class;   <span class="comment">//响应的资源记录的类型 一般为[IN:0x0001]</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> ttl;        <span class="comment">//该资源记录被缓存的秒数。</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> data_len; <span class="comment">//RDATA部分的长度</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> pre;      <span class="comment">//MX特有的优先级 Preference</span></span><br><span class="line">	<span class="keyword">char</span> *rdata;	         <span class="comment">//[A:32位的IP地址（4字节）] [CNAME/NS/MX:域名]</span></span><br><span class="line">&#125;DNS_ResouceRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">tag</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> qr;       <span class="comment">//[1]标示该消息是请求消息（该位为0）还是应答消息（该位为1）</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> opcode;   <span class="comment">//[4]0  QUERY。标准查询</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> aa;       <span class="comment">//[1]只在响应消息中有效。该位标示响应该消息的域名服务器是该域中的权威域名服务器。因为Answer Section中可能会有很多域名</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> tc;       <span class="comment">//[1]标示这条消息是否因为长度超过UDP数据包的标准长度512字节，如果超过512字节，该位被设置为1</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> rd;       <span class="comment">//[1]是否递归查询。1为递归查询</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> ra;       <span class="comment">//[1]在响应消息中清除并设置。标示该DNS域名服务器是否支持递归查询。</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> z;        <span class="comment">//[3] 冗余res 0</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> rcode;    <span class="comment">//[4] 0  成功的响应</span></span><br><span class="line">&#125;TAG;</span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************缓冲区操作和工具***************************/</span></span><br><span class="line"><span class="comment">/*此函数用于向buffer中写入8bit数据</span></span><br><span class="line"><span class="comment"> * *buffer:指向缓冲区</span></span><br><span class="line"><span class="comment"> * *bufferPointer:目前已写入的缓冲区最新一位的下一位</span></span><br><span class="line"><span class="comment"> * 以下put16bits put32bits同理</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">put1Byte</span><span class="params">(<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer, <span class="keyword">char</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//调整value为网络字节序</span></span><br><span class="line">	value = htons(value);</span><br><span class="line">	<span class="comment">//void *memcpy(void *dest, void *src, unsigned int count);</span></span><br><span class="line">	<span class="comment">//用于 把资源内存（src所指向的内存区域） 拷贝到目标内存（dest所指向的内存区域）,count为拷贝区域大小.</span></span><br><span class="line">	<span class="comment">//buffer为缓冲区首地址，bufferPointer为缓冲区已写入下标，此函数参数为指向这两个量的指针，通过传递地址来实现主函数与子函数的实参传递。</span></span><br><span class="line">	<span class="comment">//value为欲写入缓冲区的数据(value为8bit)</span></span><br><span class="line">	<span class="built_in">memcpy</span>(buffer + *bufferPointer,&amp;value,<span class="number">1</span>);</span><br><span class="line">	<span class="comment">//缓冲区已写入下标向后移动，使其指向下一次写入时应该写入的位置,*bufferPointer为指针bufferPointer所指地址的内容</span></span><br><span class="line">	*bufferPointer += <span class="number">1</span>;	</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">put2Bytes</span><span class="params">(<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer, <span class="keyword">unsigned</span> <span class="keyword">short</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	value = htons(value);</span><br><span class="line">	<span class="built_in">memcpy</span>(buffer + *bufferPointer,&amp;value,<span class="number">2</span>);</span><br><span class="line">	*bufferPointer += <span class="number">2</span>;	</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">put4Bytes</span><span class="params">(<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer, <span class="keyword">unsigned</span> <span class="keyword">int</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	value = htons(value);</span><br><span class="line">	<span class="built_in">memcpy</span>(buffer + *bufferPointer,&amp;value,<span class="number">4</span>);</span><br><span class="line">	*bufferPointer += <span class="number">4</span>;	</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将变长字符串str写入buffer  </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">putDomainName</span><span class="params">(<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer, <span class="keyword">char</span> *str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">memcpy</span>(buffer + *bufferPointer,str,<span class="built_in">strlen</span>(str)+<span class="number">1</span>); <span class="comment">//末尾0需要一起打印</span></span><br><span class="line">	*bufferPointer += <span class="built_in">strlen</span>(str)+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//从缓冲区取16个位</span></span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="keyword">short</span> <span class="title">get2Bytes</span><span class="params">(<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> value;</span><br><span class="line">	<span class="built_in">memcpy</span>(&amp;value,buffer + *bufferPointer,<span class="number">2</span>);</span><br><span class="line">	*bufferPointer += <span class="number">2</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> ntohs(value);  </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="keyword">int</span> <span class="title">get4bits</span><span class="params">(<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> value;</span><br><span class="line">	<span class="built_in">memcpy</span>(&amp;value,buffer + *bufferPointer,<span class="number">4</span>);</span><br><span class="line">	*bufferPointer += <span class="number">4</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> ntohs(value);  </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//读取变长字符串str 读取到0即停止 0即为'\0' </span></span><br><span class="line"><span class="comment">//域名不考虑字节序问题 ，也不用考虑编码问题，都是一个字节一个字节读</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getDomainName</span><span class="params">(<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer,<span class="keyword">int</span> *lengthOfDomain)</span>  </span></span><br><span class="line"><span class="function"></span>&#123;   </span><br><span class="line">	</span><br><span class="line">	<span class="keyword">int</span> valueWriting=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(buffer[*bufferPointer]!=<span class="number">0</span>)  </span><br><span class="line">	&#123;        </span><br><span class="line">		domain_temp[valueWriting] = buffer[*bufferPointer]; </span><br><span class="line">		valueWriting++;</span><br><span class="line">		(*bufferPointer)++;</span><br><span class="line">	&#125;</span><br><span class="line">	domain_temp[valueWriting] = <span class="number">0</span>; <span class="comment">//末尾为0，写入字符串结束符，方便对字符数组进行字符串操作</span></span><br><span class="line">	(*bufferPointer)++; <span class="comment">//缓冲区读写下一位指针指示跳过末尾0</span></span><br><span class="line">	*lengthOfDomain = valueWriting+<span class="number">1</span>; <span class="comment">//包含了末尾结束符 </span></span><br><span class="line">	</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//eg 3www6google3com0   </span></span><br><span class="line"><span class="comment">//生成域名编码</span></span><br><span class="line"><span class="comment">//一个UTF8 数字占1个字节。一个UTF8汉字占3个字节</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">encode_domain</span><span class="params">(<span class="keyword">char</span>* domain)</span>           </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">memset</span>(domain_temp,<span class="number">0</span>,MAX_SIZE_OF_DOMAIN);   </span><br><span class="line">	<span class="keyword">int</span> valueWriting=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">char</span> *p,*q;</span><br><span class="line">	q = domain;</span><br><span class="line">	p = q;</span><br><span class="line">	<span class="keyword">char</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)   </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span>((*p==<span class="string">'.'</span>)||(*p==<span class="number">0</span>))</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="comment">//第一位为count,写入字符串  </span></span><br><span class="line">			*(domain_temp+valueWriting)=count;  <span class="comment">//此处最后一位0的情况写入了 </span></span><br><span class="line">			valueWriting += <span class="number">1</span>;</span><br><span class="line">			<span class="comment">//写入q开始，长度为count的字符串（长度为count)</span></span><br><span class="line">			<span class="built_in">memcpy</span>(domain_temp+valueWriting,q,count);</span><br><span class="line">			valueWriting += count; </span><br><span class="line">			</span><br><span class="line">			<span class="comment">//计数清0</span></span><br><span class="line">			count = <span class="number">0</span>;</span><br><span class="line">			<span class="comment">//如果未读到字符串末尾，将q移动到p+1的位置，重新开始下一轮</span></span><br><span class="line">			<span class="keyword">if</span> (*p==<span class="string">'.'</span>)</span><br><span class="line">			&#123;</span><br><span class="line">				q=p+<span class="number">1</span>;</span><br><span class="line">				p = q;</span><br><span class="line">			&#125;<span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			p++;</span><br><span class="line">			count++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//解析编码的域名</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">decode_domain</span><span class="params">(<span class="keyword">char</span>* domain)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">memset</span>(domain_temp,<span class="number">0</span>,MAX_SIZE_OF_DOMAIN);</span><br><span class="line">	<span class="keyword">int</span> valueWriting = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">char</span> *p = domain;  </span><br><span class="line">	<span class="keyword">int</span> count = *p;</span><br><span class="line">	<span class="keyword">while</span>(count!=<span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;count;i++)</span><br><span class="line">		&#123;</span><br><span class="line">			p += <span class="number">1</span>;</span><br><span class="line">			domain_temp[valueWriting] = *p;</span><br><span class="line">			valueWriting++;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (*(p+<span class="number">1</span>)!=<span class="number">0</span>) </span><br><span class="line">		&#123;</span><br><span class="line">			domain_temp[valueWriting] = <span class="string">'.'</span>;</span><br><span class="line">			valueWriting++;</span><br><span class="line">		&#125;</span><br><span class="line">		p += <span class="number">1</span>;</span><br><span class="line">		count = *p;</span><br><span class="line">	&#125;</span><br><span class="line">	domain_temp[valueWriting]=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//OPCODE、Z、RCODE不用管，无论输入什么都为0。其他都是单位的</span></span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="keyword">short</span> <span class="title">create_tag</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">short</span> qr,<span class="keyword">unsigned</span> <span class="keyword">short</span> opcode,<span class="keyword">unsigned</span> <span class="keyword">short</span> aa,<span class="keyword">unsigned</span> <span class="keyword">short</span> tc,<span class="keyword">unsigned</span> <span class="keyword">short</span> rd,<span class="keyword">unsigned</span> <span class="keyword">short</span> ra,<span class="keyword">unsigned</span> <span class="keyword">short</span> z,<span class="keyword">unsigned</span> <span class="keyword">short</span> rcode)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">short</span> tag = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">if</span> (qr==<span class="number">1</span>)  tag = tag | <span class="number">0x8000</span>;</span><br><span class="line">	<span class="keyword">if</span> (aa==<span class="number">1</span>)  tag = tag | <span class="number">0x0400</span>;</span><br><span class="line">	<span class="keyword">if</span> (tc==<span class="number">1</span>)  tag = tag | <span class="number">0x0200</span>;</span><br><span class="line">	<span class="keyword">if</span> (rd==<span class="number">1</span>)  tag = tag | <span class="number">0x0100</span>;</span><br><span class="line">	<span class="keyword">if</span> (ra==<span class="number">1</span>)  tag = tag | <span class="number">0x0080</span>;</span><br><span class="line">	<span class="keyword">return</span> tag;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//类型的名字与编码的转换</span></span><br><span class="line"><span class="function"><span class="keyword">unsigned</span> <span class="keyword">short</span> <span class="title">strTypeToCode</span><span class="params">(<span class="keyword">char</span>* type)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">strcmp</span>(type,<span class="string">"A"</span>)==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0x0001</span>;</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">strcmp</span>(type,<span class="string">"NS"</span>)==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0x0002</span>;</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">strcmp</span>(type,<span class="string">"CNAME"</span>)==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0x0005</span>;</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">strcmp</span>(type,<span class="string">"MX"</span>)==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0x000F</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">codeTypeToStr</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">short</span> num)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (num==<span class="number">0x0001</span>) <span class="keyword">return</span> <span class="string">"A"</span>;</span><br><span class="line">	<span class="keyword">if</span> (num==<span class="number">0x0002</span>) <span class="keyword">return</span> <span class="string">"NS"</span>;</span><br><span class="line">	<span class="keyword">if</span> (num==<span class="number">0x0005</span>) <span class="keyword">return</span> <span class="string">"CNAME"</span>;</span><br><span class="line">	<span class="keyword">if</span> (num==<span class="number">0x000F</span>) <span class="keyword">return</span> <span class="string">"MX"</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="string">"ERROR"</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************DNS头部操作***************************/</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *此函数用于填充客户端发送请求的dns包的头部</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">create_query_header</span><span class="params">(struct DNS_Header *query_header,<span class="keyword">unsigned</span> <span class="keyword">short</span> id,<span class="keyword">unsigned</span> <span class="keyword">short</span> tag,<span class="keyword">unsigned</span> <span class="keyword">short</span> queryNum,<span class="keyword">unsigned</span> <span class="keyword">short</span> answerNum,<span class="keyword">unsigned</span> <span class="keyword">short</span> authorNum,<span class="keyword">unsigned</span> <span class="keyword">short</span> addNum)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	query_header-&gt;id = id;</span><br><span class="line">	query_header-&gt;tag = tag;</span><br><span class="line">	query_header-&gt;queryNum = queryNum;</span><br><span class="line">	query_header-&gt;answerNum = answerNum;</span><br><span class="line">	query_header-&gt;authorNum = authorNum;</span><br><span class="line">	query_header-&gt;addNum = addNum;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*此函数用于将已经填充好的dns头部结构体的成员依次写入buffer</span></span><br><span class="line"><span class="comment"> *  *header: 指向已填充好的dns头部结构体的指针</span></span><br><span class="line"><span class="comment"> *  *buffer: 指向缓冲区</span></span><br><span class="line"><span class="comment"> *  *bufferPointer: 目前已写入的缓冲区最新一位的下一位</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">encode_header</span><span class="params">(struct DNS_Header *header,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	put2Bytes(buffer,bufferPointer,header-&gt;id);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,header-&gt;tag);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,header-&gt;queryNum);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,header-&gt;answerNum);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,header-&gt;authorNum);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,header-&gt;addNum);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">decode_header</span><span class="params">(struct DNS_Header *header,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	header-&gt;id=get2Bytes(buffer,bufferPointer);</span><br><span class="line">	header-&gt;tag=get2Bytes(buffer,bufferPointer);</span><br><span class="line">	header-&gt;queryNum=get2Bytes(buffer,bufferPointer);</span><br><span class="line">	header-&gt;answerNum=get2Bytes(buffer,bufferPointer);</span><br><span class="line">	header-&gt;authorNum=get2Bytes(buffer,bufferPointer);</span><br><span class="line">	header-&gt;addNum=get2Bytes(buffer,bufferPointer);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_header</span><span class="params">(struct DNS_Header *query_header)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"[DNS HEADER]\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"ID         :         %d\n"</span>,query_header-&gt;id);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"TAG        :         0x%x\n"</span>,query_header-&gt;tag);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"QueryNum   :         %d\n"</span>,query_header-&gt;queryNum);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"AnswerNum  :         %d\n"</span>,query_header-&gt;answerNum);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"AuthorNum  :         %d\n"</span>,query_header-&gt;authorNum);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"AddNum     :         %d\n"</span>,query_header-&gt;addNum);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************DNS请求部分操作***************************/</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *生成DNS包的请求部分</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">create_query_section</span><span class="params">(struct DNS_Query *query_section,<span class="keyword">char</span>* domain_name, <span class="keyword">unsigned</span> <span class="keyword">short</span> qtype, <span class="keyword">unsigned</span> <span class="keyword">short</span> qclass)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> domain_length = <span class="built_in">strlen</span>(domain_name);</span><br><span class="line">	query_section-&gt;name = <span class="built_in">malloc</span>(domain_length+<span class="number">1</span>);</span><br><span class="line">	<span class="built_in">memcpy</span>(query_section-&gt;name,domain_name,domain_length+<span class="number">1</span>);	</span><br><span class="line">	</span><br><span class="line">	query_section-&gt;qtype = qtype;</span><br><span class="line">	query_section-&gt;qclass = qclass;	</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *将已经填充好的dns的一个请求结构体的成员依次写入buffer(调用一次该函数只写入一个请求</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">encode_query_section</span><span class="params">(struct DNS_Query *query_section,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//先计算用decodeDomain得到字符串</span></span><br><span class="line">	<span class="comment">//再用strlen计算字符串长度为点语法name长度+2（头尾多了一个数字）</span></span><br><span class="line">	<span class="comment">//再发送 </span></span><br><span class="line">	<span class="keyword">char</span> *domain_name;</span><br><span class="line">	<span class="keyword">int</span> lengthOfEncodedDomain = <span class="built_in">strlen</span>(query_section-&gt;name)+<span class="number">2</span>;</span><br><span class="line">	domain_name = <span class="built_in">malloc</span>(lengthOfEncodedDomain);</span><br><span class="line">	encode_domain(query_section-&gt;name);</span><br><span class="line">	<span class="built_in">memcpy</span>(domain_name,domain_temp,lengthOfEncodedDomain);</span><br><span class="line">	putDomainName(buffer,bufferPointer,domain_name); </span><br><span class="line">	</span><br><span class="line">	put2Bytes(buffer,bufferPointer,query_section-&gt;qtype);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,query_section-&gt;qclass);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *解析请求部分。解析即为将缓冲区的字节流提取，转码，生成对应的结构体</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">decode_query_section</span><span class="params">(struct DNS_Query *query_section,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//从缓冲区读出编码过的域名</span></span><br><span class="line">	<span class="keyword">char</span>* domain_name = <span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN); </span><br><span class="line">	<span class="built_in">memset</span>(domain_name,<span class="number">0</span>,MAX_SIZE_OF_DOMAIN);</span><br><span class="line">	<span class="keyword">int</span> lengthOfDomain=<span class="number">0</span>;</span><br><span class="line">	getDomainName(buffer,bufferPointer,&amp;lengthOfDomain);</span><br><span class="line">	<span class="built_in">memcpy</span>(domain_name,domain_temp,lengthOfDomain);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//解码域名</span></span><br><span class="line">	decode_domain(domain_name);</span><br><span class="line">	<span class="built_in">memcpy</span>(domain_name,domain_temp,<span class="built_in">strlen</span>(domain_name));  </span><br><span class="line">	</span><br><span class="line">	query_section-&gt;name = domain_name;</span><br><span class="line">	query_section-&gt;qtype = get2Bytes(buffer,bufferPointer);</span><br><span class="line">	query_section-&gt;qclass = get2Bytes(buffer,bufferPointer);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_query_section</span><span class="params">(struct DNS_Query *query_section)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"[DNS QUERY]\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Name       :         %s\n"</span>,query_section-&gt;name);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Type       :         %s\n"</span>,codeTypeToStr(query_section-&gt;qtype));</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Class      :         IN\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************DNS RR操作和RR文件解析操作***************************/</span></span><br><span class="line"><span class="comment">//生成resource record记录</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">create_resource_record</span><span class="params">(struct DNS_RR *resource_record,<span class="keyword">char</span>* name, <span class="keyword">unsigned</span> <span class="keyword">short</span> type, <span class="keyword">unsigned</span> <span class="keyword">short</span> _class, <span class="keyword">unsigned</span> <span class="keyword">int</span> ttl, <span class="keyword">unsigned</span> <span class="keyword">short</span> pre,<span class="keyword">char</span> *rdata)</span> <span class="comment">//data_len不用输入  </span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//unsigned short pre为一个MX类型特有的优先级，定长，只有MX类型发送。</span></span><br><span class="line">	<span class="keyword">int</span> domain_length = <span class="built_in">strlen</span>(name);</span><br><span class="line">	<span class="comment">//易错点：strlen只读到0但不包含0，所以为了把结束符也复制进去，长度要+1</span></span><br><span class="line">	resource_record-&gt;name = <span class="built_in">malloc</span>(domain_length+<span class="number">1</span>);   </span><br><span class="line">	<span class="built_in">memcpy</span>(resource_record-&gt;name,name,domain_length+<span class="number">1</span>);</span><br><span class="line">	</span><br><span class="line">	resource_record-&gt;type = type;</span><br><span class="line">	resource_record-&gt;_class = _class;</span><br><span class="line">	resource_record-&gt;ttl = ttl;       <span class="comment">//data_len</span></span><br><span class="line">	<span class="keyword">if</span> (type==<span class="number">0x0001</span>) resource_record-&gt;data_len=<span class="number">4</span>;  <span class="comment">//对于IP，长度为4 data_len是编码后的长度，length是非编码长度，注意</span></span><br><span class="line">		<span class="keyword">else</span> resource_record-&gt;data_len = <span class="built_in">strlen</span>(rdata) + <span class="number">2</span>;      <span class="comment">//对于域名，生成data_len包含末尾结束符（域名末尾结束符）</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment">//pre</span></span><br><span class="line">	<span class="keyword">if</span> (type==<span class="number">0x000F</span>) &#123;</span><br><span class="line">		resource_record-&gt;pre = pre;</span><br><span class="line">		resource_record-&gt;data_len += <span class="number">2</span>;  <span class="comment">//对于邮件类型，由于有pre的存在，多占两个字节</span></span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//char* rdata</span></span><br><span class="line">	<span class="keyword">int</span> rdata_length = <span class="built_in">strlen</span>(rdata);  <span class="comment">//要加上末尾结束符</span></span><br><span class="line">	resource_record-&gt;rdata = <span class="built_in">malloc</span>(rdata_length+<span class="number">1</span>);</span><br><span class="line">	<span class="built_in">memcpy</span>(resource_record-&gt;rdata,rdata,rdata_length+<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//编码resource record记录，编码即为将结构体的内容编码，处理为字节流，写入缓冲区</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">encode_resource_record</span><span class="params">(struct DNS_RR *resource_record,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">char</span> *domain_name;</span><br><span class="line">	<span class="keyword">int</span> lengthOfEncodedDomain = <span class="built_in">strlen</span>(resource_record-&gt;name)+<span class="number">2</span>;</span><br><span class="line">	domain_name = <span class="built_in">malloc</span>(lengthOfEncodedDomain);</span><br><span class="line">	 </span><br><span class="line">	encode_domain(resource_record-&gt;name);</span><br><span class="line">	<span class="built_in">memcpy</span>(domain_name,domain_temp,lengthOfEncodedDomain);</span><br><span class="line">	</span><br><span class="line">	putDomainName(buffer,bufferPointer,domain_name); </span><br><span class="line">	</span><br><span class="line">	put2Bytes(buffer,bufferPointer,resource_record-&gt;type);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,resource_record-&gt;_class);</span><br><span class="line">	put4Bytes(buffer,bufferPointer,resource_record-&gt;ttl);</span><br><span class="line">	put2Bytes(buffer,bufferPointer,resource_record-&gt;data_len);   </span><br><span class="line">	<span class="keyword">if</span> (resource_record-&gt;type==<span class="number">0x000F</span>) </span><br><span class="line">		put2Bytes(buffer,bufferPointer,resource_record-&gt;pre);</span><br><span class="line">		</span><br><span class="line">	<span class="comment">//如果类型为A，发送的是IP，将IP写入缓冲区               </span></span><br><span class="line">	<span class="keyword">if</span>(resource_record-&gt;type == <span class="number">0x0001</span>)         </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="comment">//不能调用get put函数，因为inet_addr自带字节序变换功能</span></span><br><span class="line">		<span class="keyword">unsigned</span> <span class="keyword">int</span> rdata = inet_addr(resource_record-&gt;rdata);</span><br><span class="line">		<span class="built_in">memcpy</span>(buffer + *bufferPointer,&amp;rdata,<span class="number">4</span>);</span><br><span class="line">		*bufferPointer += <span class="number">4</span>;</span><br><span class="line">	</span><br><span class="line">	&#125;<span class="keyword">else</span>&#123;          </span><br><span class="line">	<span class="comment">//如果类型为MX、CNAME、NS</span></span><br><span class="line">	<span class="comment">//则发送的是域名，则调用域名编码</span></span><br><span class="line">		<span class="keyword">char</span> *rdata;</span><br><span class="line">		<span class="keyword">int</span> lengthOfEncodedDomain2 = <span class="built_in">strlen</span>(resource_record-&gt;rdata)+<span class="number">2</span>;</span><br><span class="line">		rdata = <span class="built_in">malloc</span>(lengthOfEncodedDomain2);</span><br><span class="line">		encode_domain(resource_record-&gt;rdata);</span><br><span class="line">		<span class="built_in">memcpy</span>(rdata,domain_temp,lengthOfEncodedDomain2);   </span><br><span class="line">		putDomainName(buffer,bufferPointer,rdata); </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//解析resource record记录</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">decode_resource_record</span><span class="params">(struct DNS_RR *resource_record,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//从缓冲区读出编码过的域名</span></span><br><span class="line">	<span class="keyword">char</span>* domain_name = <span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN); </span><br><span class="line">	<span class="built_in">memset</span>(domain_name,<span class="number">0</span>,MAX_SIZE_OF_DOMAIN);</span><br><span class="line">	<span class="keyword">int</span> lengthOfDomain=<span class="number">0</span>;</span><br><span class="line">	getDomainName(buffer,bufferPointer,&amp;lengthOfDomain);</span><br><span class="line">	<span class="built_in">memcpy</span>(domain_name,domain_temp,lengthOfDomain);</span><br><span class="line">	<span class="comment">//解码域名</span></span><br><span class="line">	</span><br><span class="line">	decode_domain(domain_name);</span><br><span class="line">	<span class="built_in">memcpy</span>(domain_name,domain_temp,<span class="built_in">strlen</span>(domain_name));  </span><br><span class="line">	resource_record-&gt;name = domain_name;</span><br><span class="line">	</span><br><span class="line">	resource_record-&gt;type = get2Bytes(buffer,bufferPointer);</span><br><span class="line">	resource_record-&gt;_class = get2Bytes(buffer,bufferPointer);</span><br><span class="line">	resource_record-&gt;ttl = get4bits(buffer,bufferPointer);   </span><br><span class="line">	resource_record-&gt;data_len = get2Bytes(buffer,bufferPointer);</span><br><span class="line">	<span class="keyword">if</span> (resource_record-&gt;type==<span class="number">0x000F</span>) </span><br><span class="line">			resource_record-&gt;pre = get2Bytes(buffer,bufferPointer);</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//如果发送的是IP（类型为A），则读出IP 。 不能采用get put方法，因为inet_ntoa方法已经更换字节序</span></span><br><span class="line">	<span class="keyword">if</span>(resource_record-&gt;type == <span class="number">0x0001</span>)   </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">unsigned</span> <span class="keyword">int</span> rdata;</span><br><span class="line">		<span class="built_in">memcpy</span>(&amp;rdata,buffer + *bufferPointer,<span class="number">4</span>);</span><br><span class="line">		*bufferPointer += <span class="number">4</span>;</span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">in_addr</span> <span class="title">in</span>;</span></span><br><span class="line">		<span class="built_in">memcpy</span>(&amp;in, &amp;rdata, <span class="number">4</span>);  </span><br><span class="line">		</span><br><span class="line">		resource_record-&gt;rdata = <span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN);</span><br><span class="line">		<span class="keyword">char</span> *temp =  inet_ntoa(in);</span><br><span class="line">		<span class="built_in">memcpy</span>(resource_record-&gt;rdata,temp,<span class="built_in">strlen</span>(temp)+<span class="number">1</span>);   <span class="comment">//+1是为了包含末尾0    </span></span><br><span class="line">	&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">		<span class="comment">//如果发送的是域名，则调用域名解码（类型为CNAME NS MX）</span></span><br><span class="line">		<span class="comment">//从缓冲区读出编码过的域名</span></span><br><span class="line">		<span class="keyword">char</span>* rdata = <span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN); </span><br><span class="line">		<span class="keyword">int</span> lengthOfDomain2=<span class="number">0</span>;</span><br><span class="line">		getDomainName(buffer,bufferPointer,&amp;lengthOfDomain2);</span><br><span class="line">		<span class="built_in">memcpy</span>(rdata,domain_temp,lengthOfDomain2);</span><br><span class="line">		<span class="comment">//解码域名</span></span><br><span class="line">		decode_domain(rdata);</span><br><span class="line">		<span class="built_in">memcpy</span>(rdata,domain_temp,<span class="built_in">strlen</span>(rdata));  </span><br><span class="line">		resource_record-&gt;rdata = rdata;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_resource_record</span><span class="params">(struct DNS_RR *resource_record)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"[RESOURCE RECORD]\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Name       :         %s\n"</span>,resource_record-&gt;name);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Type       :         %s\n"</span>,codeTypeToStr(resource_record-&gt;type));</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Class      :         IN\n"</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"TTL        :         %d\n"</span>,resource_record-&gt;ttl);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Data_Len   :         %d\n"</span>,resource_record-&gt;data_len);</span><br><span class="line">	<span class="keyword">if</span> (resource_record-&gt;type==<span class="number">0x000F</span>) </span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"Preference :         %d\n"</span>,resource_record-&gt;pre);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"IP|DOMAIN  :         %s\n"</span>,resource_record-&gt;rdata);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"===================================\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//砍掉一个域名第一个.之前的部分,如果已经是最后一节，指向域名的指针指向NULL</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cut</span><span class="params">(<span class="keyword">char</span>** domainPointer)</span>  <span class="comment">//这里传入的是 指向指向域名的指针的指针</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		(*domainPointer)++;</span><br><span class="line">		<span class="keyword">if</span> (**domainPointer==<span class="string">'.'</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			(*domainPointer)++;</span><br><span class="line">			<span class="keyword">break</span>;		</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (**domainPointer==<span class="number">0</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			*domainPointer = <span class="literal">NULL</span>;</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************文件读写***************************/</span></span><br><span class="line"><span class="comment">//将RR写进cache文件里</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addRRToCache</span><span class="params">(struct DNS_RR *resource_record, <span class="keyword">char</span>* cacheFile)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	FILE *RR = fopen(cacheFile, <span class="string">"a+"</span>);</span><br><span class="line">	<span class="built_in">fprintf</span>(RR,<span class="string">"%s         "</span>,resource_record-&gt;name);</span><br><span class="line">	<span class="built_in">fprintf</span>(RR,<span class="string">"%d         "</span>,resource_record-&gt;ttl);</span><br><span class="line">	<span class="built_in">fprintf</span>(RR,<span class="string">"IN         "</span>);</span><br><span class="line">	<span class="built_in">fprintf</span>(RR,<span class="string">"%s         "</span>,codeTypeToStr(resource_record-&gt;type));</span><br><span class="line">	<span class="built_in">fprintf</span>(RR,<span class="string">"%s\n"</span>,resource_record-&gt;rdata);</span><br><span class="line">	fclose(RR);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//第一次在RR文件里扫描 （初次搜索函数）</span></span><br><span class="line"><span class="comment">//如果找到了，返回1，且encode进buffer</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">firstFindRR</span><span class="params">(struct DNS_Query *query_section,<span class="keyword">char</span> *RRDOCUMENT,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> over = <span class="number">0</span>;</span><br><span class="line">	FILE *RR = fopen( RRDOCUMENT, <span class="string">"r"</span> );</span><br><span class="line">	<span class="comment">//定义一个RR结构体用来储存从文件中读入的一条RR</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span> *<span class="title">fileRR</span>;</span></span><br><span class="line">	fileRR = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">	<span class="built_in">memset</span>(fileRR,<span class="number">0</span>,<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">	fileRR-&gt;name=<span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN);  </span><br><span class="line">	fileRR-&gt;rdata=<span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN);</span><br><span class="line">	<span class="comment">//第一次搜索</span></span><br><span class="line">	<span class="keyword">while</span>(<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,fileRR-&gt;name)!=EOF)   </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">fscanf</span>(RR,<span class="string">"%d"</span>,&amp;fileRR-&gt;ttl);</span><br><span class="line">		<span class="keyword">char</span> type[<span class="number">10</span>],_class[<span class="number">10</span>];</span><br><span class="line">		<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,_class);</span><br><span class="line">		<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,type);</span><br><span class="line">		fileRR-&gt;type = strTypeToCode(type);</span><br><span class="line">		<span class="built_in">fscanf</span>(RR,<span class="string">"%s\n"</span>,fileRR-&gt;rdata);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span>((<span class="built_in">strcmp</span>(query_section-&gt;name,fileRR-&gt;name)==<span class="number">0</span>) &amp;&amp; (query_section-&gt;qtype==fileRR-&gt;type))</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">"\n发送回复：\n"</span>);</span><br><span class="line">			<span class="comment">//生成answer RR</span></span><br><span class="line">			create_resource_record(fileRR,fileRR-&gt;name, fileRR-&gt;type, <span class="number">0x0001</span>, fileRR-&gt;ttl, <span class="number">0x0000</span>,fileRR-&gt;rdata);</span><br><span class="line">			<span class="comment">//生成头</span></span><br><span class="line">			<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">header</span>;</span></span><br><span class="line">			header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">			<span class="keyword">unsigned</span> <span class="keyword">short</span> tag = create_tag(<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">			<span class="keyword">if</span> (<span class="built_in">strcmp</span>(type,<span class="string">"MX"</span>)==<span class="number">0</span>)   create_query_header(header,<span class="number">0x1235</span>,tag,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">				<span class="keyword">else</span> create_query_header(header,<span class="number">999</span>,tag,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">			<span class="comment">//将头和answer encode进buffer</span></span><br><span class="line">			encode_header(header,buffer,bufferPointer);</span><br><span class="line">			print_header(header);</span><br><span class="line">			encode_resource_record(fileRR,buffer,bufferPointer);</span><br><span class="line">			print_resource_record(fileRR);</span><br><span class="line">			over=<span class="number">1</span>;</span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">//读指针回到开头</span></span><br><span class="line">	fseek(RR,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">	<span class="comment">//对于MX类型，特殊，需要再搜索一遍，搜索到的邮件服务器域名的IP，并写入addition RR中发送    </span></span><br><span class="line">	<span class="keyword">if</span> ((fileRR-&gt;type==<span class="number">0x000F</span>)&amp;&amp;(over==<span class="number">1</span>)) </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span> *<span class="title">addFileRR</span>;</span></span><br><span class="line">		addFileRR = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">		addFileRR-&gt;name=<span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN);</span><br><span class="line">		addFileRR-&gt;rdata=<span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN);</span><br><span class="line">		<span class="keyword">while</span>(<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,addFileRR-&gt;name)!=EOF)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%d "</span>,&amp;addFileRR-&gt;ttl);</span><br><span class="line">			<span class="keyword">char</span> type[<span class="number">10</span>],_class[<span class="number">10</span>];</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,_class);</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,type);</span><br><span class="line">			addFileRR-&gt;type = strTypeToCode(type);</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%s\n"</span>,addFileRR-&gt;rdata);</span><br><span class="line">			<span class="keyword">if</span>(<span class="built_in">strcmp</span>(fileRR-&gt;rdata,addFileRR-&gt;name)==<span class="number">0</span>)</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">"邮件服务器：\n"</span>);</span><br><span class="line">				<span class="comment">//生成addition RR</span></span><br><span class="line">				create_resource_record(addFileRR,fileRR-&gt;rdata, <span class="number">1</span>, <span class="number">1</span>, fileRR-&gt;ttl, <span class="number">0</span>, addFileRR-&gt;rdata);</span><br><span class="line">				encode_resource_record(addFileRR,buffer,bufferPointer);</span><br><span class="line">				print_resource_record(addFileRR);</span><br><span class="line">				<span class="keyword">break</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;	</span><br><span class="line">	&#125;</span><br><span class="line">	fclose(RR);</span><br><span class="line">	<span class="keyword">return</span> over;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">loopFindNS</span><span class="params">(struct DNS_Query *query_section,<span class="keyword">char</span> *RRDOCUMENT,<span class="keyword">char</span> *buffer,<span class="keyword">int</span> *bufferPointer)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	FILE *RR = fopen( RRDOCUMENT, <span class="string">"r"</span> );</span><br><span class="line">	cut(&amp;query_section-&gt;name);</span><br><span class="line">	<span class="comment">//剪掉首段地址，进行第二次搜索</span></span><br><span class="line">	<span class="keyword">while</span>(query_section-&gt;name!=<span class="literal">NULL</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		fseek(RR,<span class="number">0</span>,<span class="number">0</span>);  </span><br><span class="line">		</span><br><span class="line">		<span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span> *<span class="title">nextRR</span>;</span></span><br><span class="line">		nextRR = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">		nextRR-&gt;name=<span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN);</span><br><span class="line">		nextRR-&gt;rdata=<span class="built_in">malloc</span>(MAX_SIZE_OF_DOMAIN);</span><br><span class="line">		<span class="keyword">while</span>(<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,nextRR-&gt;name)!=EOF)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%d "</span>,&amp;nextRR-&gt;ttl);</span><br><span class="line">			<span class="keyword">char</span> type[<span class="number">10</span>],_class[<span class="number">10</span>];</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,_class);</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%s "</span>,type);</span><br><span class="line">			nextRR-&gt;type = strTypeToCode(type);</span><br><span class="line">			<span class="built_in">fscanf</span>(RR,<span class="string">"%s\n"</span>,nextRR-&gt;rdata);</span><br><span class="line">			<span class="keyword">if</span>(<span class="built_in">strcmp</span>(query_section-&gt;name,nextRR-&gt;name)==<span class="number">0</span>)</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">"\n下一级服务器信息：\n"</span>);</span><br><span class="line">				<span class="comment">//生成头</span></span><br><span class="line">				<span class="class"><span class="keyword">struct</span> <span class="title">DNS_Header</span> *<span class="title">header</span>;</span></span><br><span class="line">				header = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_HEAD));</span><br><span class="line">				<span class="keyword">unsigned</span> <span class="keyword">short</span> tag = create_tag(<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">				create_query_header(header,<span class="number">999</span>,tag,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">				encode_header(header,buffer,bufferPointer);</span><br><span class="line">				print_header(header);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">//生成authority RR  NS记录type=2   此时query_section-&gt;name经过cut后已经变成了下一个要去的DNS服务器域名</span></span><br><span class="line">				<span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span> *<span class="title">authRR</span>;</span></span><br><span class="line">				authRR = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">				create_resource_record(authRR, query_section-&gt;name, <span class="number">2</span>, <span class="number">1</span>, nextRR-&gt;ttl, <span class="number">0</span>, query_section-&gt;name);</span><br><span class="line">				encode_resource_record(authRR,buffer,bufferPointer);</span><br><span class="line">				print_resource_record(authRR);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">//生成additon RR   A记录type=1</span></span><br><span class="line">				<span class="class"><span class="keyword">struct</span> <span class="title">DNS_RR</span> *<span class="title">addRR</span>;</span></span><br><span class="line">				addRR = <span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(DNS_ResouceRecord));</span><br><span class="line">				create_resource_record(addRR, query_section-&gt;name, <span class="number">1</span>, <span class="number">1</span>, nextRR-&gt;ttl, <span class="number">0</span>, nextRR-&gt;rdata);</span><br><span class="line">				encode_resource_record(addRR,buffer,bufferPointer);</span><br><span class="line">				print_resource_record(addRR);</span><br><span class="line">				</span><br><span class="line">				<span class="keyword">goto</span> out;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;	</span><br><span class="line">		cut(&amp;query_section-&gt;name);	</span><br><span class="line">	&#125;</span><br><span class="line">	out:</span><br><span class="line">	fclose(RR);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title>2018</title>
    <url>/2018/01/01/2018/</url>
    <content><![CDATA[<p>呼，又一年过去了。 愿望实现了吗？努力了吗？坚持希望了吗？有未来吗？</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>递归算法设计</title>
    <url>/2017/10/19/%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<p>一、递归算法设计</p>
<p>设计一个递归算法，先要搞清楚最基本的模型</p>
<p>先考虑最小模型（最基本模型）应该如何操作</p>
<p>写出基本模型和递推式之后，再确定参数传递</p>
<p>其中注意抓住（单或多）入口与（单或多）出口</p>
<p>进入入口之后就不要管其中间过程，将他们视为整体，转而去出口语句处，思考对最小单元执行什么内容，判断是否出栈。</p>
 <a id="more"></a>
<p>（即只管整体（入栈）与最小单元（出栈口判断与操作））</p>
<p>（这时候结合二叉树递归遍历就很好理解。</p>
<p>对于栈底，先遍历左子树，然后第二条语句执行时，第一条语句遍历左子树已经执行结束，也就是说对于栈底层来说，左子树已经完全遍历结束</p>
<p>这对于每一个中间节点都是成立的，我们只需要考虑当层，不用管中间过程，这个中间过程就是递归，我们在设计最小单元、入口出口的时候就已经设计好了）</p>
<p>二、递归大体等效为两种</p>
<p>一种等效于正向循环（直到不满足前一直执行），另一种回溯（直到满足条件再反向执行）</p>
<p>这取决于执行体相对于递归语句的位置</p>
<p>递归思想在树与图中应用广泛，可以说是重要爆了</p>
<p>设计算法要充分发掘隐含实际情况的隐含特性</p>
<p>TBC.</p>
]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>验收</title>
    <url>/2017/05/15/%E9%AA%8C%E6%94%B6/</url>
    <content><![CDATA[<p>问老师验收中</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>Know more ,know less</title>
    <url>/2017/04/20/know-more-know-less/</url>
    <content><![CDATA[<p>知道越多，知道越少</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>がんばれ！</title>
    <url>/2016/11/09/%E3%81%8C%E3%82%93%E3%81%B0%E3%82%8C%EF%BC%81/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>CO-原码、反码、补码</title>
    <url>/2016/10/18/CO-%E5%8E%9F%E7%A0%81%E3%80%81%E5%8F%8D%E7%A0%81%E3%80%81%E8%A1%A5%E7%A0%81/</url>
    <content><![CDATA[<p>转载自<a href="http://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html" target="_blank" rel="noopener">http://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html</a></p>
<h1 id="原码-反码-补码-详解"><a href="#原码-反码-补码-详解" class="headerlink" title="原码, 反码, 补码 详解"></a><a href="http://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html" target="_blank" rel="noopener">原码, 反码, 补码 详解</a></h1><p>本篇文章讲解了计算机的原码, 反码和补码. 并且进行了深入探求了为何要使用反码和补码, 以及更进一步的论证了为何可以用反码, 补码的加法计算原码的减法. 论证部分如有不对的地方请各位牛人帮忙指正! 希望本文对大家学习计算机基础有所帮助!  </p>
<h2 id="一-机器数和真值"><a href="#一-机器数和真值" class="headerlink" title="一. 机器数和真值"></a>一. 机器数和真值</h2><p>在学习原码, 反码和补码之前, 需要先了解机器数和真值的概念.</p>
 <a id="more"></a>
<h3 id="1、机器数"><a href="#1、机器数" class="headerlink" title="1、机器数"></a>1、机器数</h3><p>一个数在计算机中的二进制表示形式,  叫做这个数的机器数。机器数是带符号的，在计算机用一个数的最高位存放符号, 正数为0, 负数为1. 比如，十进制中的数 +3 ，计算机字长为8位，转换成二进制就是00000011。如果是 -3 ，就是 10000011 。 那么，这里的 00000011 和 10000011 就是机器数。</p>
<h3 id="2、真值"><a href="#2、真值" class="headerlink" title="2、真值"></a>2、真值</h3><blockquote>
<p>因为第一位是符号位，所以机器数的形式值就不等于真正的数值。例如上面的有符号数 10000011，其最高位1代表负，其真正数值是 -3 而不是形式值131（10000011转换成十进制等于131）。所以，为区别起见，将带符号位的机器数对应的真正数值称为机器数的真值。</p>
</blockquote>
<p>例：0000 0001的真值 = +000 0001 = +1，1000 0001的真值 = –000 0001 = –1  </p>
<h2 id="二-原码-反码-补码的基础概念和计算方法"><a href="#二-原码-反码-补码的基础概念和计算方法" class="headerlink" title="二. 原码, 反码, 补码的基础概念和计算方法."></a>二. 原码, 反码, 补码的基础概念和计算方法.</h2><p>在探求为何机器要使用补码之前, 让我们先了解原码, 反码和补码的概念.对于一个数, 计算机要使用一定的编码方式进行存储. 原码, 反码, 补码是机器存储一个具体数字的编码方式.</p>
<h3 id="1-原码"><a href="#1-原码" class="headerlink" title="1. 原码"></a>1. 原码</h3><p>原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制:</p>
<blockquote>
<p>[+1]原 = 0000 0001 [-1]原 = 1000 0001</p>
</blockquote>
<p>第一位是符号位. 因为第一位是符号位, 所以8位二进制数的取值范围就是:</p>
<blockquote>
<p>[1111 1111 , 0111 1111]</p>
</blockquote>
<p>即</p>
<blockquote>
<p>[-127 , 127]</p>
</blockquote>
<p>原码是人脑最容易理解和计算的表示方式.</p>
<h3 id="2-反码"><a href="#2-反码" class="headerlink" title="2. 反码"></a>2. 反码</h3><p>反码的表示方法是: 正数的反码是其本身 负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.</p>
<blockquote>
<p>[+1] = [00000001]原 = [00000001]反 [-1] = [10000001]原 = [11111110]反</p>
</blockquote>
<p>可见如果一个反码表示的是负数, 人脑无法直观的看出来它的数值. 通常要将其转换成原码再计算.</p>
<h3 id="3-补码"><a href="#3-补码" class="headerlink" title="3. 补码"></a>3. 补码</h3><p>补码的表示方法是: 正数的补码就是其本身 负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)</p>
<blockquote>
<p>[+1] = [00000001]原 = [00000001]反 = [00000001]补 [-1] = [10000001]原 = [11111110]反 = [11111111]补</p>
</blockquote>
<p>对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码在计算其数值.  </p>
<h2 id="三-为何要使用原码-反码和补码"><a href="#三-为何要使用原码-反码和补码" class="headerlink" title="三. 为何要使用原码, 反码和补码"></a>三. 为何要使用原码, 反码和补码</h2><p>在开始深入学习前, 我的学习建议是先”死记硬背”上面的原码, 反码和补码的表示方式以及计算方法. 现在我们知道了计算机可以有三种编码方式表示一个数. 对于正数因为三种编码方式的结果都相同:</p>
<blockquote>
<p>[+1] = [00000001]原 = [00000001]反 = [00000001]补</p>
</blockquote>
<p>所以不需要过多解释. 但是对于负数:</p>
<blockquote>
<p>[-1] = [10000001]原 = [11111110]反 = [11111111]补</p>
</blockquote>
<p>可见原码, 反码和补码是完全不同的. 既然原码才是被人脑直接识别并用于计算表示方式, 为何还会有反码和补码呢? 首先, 因为人脑可以知道第一位是符号位, 在计算的时候我们会根据符号位, 选择对真值区域的加减. (真值的概念在本文最开头). 但是对于计算机, 加减乘数已经是最基础的运算, 要设计的尽量简单. 计算机辨别”符号位”显然会让计算机的基础电路设计变得十分复杂! 于是人们想出了将符号位也参与运算的方法. 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了. 于是人们开始探索 将符号位参与运算, 并且只保留加法的方法. 首先来看原码: 计算十进制的表达式: 1-1=0</p>
<blockquote>
<p>1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2</p>
</blockquote>
<p>如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数. 为了解决原码做减法的问题, 出现了反码: 计算十进制的表达式: 1-1=0</p>
<blockquote>
<p>1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0</p>
</blockquote>
<p>发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在”0”这个特殊的数值上. 虽然人们理解上+0和-0是一样的, 但是0带符号是没有任何意义的. 而且会有[0000 0000]原和[1000 0000]原两个编码表示0. 于是补码的出现, 解决了0的符号以及两个编码的问题:</p>
<blockquote>
<p>1-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原</p>
</blockquote>
<p>这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了.而且可以用[1000 0000]表示-128:</p>
<blockquote>
<p>(-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补</p>
</blockquote>
<p>-1-127的结果应该是-128, 在用补码运算的结果中, [1000 0000]补 就是-128. 但是注意因为实际上是使用以前的-0的补码来表示-128, 所以-128并没有原码和反码表示.(对-128的补码表示[1000 0000]补算出来的原码是[0000 0000]原, 这是不正确的) 使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127]. 因为机器使用补码, 所以对于编程中常用到的32位int类型, 可以表示范围是: [-231, 231-1] 因为第一位表示的是符号位.而使用补码表示时又可以多保存一个最小值.  </p>
<h2 id="四-原码-反码-补码-再深入"><a href="#四-原码-反码-补码-再深入" class="headerlink" title="四 原码, 反码, 补码 再深入"></a>四 原码, 反码, 补码 再深入</h2><p>计算机巧妙地把符号位参与运算, 并且将减法变成了加法, 背后蕴含了怎样的数学原理呢? 将钟表想象成是一个1位的12进制数. 如果当前时间是6点, 我希望将时间设置成4点, 需要怎么做呢?我们可以:</p>
<blockquote>
<p>1. 往回拨2个小时: 6 - 2 = 4 2. 往前拨10个小时: (6 + 10) mod 12 = 4 3. 往前拨10+12=22个小时: (6+22) mod 12 =4</p>
</blockquote>
<p>2,3方法中的mod是指取模操作, 16 mod 12 =4 即用16除以12后的余数是4. 所以钟表往回拨(减法)的结果可以用往前拨(加法)替代! 现在的焦点就落在了如何用一个正数, 来替代一个负数. 上面的例子我们能感觉出来一些端倪, 发现一些规律. 但是数学是严谨的. 不能靠感觉. 首先介绍一个数学中相关的概念: 同余  </p>
<h3 id="同余的概念"><a href="#同余的概念" class="headerlink" title="同余的概念"></a>同余的概念</h3><p>两个整数a，b，若它们除以整数m所得的余数相等，则称a，b对于模m同余 记作 a ≡ b (mod m) 读作 a 与 b 关于模 m 同余。 举例说明:</p>
<blockquote>
<p>4 mod 12 = 4 16 mod 12 = 4 28 mod 12 = 4</p>
</blockquote>
<p>所以4, 16, 28关于模 12 同余.  </p>
<h3 id="负数取模"><a href="#负数取模" class="headerlink" title="负数取模"></a>负数取模</h3><p>正数进行mod运算是很简单的. 但是负数呢? 下面是关于mod运算的数学定义: <a href="http://images.cnblogs.com/cnblogs_com/zhangziqiu/201103/201103302155507894.jpg" target="_blank" rel="noopener"><img src="http://images.cnblogs.com/cnblogs_com/zhangziqiu/201103/201103302155504514.jpg" alt="clip_image001" title="clip_image001"></a> 上面是截图, “取下界”符号找不到如何输入(word中粘贴过来后乱码). 下面是使用”L”和”J”替换上图的”取下界”符号:</p>
<blockquote>
<p>x mod y = x - y L x / y J</p>
</blockquote>
<p>上面公式的意思是: x mod y等于 x 减去 y 乘上 x与y的商的下界. 以 -3 mod 2 举例:</p>
<blockquote>
<p>-3 mod 2 = -3 - 2xL -3/2 J = -3 - 2xL-1.5J = -3 - 2x(-2) = -3 + 4 = 1</p>
</blockquote>
<p>所以:</p>
<blockquote>
<p>(-2) mod 12 = 12-2=10 (-4) mod 12 = 12-4 = 8 (-5) mod 12 = 12 - 5 = 7</p>
</blockquote>
<h3 id="开始证明"><a href="#开始证明" class="headerlink" title="开始证明"></a>开始证明</h3><p>再回到时钟的问题上:</p>
<blockquote>
<p>回拨2小时 = 前拨10小时 回拨4小时 = 前拨8小时 回拨5小时= 前拨7小时</p>
</blockquote>
<p>注意, 这里发现的规律! 结合上面学到的同余的概念.实际上:</p>
<blockquote>
<p>(-2) mod 12 = 10 10 mod 12 = 10</p>
</blockquote>
<p>-2与10是同余的.</p>
<blockquote>
<p>(-4) mod 12 = 8 8 mod 12 = 8</p>
</blockquote>
<p>-4与8是同余的. 距离成功越来越近了. 要实现用正数替代负数, 只需要运用同余数的两个定理: 反身性:</p>
<blockquote>
<p>a ≡ a (mod m)</p>
</blockquote>
<p>这个定理是很显而易见的. 线性运算定理:</p>
<blockquote>
<p>如果a ≡ b (mod m)，c ≡ d (mod m) 那么: (1)a ± c ≡ b ± d (mod m) (2)a <em> c ≡ b </em> d (mod m)</p>
</blockquote>
<p>如果想看这个定理的证明, 请看:<a href="http://baike.baidu.com/view/79282.htm" target="_blank" rel="noopener">http://baike.baidu.com/view/79282.htm</a> 所以:</p>
<blockquote>
<p>7 ≡ 7 (mod 12) (-2) ≡ 10 (mod 12) 7 -2 ≡ 7 + 10 (mod 12)</p>
</blockquote>
<p>现在我们为一个负数, 找到了它的正数同余数. 但是并不是7-2 = 7+10, 而是 7 -2 ≡ 7 + 10 (mod 12) , 即计算结果的余数相等. 接下来回到二进制的问题上, 看一下: 2-1=1的问题.</p>
<blockquote>
<p>2-1=2+(-1) = [0000 0010]原 + [1000 0001]原= [0000 0010]反 + [1111 1110]反</p>
</blockquote>
<p>先到这一步, -1的反码表示是1111 1110. 如果这里将[1111 1110]认为是原码, 则[1111 1110]原 = -126, 这里将符号位除去, 即认为是126. 发现有如下规律:</p>
<blockquote>
<p>(-1) mod 127 = 126 126 mod 127 = 126</p>
</blockquote>
<p>即:</p>
<blockquote>
<p>(-1) ≡ 126 (mod 127) 2-1 ≡ 2+126 (mod 127)</p>
</blockquote>
<p>2-1 与 2+126的余数结果是相同的! 而这个余数, 正式我们的期望的计算结果: 2-1=1 所以说一个数的反码, 实际上是这个数对于一个膜的同余数. 而这个膜并不是我们的二进制, 而是所能表示的最大值! 这就和钟表一样, 转了一圈后总能找到在可表示范围内的一个正确的数值! 而2+126很显然相当于钟表转过了一轮, 而因为符号位是参与计算的, 正好和溢出的最高位形成正确的运算结果. 既然反码可以将减法变成加法, 那么现在计算机使用的补码呢? 为什么在反码的基础上加1, 还能得到正确的结果?</p>
<blockquote>
<p>2-1=2+(-1) = [0000 0010]原 + [1000 0001]原 = [0000 0010]补 + [1111 1111]补</p>
</blockquote>
<p>如果把[1111 1111]当成原码, 去除符号位, 则:</p>
<blockquote>
<p>[0111 1111]原 = 127</p>
</blockquote>
<p>其实, 在反码的基础上+1, 只是相当于增加了膜的值:</p>
<blockquote>
<p>(-1) mod 128 = 127 127 mod 128 = 127 2-1 ≡ 2+127 (mod 128)</p>
</blockquote>
<p>此时, 表盘相当于每128个刻度转一轮. 所以用补码表示的运算结果最小值和最大值应该是[-128, 128]. 但是由于0的特殊情况, 没有办法表示128, 所以补码的取值范围是[-128, 127] 本人一直不善于数学, 所以如果文中有不对的地方请大家多多包含, 多多指点!</p>
]]></content>
      <categories>
        <category>计算机理论</category>
      </categories>
  </entry>
  <entry>
    <title>javascript扼要</title>
    <url>/2016/10/07/javascript%E6%89%BC%E8%A6%81/</url>
    <content><![CDATA[<p>  ~_~</p>
<p>继续开坑</p>
<p>html负责数据的显示，css负责样式的显示，javascript负责<strong>完成页面的交互</strong></p>
<p>javascript有很多<strong>框架</strong>（类似于OC中的Fundation Cocoa框架集等），最早的是prototype.js现在很少用</p>
<p>现在最流行的几种比如  jQuery  有很多插件，Extjs等，DWR框架（可以达到直接通过js调用java的代码）</p>
  <a id="more"></a>
<p>一、</p>
<p>①可直接在html中在script标签中写<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;script type=”text/javascript&gt;  (类似于css的&lt;style type=”text/css”&gt;..</span><br><span class="line"></span><br><span class="line">/*js代码*/</span><br><span class="line"></span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">②也可以引入外部文件（常用） css中用link  通过src指定外部文件位置</span><br><span class="line"></span><br><span class="line"> &lt;script type=”text/javascript” src=”hello.js&gt;&lt;/script&gt; </span><br><span class="line"></span><br><span class="line">注意：&lt;script&gt;标签**不能自结束！**</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">alert(“”);</span><br></pre></td></tr></table></figure></p>
<p>二、基本语法相关</p>
<p>很多东西 常用技巧习惯语法和java很像</p>
<p>①javascript是<strong>动态类型语言</strong>，而C java等是静态类型语言，动态类型语言是运行到哪读哪</p>
<p>②变量：js<strong>只能通过var来创建变量，</strong>eg: var a = 20 注意for(var i;i&lt;=10;i++) 不要习惯性把var写成int等 住手这不是c/java啊喂 xxxx</p>
<p><strong>注意：当在函数内部没有使用var来声明变量的时候，这个变量就会作为全局变量声明！所以在函数中定义变量一定要使用var</strong></p>
<p>③函数: 通过<strong>function来创建函数</strong></p>
<p>方法一： <code>function fn1() {   }</code></p>
<p>方法二：<code>var x=function(){ return a;}</code>然后x(); 就是这个函数</p>
<p>方法三：<code>function fn2(){}  var y=fn2;</code>（仅函数名 不带括号）</p>
<p>(理解..其实都一个原理，指针 内存等） 函数名字可以通过改指针改，这是静态语言无法做到的</p>
<p>④注释：同java</p>
<p>（html）通过<code>&lt;input  type=”xxx”&gt;&lt;/input&gt;</code>标签  等来插入按钮等  然后可以添加事件  比如xxx可以为button，则为添加一个按钮</p>
<p>⑤数据类型相关 常用</p>
<p><code>alert（typeof a);</code> 查看a的类型 注对于数组等对象而言返回object 对于函数返回function</p>
<p>强制类型转换：<code>Number(a)</code>（java中为（Number）a）  如果强制转换一个非数字的值为Number会得到一个<strong>NaN</strong>的值</p>
<p><code>parseInt(a)</code>可将字符串开头的几个数字转换为Int ，但如果开头不是数字则NaN</p>
<p>a instanceof Array 判断as是不是Array的实例 是则返回true</p>
<p><strong>布尔类型 true false 在js中，除了NaN，undefined，0这三个类型以外，其余皆为true</strong> （ 然后就可以通过这个特性搞事情</p>
<p>比如if(a) 等等</p>
<p>三、面向对象、原型（详细原理见二部分引用外部文章）</p>
<p>js中没有“类”的概念（ruby python也是） js的对象是基于原型拷贝的</p>
<p>动态语言一般都基于原型拷贝 只有一个根对象</p>
<p>“类”的定义<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">function Person(name,age)&#123;</span><br><span class="line"></span><br><span class="line">this.name=name; (定义了Person的一个属性为name)</span><br><span class="line"></span><br><span class="line">this.age= age;</span><br><span class="line"></span><br><span class="line">this.mmm=”ssssss”; (定义了Persion的一个属性为mmm）</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>必须要用this声明，如果没有this声明则仅仅只是函数的一个局部变量而不是类的属性</p>
<p><strong>就算要在类里面创建函数 也需要this！ eg :this.abc= function(){}</strong></p>
<p>js中每创建一个对象都存在一个function行为，占有内存，因为这个<strong>function行为属于一个属性</strong>，而java中不会这样，function不属于类的属性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var p1= new Person(“jack”,10);</span><br><span class="line"></span><br><span class="line">alert(p1.name);</span><br></pre></td></tr></table></figure>
<p>除了点语法以外，也可以通过对象[“属性字符串”]完成对属性的调用</p>
<p>如<code>alert(p1[“name”];</code></p>
<p>在js中对于对象而言 ，可以通过for in来遍历对象的属性 可以获取对象中<strong>所有的显示声明的属性</strong></p>
<p>e.g.<code>for (var a in p1){}</code></p>
<p>要显示p1的所有属性 遍历输出之 即可<code>for(var i in p1) {alert(p1[i]}</code></p>
<p>常用对象：查库。</p>
<hr>
<p>二部分引用外部文章：理解（重要，此文写得非常详细清晰）</p>
<h1 id="Javascript中的函数、this以及原型"><a href="#Javascript中的函数、this以及原型" class="headerlink" title="Javascript中的函数、this以及原型"></a><a href="http://www.cnblogs.com/xfrog/archive/2013/06/16/3138293.html" target="_blank" rel="noopener">Javascript中的函数、this以及原型</a></h1><p>链接：<strong><a href="http://www.cnblogs.com/xfrog/archive/2013/06/16/3138293.html" target="_blank" rel="noopener">http://www.cnblogs.com/xfrog/archive/2013/06/16/3138293.html</a></strong></p>
<p><strong>关于函数</strong></p>
<p>在Javascript中函数实际上就是一个对象，具有引用类型的特征，所以你可以将函数直接传递给变量，这个变量将表示指向函数“对象”的指针，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function test(message)&#123;</span><br><span class="line">          alert(message);</span><br><span class="line">     &#125;</span><br><span class="line">     var f = test;</span><br><span class="line">     f(&apos;hello world&apos;);</span><br></pre></td></tr></table></figure>
<p>你也可以直接将函数申明赋值给变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var f = function(message)&#123;</span><br><span class="line">          alert(message);     </span><br><span class="line">     &#125;;</span><br><span class="line">     f(&apos;hello world&apos;);</span><br></pre></td></tr></table></figure>
<p>在这种情况下，函数申明中可以省略函数名称，因为此时名称已经没有任何意义，我们可直接通过变量f来调用函数。</p>
<p>通过Function类型，我们可以更好地理解函数即对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var f = new Function(&quot;message&quot;,&quot;alert(message);&quot;);</span><br><span class="line">     f(&apos;hello world&apos;);</span><br></pre></td></tr></table></figure>
<p><strong>关于this</strong></p>
<p>this可以看成调用函数的实际作用域上下文。比较以下函数的执行结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function test()&#123;</span><br><span class="line">          this.property = &apos;hello world&apos;;</span><br><span class="line"></span><br><span class="line">     &#125;</span><br><span class="line">     test();</span><br><span class="line">     alert(window.property);   //由于在全局范围内调用，test函数中的this实际指向全局对象(window)</span><br><span class="line"></span><br><span class="line">     var obj = &#123;&#125;;</span><br><span class="line">     test.call(obj);      //通过call第一个参数指定执行上下文范围，所以test函数中this指向obj实例。</span><br><span class="line">     alert(obj.property);      </span><br><span class="line"></span><br><span class="line">     var obj2 = &#123;&#125;;</span><br><span class="line">     obj2.test2 = test;      //将obj2实例方法test指向 全局test方法</span><br><span class="line">     obj2.test2();            //由于是在obj2上调用test方法，所以test函数中的this也指向了obj2实例</span><br><span class="line">     alert(obj2.property);</span><br></pre></td></tr></table></figure>
<p><strong>定义类型</strong></p>
<p>在Javascript中可以定义构造函数，构造函数与一般函数没有任何区别，在创建实例时，如果我们使用了new关键字，那么这个函数就具有构造函数的特性，否则就是一般函数，如下所示，我们定义了一个Person类型：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function Person()&#123;</span><br><span class="line">     this.name = &apos;xfrog&apos;;</span><br><span class="line">     this.Say = function()&#123;</span><br><span class="line">          alert(this.name);</span><br><span class="line">     &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当使用new关键字时，可以创建一个新的Person对象实例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var p1 = new Person();</span><br><span class="line">p1.Say();</span><br></pre></td></tr></table></figure>
<p>如果不使用new关键字，将直接执行Person函数，由于执行上下文为全局范围，故name属性和Say方法将被添加到window对象：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; Person();</span><br><span class="line">&gt; Say();</span><br><span class="line">&gt; window.Say();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>原型</strong></p>
<p>注意上述Person的定义方式，当使用new来创建Person实例时，将会执行Person构造函数，也就是会声明name属性和Say方法，这样可能产生效率问题，注意以下代码：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; var p1 = new Person();</span><br><span class="line">&gt;      var p2 = new Person();</span><br><span class="line">&gt;      var test = p1.Say == p2.Say;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>比较p1和p2两个Say函数指针，返回false，表示每个Person实例中的Say方法都是独立的，而事实上Say函数的功能是完全一样的，我们完全没有必要为每个对象重新分配Say函数”对象“，如果Person实例很多，将会造成大量的内存耗用。</p>
<p>如果将Say函数提取出来放入全局执行范围，似乎可解决次问题：</p>
<p>&gt;<br>&gt;</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; function Person()&#123;</span><br><span class="line">&gt;           this.name = &apos;xfrog&apos;;</span><br><span class="line">&gt;           this.Say = say;     </span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      function say()&#123;</span><br><span class="line">&gt;           alert(this.name);     </span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      var p1 = new Person();</span><br><span class="line">&gt;      var p2 = new Person();</span><br><span class="line">&gt;      alert(p1.Say == p2.Say);</span><br><span class="line">&gt;      p1.name = &apos;wang&apos;;</span><br><span class="line">&gt;      p1.Say();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>由于this始终和执行上下文相关，p1和p2实例中的Say方法中会正确地返回对应实例的name属性。但是，使用此方式有违面向对象的思想，也失去了类型密封的原则。还会造成大量的全局函数。</p>
<p>为了解决这些缺点，Javascript引出了原型的概念，简单理解，原型可以看成是类型的共享区，原型本身是一个对象，而对象中的属性对于类型来说是共享的。Javascript中每个类型通过prototype属性来表示原型，通过这个属性可指定共享方法：</p>
<p>&gt;<br>&gt;</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; function Person()&#123;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt;      Person.prototype.name = &apos;xfrog&apos;;</span><br><span class="line">&gt;      Person.prototype.Say = function()&#123;</span><br><span class="line">&gt;           alert(this.name);</span><br><span class="line">&gt;      &#125;;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      var p1 = new Person();</span><br><span class="line">&gt;      var p2 = new Person();</span><br><span class="line">&gt;      alert(p1.Say == p2.Say);     //返回true</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>为什么这里可以通过p1.Say来访问Say方法呢？这是因为ECMAScript标准规定了类型属性的查找顺序：先在类型的实例上查找，如果没有则继续在类型原型上查找，这一查找路径采用短路算法，即找到首个后即返回，考虑如下代码：</p>
<p>&gt;<br>&gt;</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; function Person()&#123;</span><br><span class="line">&gt;           this.name = &apos;wang&apos;;</span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      Person.prototype.name = &apos;xfrog&apos;;</span><br><span class="line">&gt;      Person.prototype.Say = function()&#123;</span><br><span class="line">&gt;           alert(this.name);</span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      var p1 = new Person();</span><br><span class="line">&gt;      p1.Say();      //将返回wang</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>上面提到prototype实际上是一个对象，那么我们是否可以直接访问呢？ 在一些浏览器实现（如Chrome、Fixfox等）的确可通过实例的<strong>proto</strong>属性来访问内部的prototype对象，这种特征表明Javascript引擎在每个对象的内部都是通过一个变量来保存对prototype的引用，这保证了prototype对应整个类型的实例来说是共享的，例如，你可在Chrome浏览器内使用如下方式来访问Say方法：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; p1.__proto__[&quot;Say&quot;]();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>由于原型是一个对象，我们可以直接将一个对象赋值给prototype：</p>
<p>&gt;<br>&gt;</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; function Person()&#123;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      Person.prototype = &#123;name:&apos;xfrog&apos;, Say:function()&#123;</span><br><span class="line">&gt;           alert(this.name);</span><br><span class="line">&gt;      &#125;&#125;;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>注意这个方式下，实际上是完全替换了Person的prototype，这与上面Person.prototype.name方式还是有细微差异的，这是因为任何类型，Javascript引擎都会添加默认的prototype，在这个prototype中包含一个对构造函数的引用，即原型对象属性constructor，所以通常使用替代prototype方式时，我们需要手动加上constructor属性：</p>
<p>&gt;<br>&gt;</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; Person.prototype = &#123; </span><br><span class="line">&gt;           constructor: Person,</span><br><span class="line">&gt;           name :&apos;xfrog&apos;,</span><br><span class="line">&gt;           Say:function()&#123;</span><br><span class="line">&gt;                alert(this.name);</span><br><span class="line">&gt;           &#125;</span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>注意，由于prototype对于整个类型是共享的，那么在prototype中的引用类型可能会存在问题，前面的Say函数作为一个对象，也是引用类型，所以每个实例中的Say都指向原型对象中的同一个函数，这本身没有问题，也是我们使用原型的初衷，但对于其他引用对象，可能结果并不是我们想要的：</p>
<p>&gt;<br>&gt;</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; function Person()&#123;</span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      Person.prototype = &#123;</span><br><span class="line">&gt;           name: &apos;xfrog&apos;,</span><br><span class="line">&gt;           obj : &#123; age: 18 &#125;,</span><br><span class="line">&gt;           Say : function()&#123;</span><br><span class="line">&gt;                alert(this.obj.age);</span><br><span class="line">&gt;           &#125;</span><br><span class="line">&gt;      &#125;;</span><br><span class="line">&gt; </span><br><span class="line">&gt;      var p1 = new Person();</span><br><span class="line">&gt;      var p2 = new Person();</span><br><span class="line">&gt;      p1.obj.age = 20;</span><br><span class="line">&gt;      p1.Say();</span><br><span class="line">&gt;      p2.Say();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>p2.Say返回的是20，这是因为obj属性作为原型属性是共享的，在内存中只存在一个实例，所以通过p1修改后，p2只能得到修改后的状态。如果要避免此情况，可将obj属性放到实例中：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; function Person()&#123;</span><br><span class="line">&gt;           this.obj = &#123; age: 18 &#125;;</span><br><span class="line">&gt;      &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>HTML/JS/CSS</category>
      </categories>
  </entry>
  <entry>
    <title>CSS扼要-2</title>
    <url>/2016/10/03/CSS%E6%89%BC%E8%A6%81-2/</url>
    <content><![CDATA[<p>六、超链接样式</p>
<p>CSS:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">a:link&#123;      &lt;!–此为默认情况–&gt;</span><br><span class="line"></span><br><span class="line">text-decoration:none;   &lt;!–去掉下划线–&gt;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>②访问后样式 a:visited</strong>{      &lt;!–链接被访问（点击）过之后–&gt;</p>
<p>}</p>
 <a id="more"></a>
<p>兼容性问题：非IE会继承默认属性而IE不继承，故第一段未访问情况改为这样</p>
<p><strong>①未访问样式 a:link,a:visited</strong>{</p>
<p>}</p>
<p><strong>③鼠标移动到上面的时候的样式 a:hover</strong>{</p>
<p>}</p>
<p><strong>注意 :hover不只运用于超链接 对于任何元素均可设置hover的属性,表示当鼠标移动到上面的时候的样式 xxx:hovor{}</strong></p>
<p>类超链接控制器 <code>eg: a.xxx:link{} a.xxx:visited{} a.xxx:hover{}</code></p>
<p>—</p>
<p>HTML:</p>
<p><code>&lt;a href=”#”&gt;xxx&lt;/a&gt;</code></p>
<p><strong>注意CSS样式表的优先级问题 被包含的处理优先级要高（先加载它）一些</strong>，也就是先处理，所<strong>以实际套用的是优先级低（后加载的）的</strong>包含者的属性</p>
<p>比如<code>&lt;ul&gt;&lt;li&gt;&lt;/li&gt;&lt;li&gt;&lt;/li&gt;&lt;p&gt;&lt;/p&gt;&lt;/ui&gt;</code></p>
<p>ul{}</p>
<p>ul p{}</p>
<p>p被包含于ul中，所以实际上先处理了第二条 再处理了第一条，所以第二条的属性被第一条覆盖了</p>
<p>当使用了包含的操作符之后，它的加载时间比使用class的加载时间低，此时再来定义一个class的样式，就不会把使用包含的样式覆盖掉</p>
<p>七、CSS HACK</p>
<p>加<em> 使得不同浏览器读出不同的语句 加</em>语句该浏览器能识别就读 不能识别就跳过</p>
<p>八、错误总结出的经验：</p>
<p>1、float是设置给所有需要浮动的元素的属性 而不是表格整体</p>
<p>2、不能忽视[ul]和[div块]的宽度！！ul与div本身是有宽度属性的！！！</p>
<p>img文件一般放在css文件夹中（或其中的子目录） 方便background url设置等</p>
<p>两个点 ..访问上一级目录</p>
<p>cursor:pointer;   hovor中的样式 使得鼠标移动上去变成手指 </p>
<p>九、布局设计/流派</p>
<p>布局一：</p>
<p>一整块div网页的写法 这种大DIV一块的方式是以前的网站的常用布局</p>
<p>现在的布局一般不采用</p>
<p>布局二：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;div id=“head”&gt;</span><br><span class="line"></span><br><span class="line">      &lt;div id=“c_head”&gt;顶部图片&lt;/div&gt;  /*c center*/</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;div id=“nav”&gt;</span><br><span class="line"></span><br><span class="line">      &lt;div id=“c_nav”&gt;导航内容&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;div id=“content”&gt;</span><br><span class="line"></span><br><span class="line">      &lt;div id=“c_content”&gt;内容&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;div id=“bottom”&gt;</span><br><span class="line"></span><br><span class="line">      &lt;div id=“c_bottom”&gt;底部内容&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure></p>
<p>c_xxx也要用到div的居中对齐方法，见文章 <em>CSS扼要-1</em></p>
<p>这样就使得页面很宽 内容全铺在屏幕上</p>
<p>但实际上显示的内容还是在中间一部分</p>
<p>布局三：瀑布流布局</p>
<p>以专门看图片为主 仅CSS无法实现</p>
<p>十、display属性</p>
<p>display:none; 不显示</p>
<p>display:block; 显示</p>
<p><strong>导航菜单的伸缩实现</strong>：为菜单首先加上class=”a”; 但点击后使其class=”a”变成class=”b”（通过javascript实现）</p>
<p>–样式表：class=”a”{不显示} class=”b”{显示}</p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>HTML/JS/CSS</category>
      </categories>
  </entry>
  <entry>
    <title>bandwagon迁移digitalocean 大成功</title>
    <url>/2016/10/01/bandwagon%E8%BF%81%E7%A7%BBdigitalocean%20%E5%A4%A7%E6%88%90%E5%8A%9F/</url>
    <content><![CDATA[<p>centos6.5无可用php5.5源真是折腾死我了 ……………</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>CSS扼要-1</title>
    <url>/2016/09/29/CSS%E6%89%BC%E8%A6%81-1/</url>
    <content><![CDATA[<p>一、<br>HTML仅用于页面内容的显示<br>而CSS是修饰<br>可查阅CSS手册查看大量的CSS样式</p>
<p>CSS难点在于在什么情况应该用什么手段解决问题</p>
<p>参考书籍 <em>Zen Garden</em></p>
<p>引入样式表<code>&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;xxx.css&quot;&gt;</code></p>
<p>px像素</p>
<p><strong>标题和文章一般用12px/14px</strong> </p>
  <a id="more"></a>
<p>ATT注意编码问题，win10纪事本默认ANSI编码 如果meta里设置UTF-8则txt文档在保存的时候应该储存为UTF-8编码！否则会出现乱码，中文字体读不出来等情况</p>
<p><code>background:url(“xxx.xxx”) no-repeat;</code></p>
<p><code>background-position:5px 10px;</code>第一个值为左右 第二个值为上下</p>
<p>插入图片有三种方式：①img标签src属性②设置background</p>
<p>background更快，因为backgrond会把图片存入缓存中，<strong>故除非该图片会变化，则一律用background插入图片(可设置为一个div的样式）</strong></p>
<p>max-width:100%; 宽度自适应屏幕，高度同理,max-height:;</p>
<p>二、<strong>选择器</strong><br>p{} 标签选择器<br>#xx{} id选择器 id=”xx”<br>x.xx{} 类（class）选择器 class=”xx” 可以选择【一组】标签  p.xx{} 标签为p中的class为xx的标签，表示p标签的所有xx类，类不可单独拿出来选择 必须要【标签.类名】 格式<br>（W3C标准规定，在同一个页面内，不允许有相同名字的id对象出现，但是允许相同名字的class）<br>p.#xx{} 标签为p中id为xx的<br>* {} *表示所有的标签</p>
<p>div span{} 包含选择符 指p标签中的所有span标签都来设置这个信息(eg:<code>&lt;div&gt;..&lt;span&gt;&lt;/span&gt;..&lt;/div&gt;
div &gt;</code>h2{} 子对象选择符 (eg:<code>&lt;div&gt;…….&lt;h2&gt;….&lt;/h2&gt;….&lt;h2&gt;…..&lt;/h2&gt;…&lt;p&gt;..&lt;/p&gt;..&lt;div&gt;</code><br>注意区别：子对象选择符(&gt;)仅仅只针对第一级子对象，而包含选择符(空格)则针对一切子对象</p>
<p>xx,xx,#xx,#xx{} 分组选择符，用逗号同时设定多个标签，使他们都遵循下面样式</p>
<p>三、盒子模型（代替HTML中的表格布局 但不能完全替代框架）<br>对于每一层盒其margin和padding属性<br>margin:对外（上一级）<br>padding:对内（下一级）<br>不加left right top button则是四个方向统一设置(若加则单独设置margin-left,padding-right,etc）<br>也可以 margin:12px,10px,20px,20px 表示从top开始顺时针转的各方向</p>
<p><strong>千万不要使用padding来进行对齐操作</strong>。对于IE padding的值会算在width中，而对于其他浏览器，padding的值不算入width中！ <strong>不兼容问题</strong> 但若没有设置width则可以，但最建议还是用position方式</p>
<p>(如主菜单 每一个都是一个span 共存一个div中；或者文章列表，每一个标题都是一个span 存于一块div中)<br><strong>但主菜单，文章列表等等 一般用表格 ul dl等</strong></p>
<p><strong>那么如何方便居中呢 也一般使用表格ul dl</strong></p>
<p>（补充：去掉ul的点 list-style-type:none;）</p>
<p>有些标签比如h3 body标签自身margin和padding属性不为0，若需要要自己改，可以直接<strong>*{margin:0; padding:0;} css文件第一句话要写这个！一般都不用自身的margin和padding</strong></p>
<p>在html，有一些标签仅仅是用来设置文本，<strong>诸如a和span</strong>，对于这两个标签而言，在W3C标准中默认是<strong>不能进行width等样式进行修饰</strong>的，所以为这些标签设置width是没有作用的，<strong>需要display:block;</strong>（写入样式表）之后才有作用。但对于IE是有作用的（IE的标准与W3C不一样 微软强无敌= =）在开发中，一般对span加width是没有意义的，<strong>（一般不用为span加width），若果需要可能是设计有问题..</strong></p>
<p>四、定位</p>
<p>position样式  然后就可以设置top/bottom/right/left的值(eg : left:40px)</p>
<p>1、position:absolute; 绝对定位 针对浏览器而言的（此话是有误的，但很多时候是这样，见下一段）。当设置之后，该容器就不会再占用相应的空间，原有的空间会被其它元素占用</p>
<p>（<strong>实际上，绝对定位会针对父级标签中进行了absolute定位的标签来进行left/right等设置，如果父级标签都没有这样的定位方式，则根据body来定位</strong>）</p>
<p>2、position:relative;相对定位 针对父级元素进行定位，而且空间会一直占用，哪怕这个元素已经移动到其它位置</p>
<p>默认的定位方式为static，这种定位不能设置top/bottom/right/left</p>
<p>使用经验：经常会使用relative来实现文本位置的移动。所以如果要为某个容器设置里面的文本位置，可以按照如下方式来处理<strong><li><span>abscd</span></li></strong>   </p>
<p><strong>经验：内容一般都存在<span></span>或<a></a>里。</strong></p>
<p><strong>所以做居中对齐的时候，把内容放在span里，加position:relative;然后设置top/bottom/right/left</strong></p>
<p><strong>这是对齐的唯一方式！不能用padding,width！</strong></p>
<p><strong>居中一个div的方法（重要！）</strong>，text-align:center;只有IE可以居中，margin:auto;只有IE之外的浏览器可以居中，<strong>所以推荐的居中方式就是position定位的方法！见下：eg</strong></p>
<p>#container{</p>
<p><strong>width:1100px;</strong> &lt;!–宽度设为1100px–&gt;</p>
<p>border:1px solid #229;  &lt;!–别给contain div（整块的主div设高度），不设高度就让元素把它撑下来 有多少高多少–&gt;</p>
<p><strong>position:absolute;</strong> &lt;!—-&gt;</p>
<p><strong>left:50%;</strong>    &lt;!–左边框对齐页面中央–&gt;</p>
<p><strong>margin-left:-550px;</strong>  &lt;!–向左回一半–&gt;</p>
<p>}</p>
<p>当然也可两个居中方式都写..（orz这样IE和非IE都管用了，简单粗暴..）但推荐还是用以上的方式（因为如果设了margin：auto后其子类全部都继承这个属性）</p>
<p>3、position:fixed;</p>
<p>固定在页面上，不随滚动条滚动而改变位置</p>
<p>五、float</p>
<p>float:left;左飘，第一个放最左边</p>
<p>float:right;右飘，第一个放最右边</p>
<p>float可使得表格等横向浮动 （<strong>表格中在<li>样式里设置</li></strong>）</p>
<p>兼容性：对于IE，设置float后该标签依然占用空间，而其他浏览器不占用，<strong>故必须要再添加一个属性 clear:both;  将两段的float清除！否则对于IE外的浏览器设置了float的元素不再占用空间，下面的元素会飘上来！</strong></p>
<p><strong>border-bottom:1px solid red; 只设置底部边框那条 ，etc  加线</strong></p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>HTML/JS/CSS</category>
      </categories>
  </entry>
  <entry>
    <title>HTML扼要</title>
    <url>/2016/09/29/html/</url>
    <content><![CDATA[<p>一、每个块的标题 全用h 搜索引擎会搜索h<br>ul无序 （li） 诸如导航 文章列表等<br>dl 数据列表块 （dt标题 dd元素）</p>
<p>ul dl非常重要</p>
<p>表格<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line">&lt;tr&gt;  /*第一行*/</span><br><span class="line"></span><br><span class="line">&lt;td&gt;&lt;/td&gt;td&gt;&lt;/td&gt;td&gt;&lt;/td&gt;  /*第一行里每一列*/</span><br><span class="line"></span><br><span class="line">&lt;/tr&gt;</span><br><span class="line"></span><br><span class="line">&lt;tr&gt; /*第二行*/</span><br><span class="line"></span><br><span class="line">&lt;td&gt;&lt;/td&gt;td&gt;&lt;/td&gt;td&gt;&lt;/td&gt; /*第二行里每一列*/</span><br><span class="line"></span><br><span class="line">&lt;/tr&gt;</span><br><span class="line"></span><br><span class="line">&lt;/table&gt;</span><br></pre></td></tr></table></figure></p>
 <a id="more"></a>
<p>表格的样式属性 网上查</p>
<p>ol不常用<br>&amp;nbsp 页面空格<br>a超链接<code>href</code><br><code>&lt;!– –&gt;</code><br><code>&lt;br/&gt;</code><br>每个标签都要有/结束 如果只有一个不是一组则自结束<br>frame input br等都可自结束<br><code>&lt;pre&gt;</code> 格式显现出来的文本</p>
<p>二、常用布局标签<br>div于span用于进行容器控制<br>div 一般设定一个容器，这个容器中可以放置大量的内容<br>span 一般用于放置最后的文本数据，用来进行简单的控制</p>
<p>补充href属性和src属性的区别</p>
<p>href 表示超文本引用（hypertext reference），在 link和a 等元素上使用。src 表示来源地址，在 img、script、iframe 等元素上。</p>
<p>src 的内容，是页面必不可少的一部分，是引入。href 的内容，是与该页面有关联，是引用。区别就是，引入和引用。</p>
<p>三、表单<br>常用的表单标签原则上都要放在form标签中<br>input标签可以用来设置文本框密码框等数据<br>submit表示提交，提交的时候会链接到指定页面去处理<br>button是按钮，如果没有进行控制则不会发生反应<br>radio单选框 name用来分组 使得其变成单选框，当name一样表示这几个radio都在一个组中，点击其中一个会取消其他的选中<br>checkbox多选框 用法同radio 不用分组<br>下拉列表框 select 不用input 用法见下<br>textarea文本框 不用input 用法见下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;form action=”01.html”&gt;</span><br><span class="line">用户名：&lt;input type=”text”/&gt;&lt;br/&gt;</span><br><span class="line">密码：&lt;input type=”password”/&gt;&lt;br/&gt;</span><br><span class="line">性别：&lt;input type=”radio” value=”男” name=”sex”/&gt;男&lt;input type=”radio” value=”女” name=”sex”/&gt;女&lt;br/&gt;</span><br><span class="line">兴趣：&lt;input type=”checkbox”/&gt;足球&lt;input type=”checkbox”/&gt;羽毛球&lt;input type=”checkbox”/&gt;篮球&lt;input type=”checkbox”/&gt;乒乓球&amp;ltbr/&gt;</span><br><span class="line">籍贯：&lt;select&gt;</span><br><span class="line">&lt;option&gt;北京&lt;/option&gt;</span><br><span class="line">&lt;option&gt;上海&lt;/option&gt;</span><br><span class="line">&lt;option&gt;广州&lt;/option&gt;</span><br><span class="line">&lt;/select&gt;</span><br><span class="line">&lt;textarea cols=”50″ rows=”10″ &gt;&lt;/textarea&gt;&lt;br/&gt;</span><br><span class="line">&lt;input type=”submit” value=”用户注册”/&gt;</span><br><span class="line">&lt;input type=”button” value=”点一下试试”/&gt;</span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure>
<p>四、frameset 框架 布局</p>
<p>框架并不是完全被CSS布局替代，比如说在设计管理界面等页面框架的时候 局部刷新等的时候就需要框架 （当然建议也可以用div+css方式完成）<br>不能在body中设置，其上级为html标签<br>cols/rows表示让这个框架基于横/纵向方式切分，cols，rows可同时使用以平均拆分（像表格一样），每行每列达到要求。如果不想全部平均拆分（有些地方分有些地方部分），则使用frame嵌套的方法（多个框架文件嵌套）。<br>多使用frame嵌套方法实现较好的布局</p>
<p>3个框架大小依次列出150 *（剩下） 150<br>norisize使得边框不能移动；frameborder值为0表示没有边框 如果有边框则为1 border设置边框厚度<br>框架不能src属性不能为其自身文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;frameset cols=&quot;150,*,150&gt; (&lt;frameset cols=&quot;150,*,150 rows=&quot;150,*,150″/&gt;)</span><br><span class="line">&lt;frame src=&quot;01.html&quot; noresize frameborder=0 border=3/&gt;</span><br><span class="line">&lt;frame src=&quot;02.html&quot; target=&quot;content&quot;/&gt;</span><br><span class="line">&lt;frame name=&quot;content&quot; src=&quot;03.html&quot;/&gt;</span><br><span class="line">&lt;/frameset&gt;</span><br></pre></td></tr></table></figure>
<p>局部刷新的方法：在超链接的时候增加属性<code>target=”content”</code>，这个链接就会在content这个frame（href必须链接到该页面）中显示（当然，需要先把一个frame的名字设置为content）</p>
<p>如果要将整个框架居中 则需要在框架外再嵌套一层框架 使其分为左中右三栏，左右留为空白</p>
<p>如何通过css+div实现局部刷新？ 参考<a href="http://zhidao.baidu.com/link?url=ZuP0aA_mv4coT4zCflt3_wwuj9d2G-6kAgD9W22P_6sBdgWtp0iFRvZAcmb7XU0C7L35aO7VoqRH2tHhTlc8H_" target="_blank" rel="noopener">http://zhidao.baidu.com/link?url=ZuP0aA_mv4coT4zCflt3_wwuj9d2G-6kAgD9W22P_6sBdgWtp0iFRvZAcmb7XU0C7L35aO7VoqRH2tHhTlc8H_</a></p>
<p>五、通过<code>&lt;input  type=&quot;xxx&quot;&gt;&lt;/input&gt;</code>标签  等来插入按钮等  然后可以添加事件  比如xxx可以为button，则为添加一个按钮</p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>HTML/JS/CSS</category>
      </categories>
  </entry>
  <entry>
    <title>提前进入国庆状态..</title>
    <url>/2016/09/29/%E6%8F%90%E5%89%8D%E8%BF%9B%E5%85%A5%E5%9B%BD%E5%BA%86%E7%8A%B6%E6%80%81../</url>
    <content><![CDATA[<p><img src="http://aisakaki.com/wp-content/uploads/2016/05/image-1.jpeg" alt="image"></p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>172</title>
    <url>/2016/09/21/172/</url>
    <content><![CDATA[<p>——你是日向同学吧… </p>
<p>——你是说以前的我吗 </p>
<p>——…果然…你不记得了啊……已经…想不起来了吗…</p>
<p> ——做不到.我以前的记忆已经完全被消除了 </p>
<p>——什么都…能做到哦…日向同学的话……你看，去做的话总有办法… ………. </p>
<p>——果然…还是不行啊… ………. </p>
<p>——没能救…日向同学…对不起…</p>
<p> ——就算成了这种状况，你还是想着保护别人啊</p>
<p> ——因为…我喜欢大家啊………不要…不想死…我还有…想做的事…还想继续和大家做同学…还想再一次和日向同学…一起打游戏啊… </p>
<p>--- 我才是…谢谢你。 大家的事…我不会忘记哦… 永远永远…不会忘记哦… 从今以后…我也会在某处给大家加油哦。 因为…我们永远都是同伴嘛。</p>
<p> --- 你的愿望最终实现了哟</p>
]]></content>
      <categories>
        <category>Anime</category>
      </categories>
  </entry>
  <entry>
    <title>今年成都的夏天真热</title>
    <url>/2016/08/29/%E4%BB%8A%E5%B9%B4%E6%88%90%E9%83%BD%E7%9A%84%E5%A4%8F%E5%A4%A9%E7%9C%9F%E7%83%AD/</url>
    <content><![CDATA[<p>终于降温了~</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>iOS-1</title>
    <url>/2016/08/12/ios-1/</url>
    <content><![CDATA[<p>1、获取UIApplication代理： AppDelegate* appDelegate = [UIApplication shareApplication].delegate 应用程序代理是整个iOS应用的通讯中心，其他应用程序组件都可以通过该对象进行数据交换，同时ios应用代理还负责处理用用程序执行中的事件循环 应用程序代理需要满足两个规则：继承UIResponder基类和遵守UIApplicationDelegate协议（UIResponder是iOS应用提供的一个基类，所有需要向用户提供响应的对象都需要继承UIResponder基类）   </p>
 <a id="more"></a>
<p>2、MVC模式 百度百科：</p>
<p><a href="https://www.baidu.com/s?wd=MVC&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y3nHP-Pv7bPW6LnHF9rynv0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPHDsrjb1P101" target="_blank" rel="noopener">MVC</a>全名是Model View Controller，是模型(model)－视图(view)－<a href="https://www.baidu.com/s?wd=%E6%8E%A7%E5%88%B6%E5%99%A8&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y3nHP-Pv7bPW6LnHF9rynv0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPHDsrjb1P101" target="_blank" rel="noopener">控制器</a>(controller)的缩写，一种软件设计典范，用于组织代码用一种业务逻辑和数据显示分离的方法，这个方法的假设前提是如果业务逻辑被聚集到一个部件里面，而且界面和用户围绕数据的交互能被改进和个性化定制而不需要重新编写业务逻辑<a href="https://www.baidu.com/s?wd=MVC&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y3nHP-Pv7bPW6LnHF9rynv0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPHDsrjb1P101" target="_blank" rel="noopener">MVC</a>被独特的发展起来用于映射传统的输入、处理和输出功能在一个逻辑的图形化用户界面的结构中。</p>
<p>个人观点部分：<br><a href="https://www.baidu.com/s?wd=MVC&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y3nHP-Pv7bPW6LnHF9rynv0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPHDsrjb1P101" target="_blank" rel="noopener">MVC</a>也可以说是一个架构，无论架构还是设计模式也就离不开灵活性、重用性跟扩展性<br>Model-View-Control，可以看到，他的原则就是把一个项目分成三个部分，分别对项目中的三种元素进行拆解<br>Model：用于保存实体部分，保存了关于这个实体的某些算法功能、读写资料的功能<br>Control：顾名思义。<a href="https://www.baidu.com/s?wd=%E6%8E%A7%E5%88%B6%E5%99%A8&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y3nHP-Pv7bPW6LnHF9rynv0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPHDsrjb1P101" target="_blank" rel="noopener">控制器</a>就是将由View传来的任务分配给特定的Model去处理，再将处理完的结果返回到目地View。<br>View：用来将结果做显示。这是展现给用户看的一面<br>所以可以看到，只要遵循约定，Mdoel层中某个实体的输入输出算法需要改变或扩展的时候并不影响到Control跟View。而Control就像一个导航指针，作为Model跟View的中间桥梁，View则是象征着输出的部分。</p>
<p>另外还要遵循设计模式中的原则之一：要面向接口编程。这样才能谈得上其中一个元素的更改不会影响到另外两个元素。</p>
<p>最后再次引用<a href="https://www.baidu.com/s?wd=%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y3nHP-Pv7bPW6LnHF9rynv0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPHDsrjb1P101" target="_blank" rel="noopener">百度百科</a>：MVC使开发和维护用户接口的技术含量降低。　分离视图层和业务逻辑层也使得WEB应用更易于维护和修改。</p>
<p>原来以前做java大作业的时候，为了更好地设计程序架构，我自己无意间就实现了MVC模式..不过因为那时不知道，我把M叫底层，V叫表层，C叫数据传输   考虑到iOS， View组件：*.stroyboard等 Controller组件：View组件由ViewController来负责加载管理 Model组件：『数据，底层』</p>
]]></content>
      <categories>
        <category>开发</category>
        <category>IOS开发</category>
      </categories>
  </entry>
  <entry>
    <title>OC-4</title>
    <url>/2016/08/12/oc-4/</url>
    <content><![CDATA[<p>OC学习过程中的笔记</p>
<p>1、 NSString *str = @”Hello OC~~~”;</p>
<p>  NSLog(@”%@”,str);</p>
<p>只要输出一个字符串，则必须在””前加@。（只输出一个格式控制符也叫一个字符串 两个双引号出现就是一个字符串）</p>
<p>NSString的格式标识符为%@ 故如此输出</p>
<p>2、应该这么理解方法定义：</p>
<p>-(void) setWord1:(NSString <em>) word1 setWord2:(NSString </em>) word2</p>
 <a id="more"></a>
<p>“setWord1:setWord2:”是方法名，word1和word2是参数名。你所谓的参数2的名称（setWord2）其实是方法名的一部分，而不是参数名。objective-c的发明者希望方法名读起来像一个通顺的句子，结合这点来理解方法定义，就不会对objective-c怪异的语法感到困惑了。</p>
<p>例如，定义一个求两个数的和的方法，</p>
<p>-(float) addNumber1:(float)num1 toNumber2:(float)num2;</p>
<p>方法名“addNumber1:toNumber2:”读起来就像一个通顺的句子。</p>
<p>3、OC中self 类似于 java中的this</p>
<p>①OC没有私有成员，而是通过接口与实现两部分的方式来实现封装，在实现中定义接口没有的成员即为私有成员（仅供该类使用 不提供给子类及外部使用）</p>
<p>②使用interface声明的成员变量是只能在自己类和子类使用的（oc的默认访问权限是protected），而不能在类的外部使用，（就是通过类名. 点的方式是显示不出来的），pproperty则相反，它可以在类的外部访问，在类的内部可以通过下划线+变量名或者self.变量名的方式来访问。</p>
<p>③@property给默认访问权限的成员变量提供了一套getter和setter给外界 并且可以使用点语法</p>
<p>④另可使用访问控制符</p>
<p>但注意myWorld是指向对象的该对象的类的类型的引用，在使用的时候不能用点语法 而是用-&gt;  eg:myWORLD-&gt;cc</p>
<p>而对property定义的变量 外部调用只能用点语法不能用-&gt;</p>
<p>-&gt;是直接调用改成员 （因为property定义的是私有变量，所以不能直接调用），而点语法是调用getter setter方法</p>
<p>OC是不推荐成员变量即使是用公共访问控制符访问的，所以在其它语言中很常用的.语法 ，在OC中只给@property定义的变量提供</p>
<p>（property定义的变量可以使用点语法 默认调用setter getter方法）</p>
<p>这体现了oc良好的封装性，需要与外界交互的变量全权由property负责定义 等。</p>
<p>property 字面意思—属性 理解之</p>
<p>【注意property不只是可以定义基本类型，还可以定义对象，声明委托等等】</p>
<p>对于通过property与interface定义变量的两种定义方法的理解</p>
<p>见我的个人网站OC1文章 ，详细解释@interface @property两种定义变量的方法</p>
<p>当然 也可以用访问控制符来实现</p>
<p>@private 本类</p>
<p>@public</p>
<p>@protected 本类及子类</p>
<p>@package</p>
<p>当然也有getter setter oc为了简便，提供了合成存取方法 @property @synthesize</p>
<p>注意 @property int abc 后 实际上的成员变量为_abc 自动加了下划线请注意 （但是 在外部运用点语法的时候，依然使用xxx.abc而不是xxx.底层成员变量，相当于套了一层引用，我觉得可以叫表层成员变量）  可以通过synthesize的参数来修改</p>
<p>若@synthesize name 则默认等效于@synthesize name=name</p>
<p>此时 就可以直接运用点语法了</p>
<p>以下资料来自百度</p>
<p>『当定义了一系列的变量时，需要写很多的getter和setter方法，而且它们的形式都是差不多的，所以Xcode提供了@property和@synthesize属性，@property用在 .h 头文件中用作声明，@synthesize用在.m 文件中用于实现。</p>
<p>在X-code4.5以前，在.h中声明完属性之后，如：</p>
<p>@property（nonatomic，assign) int age;</p>
<p>@property（nonatomic，assign) NSString *name;</p>
<p>需要在.m中写上</p>
<p>@synthesize int age;</p>
<p>@synthesize NSString *name;</p>
<p>系统会自动去实现setter和getter方法</p>
<p>而在X-code4.5之后，@synthesize就不需要再写了，系统会直接去实现setter和getter方法。</p>
<p>【补充：但是 若要自己重写setter 或getter方法 则必须在实现部分声明@synthesize 注意 若@synthesize name则等效于把name的底层变量从_name修改成了name要注意】</p>
<p>另外，声明完property属性之后，会自动生成下划线，如_age、_name；如果不想要下划线，那么就可以使用@synthesize去修饰，例如，在.m中写@synthesize age；那么_age就会变成age』</p>
<p>4、一些习俗</p>
<p>接口部分和实现部分通常放到两个文件中（接口.h 实现.m）</p>
<p>成员变量通常加一个下划线用_a 参数用a</p>
<p>库前两个大写字母加特殊标识 一般表公司</p>
<p>5、@interface helloworld:NSObject</p>
<p>任何类都继承于NSObject 在没有其它父类的时候必须标明</p>
<p>6、OC中不存在类变量，但存在类方法，可以定义一个全局变量（static）（在实现部分）然后通过类似于getMethod 定义一个类方法返回该变量，称之为模拟类变量</p>
<p>7、OC须手动装箱</p>
<p>NSNumber 方法：numberWithXxx initWithXxx xxxValue</p>
<p>自动装箱不支持ARC</p>
<p>【补充：装箱的意义：将基本类型装箱成对象， 便于将『基本类型元素』储存在像集合这种只能储存对象的结构】</p>
<p>8、只要涉及对象 都要@ 而且是建立一个引用指向对象 故</p>
<p>ABC<em> q = xxx 要打</em> 是一个指针</p>
<p>9、（A）oc中也有类似于java中的toString方法———description</p>
<p>该方法为NSObject的方法 重写该方法以实现功能（打印对象）</p>
<p>直接输出该对象等于调用该方法</p>
<p>一般重写该方法用于告诉外界该对象所具有的状态信息</p>
<p>该方法返回值为NSString*</p>
<p> （B）oc中也有java中字符串的一些特性</p>
<p>比如  常量池保证相同的字符串直接量只有一个，不会产生多个副本，即两个指针指向同一个常量池中的对象，而可以用stringWithFormat方法（类似于java中的stringBuffer）将字符串对象创建在运行时内存区（堆内存）中。</p>
<p>[NSString stringWithFormat:”abc”]</p>
<p> （C）同java一样，==可以判断基本类型相等和指针地址相等（包括指向对象的引用 故判断字符串相等的时候要注意，需要用isEqualToString方法，注意NSObject的isEqual和==是等价的，经常需要重写）</p>
<p>10、nil相当于其它语言的NULL</p>
<p>11、OC中的『协议』相当于其它语言的接口</p>
<p>非正式协议通过类别和扩展实现 不强制实现所有方法</p>
<p>正式协议 @protocol 必须实现协议中所有方法</p>
<p>（但可以用@optional @required（默认）来改变）</p>
<p>使用协议定义的方法只可调用该协议中声明的[方法]，且只有两种定义语法 [用此方法来限定只能调用该协议中声明的方法]</p>
<p>NSObject<protocol1,protocol2>* p;</protocol1,protocol2></p>
<p>id<p1,p2> p;</p1,p2></p>
<p>p是指向对象的变量名</p>
<p>类可继承多个协议 协议可继承多个协议</p>
<p>定义方法 使用方法见书</p>
<p>@protocol XXX<xxx2,xxx3></xxx2,xxx3></p>
<p>@end</p>
<p>对委托的理解 见OC-3</p>
<p>12、扩展即为匿名类别，但类别只能有方法，而扩展可以增加实例变量 用@property @synthesize等</p>
<p>13、NSString 是不可变类 字符串一旦生成则不可改变</p>
<p>可以用NSMutableString（NSString的子类） 该类定义的字符串可以改变（但需要用到方法如appendFormat insertString 等等）</p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>OC/SWIFT</category>
      </categories>
  </entry>
  <entry>
    <title>OC-3 delegate</title>
    <url>/2016/08/08/OC-3%20delegate/</url>
    <content><![CDATA[<p>委托是协议的沿用，委托简单的理解就是某人委托某人去做某事，这个和java中的接口回调机制比较相似。委托在IOS开发中比较常用，比如我们不知道一个列表中的数据有多少，我们可以用委托的方式，将数据委托给其他类，让其他类去填充数据。委托的常用功能主要是传值和事件监听。</p>
<p>我们下面使用委托来实现2个类的委托，即老师委托学生去买本《OC开发》。</p>
<p>用来实现委托的协议(BuyBookDelegate.h)</p>
<p>@protocol BuyBookDelegate<br>//定义一个委托协议，协议中只有一个方法，用来买书<br>-(void)buyBook:(NSString*)name;<br>@end</p>
 <a id="more"></a>
<p>老师类</p>
<h1 id="import"><a href="#import" class="headerlink" title="import "></a>import <foundation foundation.h=""></foundation></h1><h1 id="import-“BuyBookDelegate-h”"><a href="#import-“BuyBookDelegate-h”" class="headerlink" title="import “BuyBookDelegate.h”"></a>import “BuyBookDelegate.h”</h1><p>@interface Teacher : NSObject<br>//定义一个委托协议<br>@property (nonatomic,assign)id<buybookdelegate> buyBookDele;<br>//定义一个老师想让学生买书的方法<br>-(void)willBuyBook:(NSString*)bookName;<br>@end</buybookdelegate></p>
<h1 id="import-“Teacher-h”"><a href="#import-“Teacher-h”" class="headerlink" title="import “Teacher.h”"></a>import “Teacher.h”</h1><p>@implementation Teacher<br>-(void)willBuyBook:(NSString*)bookName{<br>​    NSLog(@”老师想买本《%@》”,bookName);<br>​    [_buyBookDele buyBook:bookName];<br>}<br>@end</p>
<p>学生类</p>
<h1 id="import-1"><a href="#import-1" class="headerlink" title="import "></a>import <foundation foundation.h=""></foundation></h1><h1 id="import-“Teacher-h”-1"><a href="#import-“Teacher-h”-1" class="headerlink" title="import “Teacher.h”"></a>import “Teacher.h”</h1><p>@interface Student : NSObject <buybookdelegate><br>@end</buybookdelegate></p>
<h1 id="import-“Student-h”"><a href="#import-“Student-h”" class="headerlink" title="import “Student.h”"></a>import “Student.h”</h1><p>@implementation Student<br>-(void)buyBook:(NSString *)name{<br>​    NSLog(@”学生去买《%@》”,name);<br>}<br>@end</p>
<p>测试：</p>
<h1 id="import-2"><a href="#import-2" class="headerlink" title="import "></a>import <foundation foundation.h=""></foundation></h1><h1 id="import-“Teacher-h”-2"><a href="#import-“Teacher-h”-2" class="headerlink" title="import “Teacher.h”"></a>import “Teacher.h”</h1><h1 id="import-“Student-h”-1"><a href="#import-“Student-h”-1" class="headerlink" title="import “Student.h”"></a>import “Student.h”</h1><p>int main(int argc, const char <em> argv[]) {<br>​    @autoreleasepool {<br>​        Teacher</em> teacher = [[Teacher alloc]init];<br>​        Student* student = [[Student alloc]init];<br>​        //为老师设置委托的对象<br>​        [teacher setBuyBookDele:student];<br>​        //老师要买书了<br>​        [teacher willBuyBook:@”OC开发”];<br>​    }<br>​    return 0;<br>}</p>
<p>结果：</p>
<p><strong>老师想买本《**</strong>OC<strong>**开发》</strong></p>
<p><strong> </strong>学生去买《<strong>**OC</strong>开发》**</p>
<p>转载自<a href="http://m.blog.csdn.net/article/details?id=41827599" target="_blank" rel="noopener">http://m.blog.csdn.net/article/details?id=41827599</a></p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>OC/SWIFT</category>
      </categories>
  </entry>
  <entry>
    <title>OC-2 invalid operands to binary expression ('NSString *' and 'NSString *')q</title>
    <url>/2016/07/31/oc-2-invalid-operands-to-binary-expression-nsstring-and-nsstring-q/</url>
    <content><![CDATA[<p>Q： I have the following code:</p>
<pre><code>NSString  *String=TextField1.text + TextField2.text 
</code></pre><p>its giving the error: <code>-invalid operands to binary expression (&#39;NSString *&#39; and &#39;NSString *&#39;)</code>   An:</p>
 <a id="more"></a>
<p>You cannot do it this way, because objective-c doesn’t use ‘+’ operator for concatenation This way should work:</p>
<pre><code>NSString *concat = [NSString stringWithFormat@&quot;%@%@&quot;, TextField1.text, TextField2.text];
</code></pre><p>or</p>
<pre><code>NSString *concat = [TextField1.text stringByAppendingString:TextField2.text];
</code></pre><p>Hope this works for you ;)</p>
<pre><code>from stackoverflow
</code></pre>]]></content>
      <categories>
        <category>程序语言</category>
        <category>OC/SWIFT</category>
      </categories>
  </entry>
  <entry>
    <title>OC 1</title>
    <url>/2016/07/30/oc-1/</url>
    <content><![CDATA[<p><strong>**1、oc的默认访问权限：变量是protected，函数是public</strong></p>
<p>@property给默认访问权限的成员变量提供了一套getter和setter给外界</p>
<p>封装</p>
<p>2、</p>
<p>一直有疑问，在objective_C中声明变量会有 2种方式，今天有空和网友讨论了下，并且自己查了stackoverflew后算是稍微弄懂了一点。记录如下：</p>
<p>用了一段oc；会发现有2种定义变量的方式</p>
<p>1.在  @interface :NSObject{} 的括号中，当然NSObject 是指一个父类，可以是其他的。</p>
 <a id="more"></a>
<p>形式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 @interface GCTurnBasedMatchHelper : NSObject &#123;</span><br><span class="line">2 BOOL gameCenterAvailable;</span><br><span class="line">3 BOOL userAuthenticated;</span><br><span class="line">4 &#125;</span><br></pre></td></tr></table></figure>
<p>2.另外一种是直接在 @interface : NSObject{}括号之后，用 @property 去定义一个变量。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 @property (assign, readonly) BOOL gameCenterAvailable;</span><br></pre></td></tr></table></figure>
<p>你会发现，有人会再@interface中定义了变量后，又在 @property中重复定义相同的变量，而且很常见。</p>
<p>结果可能是这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 @interface GCTurnBasedMatchHelper : NSObject &#123;</span><br><span class="line">2 BOOL gameCenterAvailable;</span><br><span class="line">3 BOOL userAuthenticated;</span><br><span class="line">4 &#125;</span><br><span class="line">5 </span><br><span class="line">6 @property (assign, readonly) BOOL gameCenterAvailable;</span><br></pre></td></tr></table></figure>
<p>而且你可以单独在@interface中定义变量，而不用@property定义；也可以只用@property去定义，而不在@interface中定义，当然用了@property去定义，一般要在.m文件中用@synthsize去合成相应的setter，getter方法。否则会得到一个警告。当然@synthsize是可选的，但是是Apple推荐的，不用会有什么后果，我没试过，有兴趣的童鞋可以试一下。</p>
<p>那这两种方式有什么区别呢。</p>
<p>\1. 只在@interface中定义变量的话，你所定义的变量只能在当前的类中访问，在其他类中是访问不了的；而用@property声明的变量可以在外部访问。</p>
<p>2.用了@property去声明的变量，可以使用“self.变量名”的方式去读写变量。而用@interface的方式就不可以。</p>
<p>\3.  这里给出一个链接：<a href="http://stackoverflow.com/questions/9702258/difference-between-properties-and-variables-in-ios-header-file" target="_blank" rel="noopener">http://stackoverflow.com/questions/9702258/difference-between-properties-and-variables-in-ios-header-file</a>    里面讲到：  我英语菜，简单翻一下：</p>
<p>Defining the variables in the brackets simply declares them instance variables.</p>
<p>在括号中定义一个变量只是简单的声明了一个实例变量（实例变量应该指的成员变量）。  博主注：老外对variable 和instance variable是有不同理解的。所以下文中 用了一个模糊的词 ivar。</p>
<p>Declaring (and synthesizing) a property generates getters and setters for the instance variable, according to the criteria within the parenthesis. This is particularly important in Objective-C because it is often by way of getters and setters that memory is managed (e.g., when a value is assigned to an ivar, it is by way of the setter that the object assigned is retained and ultimately released). Beyond a memory management strategy, the practice also promotes encapsulation and reduces the amount of trivial code that would otherwise be required.</p>
<p>声明（和 @synthsize）一个属性会为成员变量生成 getter 和setter方法，根据括号内的标准,在oc中经常用setter和getter 做内存管理，这是很重要的。（例如： 当一个值被赋给这个变量，对象是通过setter函数去分配，修改计数器，并最后释放的）。更高一个层次来说，这种做法也促进了封装，减少了一些不必要的代码。</p>
<p>It is very common to declare an ivar in brackets and then an associated property (as in your example), but that isn’t strictly necessary. Defining the property and synthesizing is all that’s required, because synthesizing the property implicitly also creates an ivar.</p>
<p>在@interface括号中定义一个变量并用@property 重复定义一次是很普遍的，实际上不是必要的。用@property和@synthszie就够了，因为在用@synthsize合成这个属性的读写方法时就会创建一个变量。</p>
<p>The approach currently suggested by Apple (in templates) is:</p>
<p>目前苹果（在模板中）建议的方法是这样的：</p>
<p>-Define property in header file, e.g.:</p>
<p>先在头文件中定义一个属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 @property int gameCenter;</span><br></pre></td></tr></table></figure>
<p>Then synthesize &amp; declare ivar in implementation:</p>
<p>然后在实现文件中  synthsize和declare成这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 @synthesize gameCenter = __ gameCenter;</span><br></pre></td></tr></table></figure>
<p>The last line synthesizes the gameCenter property and asserts that whatever value is assigned to the property will be stored in the __gameCenter ivar. Again, this isn’t necessary, but by defining the ivar next to the synthesizer, you are reducing the locations where you have to type the name of the ivar while still explicitly naming it.</p>
<p>最后一行synthsize  gameCenter 属性并说明了不管什么值被分配给这个属性，都会存储到_gameCenter这个变量中。 再次说明，这不是必要的，但是，这样写了之后，你能减少输入已经明确命名的变量名。</p>
<p>最后一句的意思you are reducing the locations where you have to type the name of the ivar while still explicitly naming it .不好翻。</p>
<p>据千锋的第2节语法课课程的讲解，这样写之后可以使得 @synthsize 时内部getter方法会展成</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 -(int)gameCenter</span><br><span class="line">2 &#123;</span><br><span class="line">3    return  _gameCenter;</span><br><span class="line">4 &#125;</span><br></pre></td></tr></table></figure>
<p>而直接写  @synthsize  gameCenter；</p>
<p>setter函数会在内部展开成</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 -(int)gameCenter</span><br><span class="line">2 &#123;</span><br><span class="line">3    return  gameCenter;</span><br><span class="line">4 &#125;</span><br></pre></td></tr></table></figure>
<p>注意到：函数名和变量名是一样的。在斯坦福的课程中，白胡子教授也模糊的说道这样的同名有可能带来bug，具体什么bug他没说，我也没见过，所以还是养成这样写的习惯为好。其他语言的getter函数  一般会在变量前加 get；但oc没有，可能是为了与其他语言做区分，算是oc的特色，结果却带来这么个麻烦。</p>
<p>转载自<a href="http://www.cnblogs.com/letmefly/archive/2012/07/20/2601338.html" target="_blank" rel="noopener">http://www.cnblogs.com/letmefly/archive/2012/07/20/2601338.html</a></p>
]]></content>
      <categories>
        <category>程序语言</category>
        <category>OC/SWIFT</category>
      </categories>
  </entry>
  <entry>
    <title>终于考完了 进入苦逼的小学期</title>
    <url>/2016/07/05/%E7%BB%88%E4%BA%8E%E8%80%83%E5%AE%8C%E4%BA%86%20%E8%BF%9B%E5%85%A5%E8%8B%A6%E9%80%BC%E7%9A%84%E5%B0%8F%E5%AD%A6%E6%9C%9F/</url>
    <content><![CDATA[<p>先来局屁股</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>...</title>
    <url>/2016/05/18/113/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>不要让你的生命留下太多遗憾</title>
    <url>/2016/05/09/%E4%B8%8D%E8%A6%81%E8%AE%A9%E4%BD%A0%E7%9A%84%E7%94%9F%E5%91%BD%E7%95%99%E4%B8%8B%E5%A4%AA%E5%A4%9A%E9%81%97%E6%86%BE/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>TestMovie</title>
    <url>/2016/05/09/TestMovie/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>链表，栈，队列、树四种结构的构建方式的简单理解</title>
    <url>/2016/05/07/%E9%93%BE%E8%A1%A8%EF%BC%8C%E6%A0%88%EF%BC%8C%E9%98%9F%E5%88%97%E3%80%81%E6%A0%91%E5%9B%9B%E7%A7%8D%E7%BB%93%E6%9E%84%E7%9A%84%E6%9E%84%E5%BB%BA%E6%96%B9%E5%BC%8F%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>开个坑，有空填，方便自己考前速看一下</p>
<hr>
<p>两种储存结构 链式和顺式</p>
<p>比较一下链表的链式储存结构、栈和队列的顺式储存结构（不代表他们只有链式/顺式，只是着重强调）</p>
<p>假设所有的储存数据都是int型</p>
<ul>
<li>顺序栈的结构构建</li>
</ul>
<p>两个参数：初始容量、每次增加的容量</p>
<p>struct stack</p>
<p>{</p>
<p>int <em>base,</em>top;</p>
<p>int length;</p>
<p>};</p>
  <a id="more"></a>
<p>base指向栈底，top指向栈顶</p>
<p>length表明现在栈的容量</p>
<p>初始化的时候，需要初始化一个初始容量大小的空间。base指向第一个位置，top指向顶位置</p>
<p>base和top是指针，代表指向的位置，<strong>要储存的数据储存在『top指针当时指向的那一个位置』</strong></p>
<p>当到达栈顶，开辟新空间</p>
<ul>
<li>循环队列的结构构建</li>
</ul>
<p>参数： 最大容量</p>
<p>struct queen</p>
<p>{</p>
<p>int *base;</p>
<p>int front,rear;</p>
<p>};</p>
<p>其实顺序结构实际上是个数组。数组和指针在用法上有非常多的相似，这里用指针的操作方式建立了一个数组，</p>
<p>base是指向数组头的指针；base手动开辟一段最大容量大小的空间</p>
<p>front和rear是数组的下标，front在第一位数，rear在最后一个存放数据的位置的后一位</p>
<p><strong>要储存的内容即存放在该数组中</strong></p>
<p>这里，可以使用数组操作了。 比如base[a]</p>
<p>front-base即为队列长度</p>
<p>但是对于普通的一长列，可以用后-前来算长度，但由于这是循环的队列，所以有一些数学上的东西要修正：</p>
<p>若为不循环的表，b在a后面，那么b-a即可</p>
<p>若为一个循环列表 a到b中间空了多少格？</p>
<p>1、假设b在a前面，那么这就尴尬了。b-a为负数，这时候求个补，即加一个总长，b-a+length，这时候假设2-4=-2 ，那么就变成了-2+6（总长）=4，这即为他们之间的距离（注意：距离是朝顺方向的距离）。但是如果b-a&gt;0的话，这tm就又尴尬了，4-2+6=8了，再结合2、处理</p>
<p>2、若为一个循环链表，一个数到达顶端了要跳回第一个数怎么处理？</p>
<p>对它加一然后求余即可，故在循环链表中，表示一个数进一位用（a+1）%length表示。对于循环列表中的数，下标%length即为其该在的位置</p>
<ul>
<li>链表</li>
</ul>
<p>struct linknode</p>
<p>{</p>
<p>int data;</p>
<p>int *next;</p>
<p>};</p>
<p><strong>链表的内容储存在每一个节点中，每一个节点都是一个struct，而栈和队列都只有一个struct，即为栈、队列本身。</strong></p>
<p>串珠子，很容易理解</p>
<p>{对所有数据结构}一个易错点，struct sim;在主函数创建一个 sim a 还是 sim* a 要注意，两者皆可以但用法不同</p>
<p>建议链式结构 sim*a  顺式结构 sim a</p>
<ul>
<li>二叉树</li>
</ul>
<p>栈的递归运用之–二叉树</p>
<p>注意这里和链表的感觉很像，实则不然，创建方式和遍历方式是不同的，栈的创建需要至少两个指针p，q，像爬梯子一样创建。而二叉树的创建只需要一个根，然后递归创建</p>
<p>TBC</p>
<p><img src="http://aisakaki.com/wp-content/uploads/2016/05/image-2-300x298.jpeg" alt="image"></p>
]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>VMware虚拟机ping不通主机</title>
    <url>/2016/05/03/VMware%E8%99%9A%E6%8B%9F%E6%9C%BAping%E4%B8%8D%E9%80%9A%E4%B8%BB%E6%9C%BA/</url>
    <content><![CDATA[<p>待解决  </p>
<hr>
<p>5月4日更新 </p>
<p>问题解决了 <img src="http://aisakaki.com/wp-content/uploads/2016/05/image-2-300x298.jpeg" alt="image"> 使用桥接模式需要dhclient自动获取IP地址………..</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>Test image</title>
    <url>/2016/05/01/test-image/</url>
    <content><![CDATA[<p><img src="http://aisakaki.com/wp-content/uploads/2016/05/image-1.jpeg" alt="image"> 一脸懵逼</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>Sever.</title>
    <url>/2016/04/30/sever/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title>hello world</title>
    <url>/2016/04/30/hello-world-2/</url>
    <content><![CDATA[<p>from aisaka</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
</search>
